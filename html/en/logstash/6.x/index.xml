<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<?asciidoc-toc?>
<?asciidoc-numbered?>

<book lang="en">
<bookinfo>
    <title>Logstash Reference</title>
</bookinfo>
<chapter id="introduction">
<title>Logstash Introduction<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/6.x/docs/index-shared1.asciidoc">Edit me</ulink></title>
<simpara>Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
your data for diverse advanced downstream analytics and visualization use cases.</simpara>
<simpara>While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
volume and variety of data.</simpara>
<remark> The pass blocks here point to the correct repository for the edit links in the guide.</remark>
<remark> Introduction</remark>
<bridgehead id="power-of-logstash" renderas="sect1">The Power of Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara><emphasis role="strong">The ingestion workhorse for Elasticsearch and more</emphasis></simpara>
<simpara>Horizontally scalable data processing pipeline with strong Elasticsearch and Kibana synergy</simpara>
<simpara><emphasis role="strong">Pluggable pipeline architecture</emphasis></simpara>
<simpara>Mix, match, and orchestrate different inputs, filters, and outputs to play in pipeline harmony</simpara>
<simpara><emphasis role="strong">Community-extensible and developer-friendly plugin ecosystem</emphasis></simpara>
<simpara>Over 200 plugins available, plus the flexibility of creating and contributing your own</simpara>
<simpara><inlinemediaobject>
  <imageobject>
  <imagedata fileref="static/images/logstash.png"/>
  </imageobject>
  <textobject><phrase>static/images/logstash.png</phrase></textobject>
</inlinemediaobject></simpara>
<bridgehead id="_logstash_loves_data" renderas="sect1">Logstash Loves Data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Collect more, so you can know more. Logstash welcomes data of all shapes and sizes.</simpara>
<bridgehead id="_logs_and_metrics" renderas="sect2">Logs and Metrics<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Where it all started.</simpara>
<itemizedlist>
<listitem>
<simpara>
Handle all types of logging data
</simpara>
<itemizedlist>
<listitem>
<simpara>
Easily ingest a multitude of web logs like <ulink url="http://www.elastic.co/guide/en/logstash/6.x/advanced-pipeline.html">Apache</ulink>, and application
logs like <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-log4j.html">log4j</ulink> for Java
</simpara>
</listitem>
<listitem>
<simpara>
Capture many other log formats like <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-syslog.html">syslog</ulink>,
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-eventlog.html">Windows event logs</ulink>, networking and firewall logs, and more
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Enjoy complementary secure log forwarding capabilities with <ulink url="https://www.elastic.co/products/beats/filebeat">Filebeat</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
Collect metrics from <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-ganglia.html">Ganglia</ulink>, <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-codecs-collectd.html">collectd</ulink>,
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-codecs-netflow.html">NetFlow</ulink>, <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-jmx.html">JMX</ulink>, and many other infrastructure
and application platforms over <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-tcp.html">TCP</ulink> and <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-udp.html">UDP</ulink>
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_the_web" renderas="sect2">The Web<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Unlock the World Wide Web.</simpara>
<itemizedlist>
<listitem>
<simpara>
Transform <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-http.html">HTTP requests</ulink> into events
</simpara>
<itemizedlist>
<listitem>
<simpara>
Consume from web service firehoses like <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-twitter.html">Twitter</ulink> for social sentiment analysis
</simpara>
</listitem>
<listitem>
<simpara>
Webhook support for GitHub, HipChat, JIRA, and countless other applications
</simpara>
</listitem>
<listitem>
<simpara>
Enables many <ulink url="https://www.elastic.co/products/x-pack/alerting">Watcher</ulink> alerting use cases
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Create events by polling <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-http_poller.html">HTTP endpoints</ulink> on demand
</simpara>
<itemizedlist>
<listitem>
<simpara>
Universally capture health, performance, metrics, and other types of data from web application interfaces
</simpara>
</listitem>
<listitem>
<simpara>
Perfect for scenarios where the control of polling is preferred over receiving
</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<bridgehead id="_data_stores_and_streams" renderas="sect2">Data Stores and Streams<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Discover more value from the data you already own.</simpara>
<itemizedlist>
<listitem>
<simpara>
Better understand your data from any relational database or NoSQL store with a
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-jdbc.html">JDBC</ulink> interface
</simpara>
</listitem>
<listitem>
<simpara>
Unify diverse data streams from messaging queues like Apache <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-kafka.html">Kafka</ulink>,
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-rabbitmq.html">RabbitMQ</ulink>, <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-sqs.html">Amazon SQS</ulink>, and <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-zeromq.html">ZeroMQ</ulink>
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_sensors_and_iot" renderas="sect2">Sensors and IoT<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Explore an expansive breadth of other data.</simpara>
<itemizedlist>
<listitem>
<simpara>
In this age of technological advancement, the massive IoT world unleashes endless use cases through capturing and
harnessing data from connected sensors.
</simpara>
</listitem>
<listitem>
<simpara>
Logstash is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent
homes, connected vehicles, healthcare sensors, and many other industry specific applications.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_easily_enrich_everything" renderas="sect1">Easily Enrich Everything<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>The better the data, the better the knowledge. Clean and transform your data during ingestion to gain near real-time
insights immediately at index or output time. Logstash comes out-of-box with many aggregations and mutations along
with pattern matching, geo mapping, and dynamic lookup capabilities.</simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-grok.html">Grok</ulink> is the bread and butter of Logstash filters and is used ubiquitously to derive
structure out of unstructured data. Enjoy a wealth of integrated patterns aimed to help quickly resolve web, systems,
networking, and other types of event formats.
</simpara>
</listitem>
<listitem>
<simpara>
Expand your horizons by deciphering <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-geoip.html">geo coordinates</ulink> from IP addresses, normalizing
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-date.html">date</ulink> complexity, simplifying <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-kv.html">key-value pairs</ulink> and
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-csv.html">CSV</ulink> data, <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-fingerprint.html">fingerprinting</ulink>(anonymizing) sensitive information,
and further enriching your data with <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-translate.html">local lookups</ulink> or Elasticsearch
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-elasticsearch.html">queries</ulink>.
</simpara>
</listitem>
<listitem>
<simpara>
Codecs are often used to ease the processing of common event structures like <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-codecs-json.html">JSON</ulink>
and <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-codecs-multiline.html">multiline</ulink> events.
</simpara>
</listitem>
</itemizedlist>
<simpara>See <ulink url="http://www.elastic.co/guide/en/logstash/6.x/transformation.html">Transforming Data</ulink> for an overview of some of the popular data processing plugins.</simpara>
<bridgehead id="_choose_your_stash" renderas="sect1">Choose Your Stash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc">Edit me</ulink></bridgehead>
<simpara>Route your data where it matters most. Unlock various downstream analytical and operational use cases by storing,
analyzing, and taking action on your data.</simpara>
<simpara><emphasis role="strong">Analysis</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-elasticsearch.html">Elasticsearch</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
Data stores such as <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-mongodb.html">MongoDB</ulink> and <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-riak.html">Riak</ulink>
</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">Archiving</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-webhdfs.html">HDFS</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-s3.html">S3</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-google_cloud_storage.html">Google Cloud Storage</ulink>
</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">Monitoring</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-nagios.html">Nagios</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-ganglia.html">Ganglia</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-zabbix.html">Zabbix</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-graphite.html">Graphite</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-datadog.html">Datadog</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-cloudwatch.html">CloudWatch</ulink>
</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">Alerting</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="https://www.elastic.co/products/watcher">Watcher</ulink> with Elasticsearch
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-email.html">Email</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-pagerduty.html">Pagerduty</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-irc.html">IRC</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-sns.html">SNS</ulink>
</simpara>
</listitem>
</itemizedlist>
<remark> Glossary and core concepts go here</remark>
<remark> Getting Started with Logstash</remark>
</chapter>
<chapter id="getting-started-with-logstash">
<title>Getting Started with Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></title>
<simpara>This section guides you through the process of installing Logstash and verifying that everything is running properly.
After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.</simpara>
<simpara>This section includes the following topics:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="installing-logstash"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="first-event"/>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/advanced-pipeline.html">Advanced Pipeline</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/multiple-input-output-plugins.html">Multiple Output Plugins</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="installing-logstash">
<title>Installing Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></title>
<note><simpara>Logstash requires Java 8. Java 9 is not supported. Use the
<ulink url="http://www.oracle.com/technetwork/java/javase/downloads/index.html">official Oracle distribution</ulink> or an open-source
distribution such as <ulink url="http://openjdk.java.net/">OpenJDK</ulink>.</simpara></note>
<simpara>To check your Java version, run the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">java -version</programlisting>
<simpara>On systems with Java installed, this command produces output similar to the following:</simpara>
<programlisting language="shell" linenumbering="unnumbered">java version "1.8.0_65"
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)</programlisting>
<simpara>On some Linux systems, you may also need to have the <literal>JAVA_HOME</literal> environment
exported before attempting the install, particularly if you installed Java from
a tarball.  This is because Logstash uses Java during installation to
automatically detect your environment and install the correct startup method
(SysV init scripts, Upstart, or systemd).  If Logstash is unable to find the
JAVA_HOME environment variable during package installation time, you may get an
error message, and Logstash will be unable to start properly.</simpara>
<bridgehead id="installing-binary" renderas="sect2">Installing from a Downloaded Binary<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></bridgehead>
<simpara>Download the <ulink url="https://www.elastic.co/downloads/logstash">Logstash installation file</ulink> that matches your host environment.
Unpack the file. Do not install Logstash into a directory path that contains colon (:) characters.</simpara>
<simpara>On supported Linux operating systems, you can use a package manager to install Logstash.</simpara>
<bridgehead id="package-repositories" renderas="sect2">Installing from Package Repositories<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></bridgehead>
<simpara>We also have repositories available for APT and YUM based distributions. Note
that we only provide binary packages, but no source packages, as the packages
are created as part of the Logstash build.</simpara>
<simpara>We have split the Logstash package repositories by version into separate urls
to avoid accidental upgrades across major versions. For all 6.x.y
releases use 6.x as version number.</simpara>
<simpara>We use the PGP key
<ulink url="https://pgp.mit.edu/pks/lookup?op=vindex&amp;search=0xD27D666CD88E42B4">D88E42B4</ulink>,
Elastic&#8217;s Signing Key, with fingerprint</simpara>
<literallayout class="monospaced">4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4</literallayout>
<simpara>to sign all our packages. It is available from <ulink url="https://pgp.mit.edu">https://pgp.mit.edu</ulink>.</simpara>
<bridgehead id="_apt" renderas="sect3">APT<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></bridgehead>
<simpara>Version 6.1.0 of Logstash has not yet been released.</simpara>
<bridgehead id="_yum" renderas="sect3">YUM<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></bridgehead>
<simpara>Version 6.1.0 of Logstash has not yet been released.</simpara>
<section id="_docker">
<title>Docker<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></title>
<simpara>Images are available for running Logstash as a Docker container. They are
available from the Elastic Docker registry.</simpara>
<simpara>See <link linkend="docker">Running Logstash on Docker</link> for
details on how to configure and run Logstash Docker containers.</simpara>
</section>
</section>
<section id="first-event">
<title>Stashing Your First Event<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc">Edit me</ulink></title>
<simpara>First, let&#8217;s test your Logstash installation by running the most basic <emphasis>Logstash pipeline</emphasis>.</simpara>
<simpara>A Logstash pipeline has two required elements, <literal>input</literal> and <literal>output</literal>, and one optional element, <literal>filter</literal>. The input
plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
the data to a destination.</simpara>
<remark>TODO: REPLACE WITH NEW IMAGE</remark>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/basic_logstash_pipeline.png"/>
  </imageobject>
  <textobject><phrase>static/images/basic_logstash_pipeline.png</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>To test your Logstash installation, run the most basic Logstash pipeline. For
example:</simpara>
<programlisting language="sh" linenumbering="unnumbered">cd logstash-6.1.0
bin/logstash -e 'input { stdin { } } output { stdout {} }'</programlisting>
<note><simpara>The location of the <literal>bin</literal> directory varies by platform. See <ulink url="http://www.elastic.co/guide/en/logstash/6.x/dir-layout.html">Directory layout</ulink>
to find the location of <literal>bin\logstash</literal> on your system.</simpara></note>
<simpara>The <literal>-e</literal> flag enables you to specify a configuration directly from the command line. Specifying configurations at the
command line lets you quickly test configurations without having to edit a file between iterations.
The pipeline in the example takes input from the standard input, <literal>stdin</literal>, and moves that input to the standard output,
<literal>stdout</literal>, in a structured format.</simpara>
<simpara>After starting Logstash, wait until you see "Pipeline main started" and then enter <literal>hello world</literal> at the command prompt:</simpara>
<programlisting language="shell" linenumbering="unnumbered">hello world
2013-11-21T01:22:14.405+0000 0.0.0.0 hello world</programlisting>
<simpara>Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a <emphasis role="strong">CTRL-D</emphasis> command in the
shell where Logstash is running.</simpara>
<simpara>Congratulations! You&#8217;ve created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.</simpara>
<remark> Advanced LS Pipelines</remark>
</section>
<section id="advanced-pipeline">
<title>Parsing Logs with Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></title>
<simpara>In <xref linkend="first-event"/>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.</simpara>
<simpara>In this section, you create a Logstash pipeline that uses Filebeat to take Apache web logs as input, parses those
logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
defining the pipeline configuration at the command line, you&#8217;ll define the pipeline in a config file.</simpara>
<simpara>To get started, go <ulink url="https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz">here</ulink> to
download the sample data set used in this example. Unpack the file.</simpara>
<section id="configuring-filebeat">
<title>Configuring Filebeat to Send Log Lines to Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></title>
<simpara>Before you create the Logstash pipeline, you&#8217;ll configure Filebeat to send log lines to Logstash.
The <ulink url="https://github.com/elastic/beats/tree/master/filebeat">Filebeat</ulink> client is a lightweight, resource-friendly tool
that collects logs from files on the server and forwards these logs to your Logstash instance for processing.
Filebeat is designed for reliability and low latency. Filebeat has a light resource footprint on the host machine,
and the <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-beats.html"><literal>Beats input</literal></ulink> plugin minimizes the resource demands on the Logstash
instance.</simpara>
<note><simpara>In a typical use case, Filebeat runs on a separate machine from the machine running your
Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
same machine.</simpara></note>
<simpara>The default Logstash installation includes the <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-beats.html"><literal>Beats input</literal></ulink> plugin. The Beats
input plugin enables Logstash to receive events from the Elastic Beats framework, which means that any Beat written
to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash.</simpara>
<simpara>To install Filebeat on your data source machine, download the appropriate package from the Filebeat <ulink url="https://www.elastic.co/downloads/beats/filebeat">product page</ulink>. You can also refer to
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-getting-started.html">Getting Started with Filebeat</ulink> in the Beats documentation for additional
installation instructions.</simpara>
<simpara>After installing Filebeat, you need to configure it. Open the <literal>filebeat.yml</literal> file located in your Filebeat installation
directory, and replace the contents with the following lines. Make sure <literal>paths</literal> points to the example Apache log file,
<literal>logstash-tutorial.log</literal>, that you downloaded earlier:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">filebeat.prospectors:
- type: log
  paths:
    - /path/to/file/logstash-tutorial.log <co id="CO1-1"/>
output.logstash:
  hosts: ["localhost:5043"]</programlisting>
<calloutlist>
<callout arearefs="CO1-1">
<para>
Absolute path to the file or files that Filebeat processes.
</para>
</callout>
</calloutlist>
<simpara>Save your changes.</simpara>
<simpara>To keep the configuration simple, you won&#8217;t specify TLS/SSL settings as you would in a real world
scenario.</simpara>
<simpara>At the data source machine, run Filebeat with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo ./filebeat -e -c filebeat.yml -d "publish"</programlisting>
<note><simpara>If you run Filebeat as root, you need to change ownership of the configuration file (see
<ulink url="https://www.elastic.co/guide/en/beats/libbeat/6.x/config-file-permissions.html">Config File Ownership and Permissions</ulink>
in the <emphasis>Beats Platform Reference</emphasis>).</simpara></note>
<simpara>Filebeat will attempt to connect on port 5043. Until Logstash starts with an active Beats plugin, there
won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.</simpara>
</section>
<section id="_configuring_logstash_for_filebeat_input">
<title>Configuring Logstash for Filebeat Input<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></title>
<simpara>Next, you create a Logstash configuration pipeline that uses the Beats input plugin to receive
events from Beats.</simpara>
<simpara>The following text represents the skeleton of a configuration pipeline:</simpara>
<programlisting language="json" linenumbering="unnumbered"># The # character at the beginning of a line indicates a comment. Use
# comments to describe your configuration.
input {
}
# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }
output {
}</programlisting>
<simpara>This skeleton is non-functional, because the input and output sections don’t have any valid options defined.</simpara>
<simpara>To get started, copy and paste the skeleton configuration pipeline into a file named <literal>first-pipeline.conf</literal> in your home
Logstash directory.</simpara>
<simpara>Next, configure your Logstash instance to use the Beats input plugin by adding the following lines to the <literal>input</literal> section
of the <literal>first-pipeline.conf</literal> file:</simpara>
<programlisting language="json" linenumbering="unnumbered">    beats {
        port =&gt; "5043"
    }</programlisting>
<simpara>You&#8217;ll configure Logstash to write to Elasticsearch later. For now, you can add the following line
to the <literal>output</literal> section so that the output is printed to stdout when you run Logstash:</simpara>
<programlisting language="json" linenumbering="unnumbered">    stdout { codec =&gt; rubydebug }</programlisting>
<simpara>When you&#8217;re done, the contents of <literal>first-pipeline.conf</literal> should look like this:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
    beats {
        port =&gt; "5043"
    }
}
# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }
output {
    stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>To verify your configuration, run the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f first-pipeline.conf --config.test_and_exit</programlisting>
<simpara>The <literal>--config.test_and_exit</literal> option parses your configuration file and reports any errors.</simpara>
<simpara>If the configuration file passes the configuration test, start Logstash with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f first-pipeline.conf --config.reload.automatic</programlisting>
<simpara>The <literal>--config.reload.automatic</literal> option enables automatic config reloading so that you don&#8217;t have to stop and restart Logstash
every time you modify the configuration file.</simpara>
<simpara>As Logstash starts up, you might see one or more warning messages about Logstash ignoring the <literal>pipelines.yml</literal> file. You
can safely ignore this warning. The <literal>pipelines.yml</literal> file is used for running <link linkend="multiple-pipelines">multiple pipelines</link>
in a single Logstash instance. For the examples shown here, you are running a single pipeline.</simpara>
<simpara>If your pipeline is working correctly, you should see a series of events like the following written to the console:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
    "@timestamp" =&gt; 2017-11-09T01:44:20.071Z,
        "offset" =&gt; 325,
      "@version" =&gt; "1",
          "beat" =&gt; {
            "name" =&gt; "My-MacBook-Pro.local",
        "hostname" =&gt; "My-MacBook-Pro.local",
         "version" =&gt; "6.0.0"
    },
          "host" =&gt; "My-MacBook-Pro.local",
    "prospector" =&gt; {
        "type" =&gt; "log"
    },
        "source" =&gt; "/path/to/file/logstash-tutorial.log",
       "message" =&gt; "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
          "tags" =&gt; [
        [0] "beats_input_codec_plain_applied"
    ]
}
...</programlisting>
<bridgehead id="configuring-grok-filter" renderas="sect3">Parsing Web Logs with the Grok Filter Plugin<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>Now you have a working pipeline that reads log lines from Filebeat. However you&#8217;ll notice that the format of the log messages
is not ideal. You want to parse the log messages to create specific, named fields from the logs.
To do this, you&#8217;ll use the <literal>grok</literal> filter plugin.</simpara>
<simpara>The <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-grok.html"><literal>grok</literal></ulink> filter plugin is one of several plugins that are available by default in
Logstash. For details on how to manage Logstash plugins, see the <link linkend="working-with-plugins">reference documentation</link> for
the plugin manager.</simpara>
<simpara>The <literal>grok</literal> filter plugin enables you to parse the unstructured log data into something structured and queryable.</simpara>
<simpara>Because the <literal>grok</literal> filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to
make decisions about how to identify the patterns that are of interest to your use case. A representative line from the
web server log sample looks like this:</simpara>
<programlisting language="shell" linenumbering="unnumbered">83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png
HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel
Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"</programlisting>
<simpara>The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the <literal>%{COMBINEDAPACHELOG}</literal> grok pattern, which structures lines from the Apache log using the following schema:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<emphasis role="strong">Information</emphasis>
</simpara>
</entry>
<entry>
<simpara>
<emphasis role="strong">Field Name</emphasis>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
IP Address
</simpara>
</entry>
<entry>
<simpara>
<literal>clientip</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
User ID
</simpara>
</entry>
<entry>
<simpara>
<literal>ident</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
User Authentication
</simpara>
</entry>
<entry>
<simpara>
<literal>auth</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
timestamp
</simpara>
</entry>
<entry>
<simpara>
<literal>timestamp</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
HTTP Verb
</simpara>
</entry>
<entry>
<simpara>
<literal>verb</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
Request body
</simpara>
</entry>
<entry>
<simpara>
<literal>request</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
HTTP Version
</simpara>
</entry>
<entry>
<simpara>
<literal>httpversion</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
HTTP Status Code
</simpara>
</entry>
<entry>
<simpara>
<literal>response</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
Bytes served
</simpara>
</entry>
<entry>
<simpara>
<literal>bytes</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
Referrer URL
</simpara>
</entry>
<entry>
<simpara>
<literal>referrer</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
User agent
</simpara>
</entry>
<entry>
<simpara>
<literal>agent</literal>
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<tip><simpara>If you need help building grok patterns, try out the
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/xpack-grokdebugger.html">Grok Debugger</ulink>. The Grok Debugger is an
X-Pack feature under the Basic License and is therefore <emphasis role="strong">free to use</emphasis>.</simpara></tip>
<simpara>Edit the <literal>first-pipeline.conf</literal> file and replace the entire <literal>filter</literal> section with the following text:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
    grok {
        match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}"}
    }
}</programlisting>
<simpara>When you&#8217;re done, the contents of <literal>first-pipeline.conf</literal> should look like this:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
    beats {
        port =&gt; "5043"
    }
}
filter {
    grok {
        match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}"}
    }
}
output {
    stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Save your changes. Because you&#8217;ve enabled automatic config reloading, you don&#8217;t have to restart Logstash to
pick up your changes. However, you do need to force Filebeat to read the log file from scratch. To do this,
go to the terminal window where Filebeat is running and press Ctrl+C to shut down Filebeat. Then delete the
Filebeat registry file. For example, run:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo rm data/registry</programlisting>
<simpara>Since Filebeat stores the state of each file it harvests in the registry, deleting the registry file forces
Filebeat to read all the files it&#8217;s harvesting from scratch.</simpara>
<simpara>Next, restart Filebeat with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo ./filebeat -e -c filebeat.yml -d "publish"</programlisting>
<simpara>There might be a slight delay before Filebeat begins processing events if it needs to wait for Logstash to reload the
config file.</simpara>
<simpara>After Logstash applies the grok pattern, the events will have the following JSON representation:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
        "request" =&gt; "/presentations/logstash-monitorama-2013/images/kibana-search.png",
          "agent" =&gt; "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
         "offset" =&gt; 325,
           "auth" =&gt; "-",
          "ident" =&gt; "-",
           "verb" =&gt; "GET",
     "prospector" =&gt; {
        "type" =&gt; "log"
    },
         "source" =&gt; "/path/to/file/logstash-tutorial.log",
        "message" =&gt; "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
           "tags" =&gt; [
        [0] "beats_input_codec_plain_applied"
    ],
       "referrer" =&gt; "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
     "@timestamp" =&gt; 2017-11-09T02:51:12.416Z,
       "response" =&gt; "200",
          "bytes" =&gt; "203023",
       "clientip" =&gt; "83.149.9.216",
       "@version" =&gt; "1",
           "beat" =&gt; {
            "name" =&gt; "My-MacBook-Pro.local",
        "hostname" =&gt; "My-MacBook-Pro.local",
         "version" =&gt; "6.0.0"
    },
           "host" =&gt; "My-MacBook-Pro.local",
    "httpversion" =&gt; "1.1",
      "timestamp" =&gt; "04/Jan/2015:05:13:42 +0000"
}</programlisting>
<simpara>Notice that the event includes the original message, but the log message is also broken down into specific fields.</simpara>
<bridgehead id="configuring-geoip-plugin" renderas="sect3">Enhancing Your Data with the Geoip Filter Plugin<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
data. As an example, the <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-geoip.html"><literal>geoip</literal></ulink> plugin looks up IP addresses, derives geographic
location information from the addresses, and adds that location information to the logs.</simpara>
<simpara>Configure your Logstash instance to use the <literal>geoip</literal> filter plugin by adding the following lines to the <literal>filter</literal> section
of the <literal>first-pipeline.conf</literal> file:</simpara>
<programlisting language="json" linenumbering="unnumbered">    geoip {
        source =&gt; "clientip"
    }</programlisting>
<simpara>The <literal>geoip</literal> plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the <literal>clientip</literal> field contains the IP address.</simpara>
<simpara>Since filters are evaluated in sequence, make sure that the <literal>geoip</literal> section is after the <literal>grok</literal> section of
the configuration file and that both the <literal>grok</literal> and <literal>geoip</literal> sections are nested within the <literal>filter</literal> section.</simpara>
<simpara>When you&#8217;re done, the contents of <literal>first-pipeline.conf</literal> should look like this:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
    beats {
        port =&gt; "5043"
    }
}
 filter {
    grok {
        match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}"}
    }
    geoip {
        source =&gt; "clientip"
    }
}
output {
    stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C),
delete the registry file, and then restart Filebeat with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo ./filebeat -e -c filebeat.yml -d "publish"</programlisting>
<simpara>Notice that the event now contains geographic location information:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
        "request" =&gt; "/presentations/logstash-monitorama-2013/images/kibana-search.png",
          "agent" =&gt; "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
          "geoip" =&gt; {
              "timezone" =&gt; "Europe/Moscow",
                    "ip" =&gt; "83.149.9.216",
              "latitude" =&gt; 55.7485,
        "continent_code" =&gt; "EU",
             "city_name" =&gt; "Moscow",
          "country_name" =&gt; "Russia",
         "country_code2" =&gt; "RU",
         "country_code3" =&gt; "RU",
           "region_name" =&gt; "Moscow",
              "location" =&gt; {
            "lon" =&gt; 37.6184,
            "lat" =&gt; 55.7485
        },
           "postal_code" =&gt; "101194",
           "region_code" =&gt; "MOW",
             "longitude" =&gt; 37.6184
    },
    ...</programlisting>
<bridgehead id="indexing-parsed-data-into-elasticsearch" renderas="sect3">Indexing Your Data into Elasticsearch<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
Elasticsearch cluster. Edit the <literal>first-pipeline.conf</literal> file and replace the entire <literal>output</literal> section with the following
text:</simpara>
<programlisting language="json" linenumbering="unnumbered">output {
    elasticsearch {
        hosts =&gt; [ "localhost:9200" ]
    }
}</programlisting>
<simpara>With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
the <literal>hosts</literal> configuration to specify something like <literal>hosts =&gt; [ "es-machine:9092" ]</literal>.</simpara>
<simpara>At this point, your <literal>first-pipeline.conf</literal> file has input, filter, and output sections properly configured, and looks
something like this:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
    beats {
        port =&gt; "5043"
    }
}
 filter {
    grok {
        match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}"}
    }
    geoip {
        source =&gt; "clientip"
    }
}
output {
    elasticsearch {
        hosts =&gt; [ "localhost:9200" ]
    }
}</programlisting>
<simpara>Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C),
delete the registry file, and then restart Filebeat with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo ./filebeat -e -c filebeat.yml -d "publish"</programlisting>
<bridgehead id="testing-initial-pipeline" renderas="sect4">Testing Your Pipeline<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>Now that the Logstash pipeline is configured to index the data into an
Elasticsearch cluster, you can query Elasticsearch.</simpara>
<simpara>Try a test query to Elasticsearch based on the fields created by the <literal>grok</literal> filter plugin.
Replace $DATE with the current date, in YYYY.MM.DD format:</simpara>
<programlisting language="shell" linenumbering="unnumbered">curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&amp;q=response=200'</programlisting>
<note><simpara>The date used in the index name is based on UTC, not the timezone where Logstash is running.
If the query returns <literal>index_not_found_exception</literal>, make sure that <literal>logstash-$DATE</literal> reflects the actual
name of the index. To see a list of available indexes, use this query: <literal>curl 'localhost:9200/_cat/indices?v'</literal>.</simpara></note>
<simpara>You should get multiple hits back. For example:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "took": 50,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 98,
    "max_score": 2.793642,
    "hits": [
      {
        "_index": "logstash-2017.11.09",
        "_type": "doc",
        "_id": "3IzDnl8BW52sR0fx5wdV",
        "_score": 2.793642,
        "_source": {
          "request": "/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
          "agent": """"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"""",
          "geoip": {
            "timezone": "Europe/Moscow",
            "ip": "83.149.9.216",
            "latitude": 55.7485,
            "continent_code": "EU",
            "city_name": "Moscow",
            "country_name": "Russia",
            "country_code2": "RU",
            "country_code3": "RU",
            "region_name": "Moscow",
            "location": {
              "lon": 37.6184,
              "lat": 55.7485
            },
            "postal_code": "101194",
            "region_code": "MOW",
            "longitude": 37.6184
          },
          "offset": 2932,
          "auth": "-",
          "ident": "-",
          "verb": "GET",
          "prospector": {
            "type": "log"
          },
          "source": "/path/to/file/logstash-tutorial.log",
          "message": """83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"""",
          "tags": [
            "beats_input_codec_plain_applied"
          ],
          "referrer": """"http://semicomplete.com/presentations/logstash-monitorama-2013/"""",
          "@timestamp": "2017-11-09T03:11:35.304Z",
          "response": "200",
          "bytes": "52878",
          "clientip": "83.149.9.216",
          "@version": "1",
          "beat": {
            "name": "My-MacBook-Pro.local",
            "hostname": "My-MacBook-Pro.local",
            "version": "6.0.0"
          },
          "host": "My-MacBook-Pro.local",
          "httpversion": "1.1",
          "timestamp": "04/Jan/2015:05:13:45 +0000"
        }
      },
    ...</programlisting>
<simpara>Try another search for the geographic information derived from the IP address.
Replace $DATE with the current date, in YYYY.MM.DD format:</simpara>
<programlisting language="shell" linenumbering="unnumbered">curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&amp;q=geoip.city_name=Buffalo'</programlisting>
<simpara>A few log entries come from Buffalo, so the query produces the following response:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "took": 9,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 2.6390574,
    "hits": [
      {
        "_index": "logstash-2017.11.09",
        "_type": "doc",
        "_id": "L4zDnl8BW52sR0fx5whY",
        "_score": 2.6390574,
        "_source": {
          "request": "/blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29",
          "agent": """"Tiny Tiny RSS/1.11 (http://tt-rss.org/)"""",
          "geoip": {
            "timezone": "America/New_York",
            "ip": "198.46.149.143",
            "latitude": 42.8864,
            "continent_code": "NA",
            "city_name": "Buffalo",
            "country_name": "United States",
            "country_code2": "US",
            "dma_code": 514,
            "country_code3": "US",
            "region_name": "New York",
            "location": {
              "lon": -78.8781,
              "lat": 42.8864
            },
            "postal_code": "14202",
            "region_code": "NY",
            "longitude": -78.8781
          },
          "offset": 22795,
          "auth": "-",
          "ident": "-",
          "verb": "GET",
          "prospector": {
            "type": "log"
          },
          "source": "/path/to/file/logstash-tutorial.log",
          "message": """198.46.149.143 - - [04/Jan/2015:05:29:13 +0000] "GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1" 200 9316 "-" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"""",
          "tags": [
            "beats_input_codec_plain_applied"
          ],
          "referrer": """"-"""",
          "@timestamp": "2017-11-09T03:11:35.321Z",
          "response": "200",
          "bytes": "9316",
          "clientip": "198.46.149.143",
          "@version": "1",
          "beat": {
            "name": "My-MacBook-Pro.local",
            "hostname": "My-MacBook-Pro.local",
            "version": "6.0.0"
          },
          "host": "My-MacBook-Pro.local",
          "httpversion": "1.1",
          "timestamp": "04/Jan/2015:05:29:13 +0000"
        }
      },
     ...</programlisting>
<simpara>If you are using Kibana to visualize your data, you can also explore the Filebeat data in Kibana:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/kibana-filebeat-data.png"/>
  </imageobject>
  <textobject><phrase>Discovering Filebeat data in Kibana</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>See the <ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-getting-started.html">Filebeat getting started docs</ulink> for info about loading the Kibana
index pattern for Filebeat.</simpara>
<simpara>You&#8217;ve successfully created a pipeline that uses Filebeat to take Apache web logs as input, parses those logs to
create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Next, you
learn how to create a pipeline that uses multiple input and output plugins.</simpara>
</section>
</section>
<section id="multiple-input-output-plugins">
<title>Stitching Together Multiple Input and Output Plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></title>
<simpara>The information you need to manage often comes from several disparate sources, and use cases can require multiple
destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
requirements.</simpara>
<simpara>In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
sends the information to an Elasticsearch cluster as well as writing the information directly to a file.</simpara>
<bridgehead id="twitter-configuration" renderas="sect3">Reading from a Twitter Feed<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>To add a Twitter feed, you use the <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-twitter.html"><literal>twitter</literal></ulink> input plugin. To
configure the plugin, you need several pieces of information:</simpara>
<itemizedlist>
<listitem>
<simpara>
A <emphasis>consumer key</emphasis>, which uniquely identifies your Twitter app.
</simpara>
</listitem>
<listitem>
<simpara>
A <emphasis>consumer secret</emphasis>, which serves as the password for your Twitter app.
</simpara>
</listitem>
<listitem>
<simpara>
One or more <emphasis>keywords</emphasis> to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
</simpara>
</listitem>
<listitem>
<simpara>
An <emphasis>oauth token</emphasis>, which identifies the Twitter account using this app.
</simpara>
</listitem>
<listitem>
<simpara>
An <emphasis>oauth token secret</emphasis>, which serves as the password of the Twitter account.
</simpara>
</listitem>
</itemizedlist>
<simpara>Visit <ulink url="https://dev.twitter.com/apps">https://dev.twitter.com/apps</ulink> to set up a Twitter account and generate your consumer
key and secret, as well as your access token and secret. See the docs for the <ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-twitter.html"><literal>twitter</literal></ulink> input plugin if you&#8217;re not sure how to generate these keys.</simpara>
<simpara>Like you did earlier when you worked on <xref linkend="advanced-pipeline"/>, create a config file (called <literal>second-pipeline.conf</literal>) that
contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
sure you pass in the correct config file name when you run Logstash.</simpara>
<simpara>Add the following lines to the <literal>input</literal> section of the <literal>second-pipeline.conf</literal> file, substituting your values for the
placeholder values shown here:</simpara>
<programlisting language="json" linenumbering="unnumbered">    twitter {
        consumer_key =&gt; "enter_your_consumer_key_here"
        consumer_secret =&gt; "enter_your_secret_here"
        keywords =&gt; ["cloud"]
        oauth_token =&gt; "enter_your_access_token_here"
        oauth_token_secret =&gt; "enter_your_access_token_secret_here"
    }</programlisting>
<bridgehead id="configuring-lsf" renderas="sect3">Configuring Filebeat to Send Log Lines to Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>As you learned earlier in <xref linkend="configuring-filebeat"/>, the <ulink url="https://github.com/elastic/beats/tree/master/filebeat">Filebeat</ulink>
client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your
Logstash instance for processing.</simpara>
<simpara>After installing Filebeat, you need to configure it. Open the <literal>filebeat.yml</literal> file located in your Filebeat installation
directory, and replace the contents with the following lines. Make sure <literal>paths</literal> points to your syslog:</simpara>
<programlisting language="shell" linenumbering="unnumbered">filebeat.prospectors:
- type: log
  paths:
    - /var/log/*.log <co id="CO2-1"/>
  fields:
    type: syslog <co id="CO2-2"/>
output.logstash:
  hosts: ["localhost:5043"]</programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>
Absolute path to the file or files that Filebeat processes.
</para>
</callout>
<callout arearefs="CO2-2">
<para>
Adds a field called <literal>type</literal> with the value <literal>syslog</literal> to the event.
</para>
</callout>
</calloutlist>
<simpara>Save your changes.</simpara>
<simpara>To keep the configuration simple, you won&#8217;t specify TLS/SSL settings as you would in a real world
scenario.</simpara>
<simpara>Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the <literal>input</literal> section
of the <literal>second-pipeline.conf</literal> file:</simpara>
<programlisting language="json" linenumbering="unnumbered">    beats {
        port =&gt; "5043"
    }</programlisting>
<bridgehead id="logstash-file-output" renderas="sect3">Writing Logstash Data to a File<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>You can configure your Logstash pipeline to write data directly to a file with the
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-file.html"><literal>file</literal></ulink> output plugin.</simpara>
<simpara>Configure your Logstash instance to use the <literal>file</literal> output plugin by adding the following lines to the <literal>output</literal> section
of the <literal>second-pipeline.conf</literal> file:</simpara>
<programlisting language="json" linenumbering="unnumbered">    file {
        path =&gt; "/path/to/target/file"
    }</programlisting>
<bridgehead id="multiple-es-nodes" renderas="sect3">Writing to Multiple Elasticsearch Nodes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
providing redundant points of entry into the cluster when a particular node is unavailable.</simpara>
<simpara>To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the <literal>output</literal> section of the <literal>second-pipeline.conf</literal> file to read:</simpara>
<programlisting language="json" linenumbering="unnumbered">output {
    elasticsearch {
        hosts =&gt; ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
    }
}</programlisting>
<simpara>Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the <literal>hosts</literal>
parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
the default port for Elasticsearch is <literal>9200</literal> and can be omitted in the configuration above.</simpara>
<bridgehead id="testing-second-pipeline" renderas="sect4">Testing the Pipeline<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc">Edit me</ulink></bridgehead>
<simpara>At this point, your <literal>second-pipeline.conf</literal> file looks like this:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
    twitter {
        consumer_key =&gt; "enter_your_consumer_key_here"
        consumer_secret =&gt; "enter_your_secret_here"
        keywords =&gt; ["cloud"]
        oauth_token =&gt; "enter_your_access_token_here"
        oauth_token_secret =&gt; "enter_your_access_token_secret_here"
    }
    beats {
        port =&gt; "5043"
    }
}
output {
    elasticsearch {
        hosts =&gt; ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
    }
    file {
        path =&gt; "/path/to/target/file"
    }
}</programlisting>
<simpara>Logstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and
indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.</simpara>
<simpara>At the data source machine, run Filebeat with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">sudo ./filebeat -e -c filebeat.yml -d "publish"</programlisting>
<simpara>Filebeat will attempt to connect on port 5043. Until Logstash starts with an active Beats plugin, there
won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.</simpara>
<simpara>To verify your configuration, run the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f second-pipeline.conf --config.test_and_exit</programlisting>
<simpara>The <literal>--config.test_and_exit</literal> option parses your configuration file and reports any errors. When the configuration file
passes the configuration test, start Logstash with the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f second-pipeline.conf</programlisting>
<simpara>Use the <literal>grep</literal> utility to search in the target file to verify that information is present:</simpara>
<programlisting language="shell" linenumbering="unnumbered">grep syslog /path/to/target/file</programlisting>
<simpara>Run an Elasticsearch query to find the same information in the Elasticsearch cluster:</simpara>
<programlisting language="shell" linenumbering="unnumbered">curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&amp;q=fields.type:syslog'</programlisting>
<simpara>Replace $DATE with the current date, in YYYY.MM.DD format.</simpara>
<simpara>To see data from the Twitter feed, try this query:</simpara>
<programlisting language="shell" linenumbering="unnumbered">curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&amp;q=client:iphone'</programlisting>
<simpara>Again, remember to replace $DATE with the current date, in YYYY.MM.DD format.</simpara>
<remark> Processing Pipeline</remark>
</section>
</chapter>
<chapter id="pipeline">
<title>How Logstash Works<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></title>
<simpara>The Logstash event processing pipeline has three stages: inputs &#8594; filters &#8594;
outputs. Inputs generate events, filters modify them, and outputs ship them
elsewhere. Inputs and outputs support codecs that enable you to encode or decode
the data as it enters or exits the pipeline without having to use a separate
filter.</simpara>
<bridgehead id="_inputs" renderas="sect2">Inputs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></bridgehead>
<simpara>You use inputs to get data into Logstash. Some of the more commonly-used inputs
are:</simpara>
<itemizedlist>
<listitem>
<simpara>
<emphasis role="strong">file</emphasis>: reads from a file on the filesystem, much like the UNIX command
<literal>tail -0F</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">syslog</emphasis>: listens on the well-known port 514 for syslog messages and parses
according to the RFC3164 format
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">redis</emphasis>: reads from a redis server, using both redis channels and redis lists.
Redis is often used as a "broker" in a centralized Logstash installation, which
queues Logstash events from remote Logstash "shippers".
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">beats</emphasis>: processes events sent by <ulink url="https://www.elastic.co/downloads/beats/filebeat">Filebeat</ulink>.
</simpara>
</listitem>
</itemizedlist>
<simpara>For more information about the available inputs, see
<link linkend="input-plugins">Input Plugins</link>.</simpara>
<bridgehead id="_filters" renderas="sect2">Filters<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></bridgehead>
<simpara>Filters are intermediary processing devices in the Logstash pipeline. You can
combine filters with conditionals to perform an action on an event if it meets
certain criteria. Some useful filters include:</simpara>
<itemizedlist>
<listitem>
<simpara>
<emphasis role="strong">grok</emphasis>: parse and structure arbitrary text. Grok is currently the best way in
Logstash to parse unstructured log data into something structured and queryable.
With 120 patterns built-in to Logstash, it&#8217;s more than likely you&#8217;ll find one
that meets your needs!
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">mutate</emphasis>: perform general transformations on event fields. You can rename,
remove, replace, and modify fields in your events.
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">drop</emphasis>: drop an event completely, for example, <emphasis>debug</emphasis> events.
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">clone</emphasis>: make a copy of an event, possibly adding or removing fields.
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">geoip</emphasis>: add information about geographical location of IP addresses (also
displays amazing charts in Kibana!)
</simpara>
</listitem>
</itemizedlist>
<simpara>For more information about the available filters, see
<link linkend="filter-plugins">Filter Plugins</link>.</simpara>
<bridgehead id="_outputs" renderas="sect2">Outputs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></bridgehead>
<simpara>Outputs are the final phase of the Logstash pipeline. An event can pass through
multiple outputs, but once all output processing is complete, the event has
finished its execution. Some commonly used outputs include:</simpara>
<itemizedlist>
<listitem>
<simpara>
<emphasis role="strong">elasticsearch</emphasis>: send event data to Elasticsearch. If you&#8217;re planning to save
your data in an efficient, convenient, and easily queryable format&#8230;
Elasticsearch is the way to go. Period. Yes, we&#8217;re biased :)
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">file</emphasis>: write event data to a file on disk.
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">graphite</emphasis>: send event data to graphite, a popular open source tool for
storing and graphing metrics. <ulink url="http://graphite.readthedocs.io/en/latest/">http://graphite.readthedocs.io/en/latest/</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">statsd</emphasis>: send event data to statsd, a service that "listens for statistics,
like counters and timers, sent over UDP and sends aggregates to one or more
pluggable backend services". If you&#8217;re already using statsd, this could be
useful for you!
</simpara>
</listitem>
</itemizedlist>
<simpara>For more information about the available outputs, see
<link linkend="output-plugins">Output Plugins</link>.</simpara>
<bridgehead id="_codecs" renderas="sect2">Codecs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></bridgehead>
<simpara>Codecs are basically stream filters that can operate as part of an input or
output. Codecs enable you to easily separate the transport of your messages from
the serialization process. Popular codecs include <literal>json</literal>, <literal>msgpack</literal>, and <literal>plain</literal>
(text).</simpara>
<itemizedlist>
<listitem>
<simpara>
<emphasis role="strong">json</emphasis>: encode or decode data in the JSON format.
</simpara>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">multiline</emphasis>: merge multiple-line text events such as java exception and
stacktrace messages into a single event.
</simpara>
</listitem>
</itemizedlist>
<simpara>For more information about the available codecs, see
<link linkend="codec-plugins">Codec Plugins</link>.</simpara>
<section id="execution-model">
<title>Execution Model<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc">Edit me</ulink></title>
<simpara>The Logstash event processing pipeline coordinates the execution of inputs,
filters, and outputs.</simpara>
<simpara>Each input stage in the Logstash pipeline runs in its own thread. Inputs write events to a common Java <ulink url="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/SynchronousQueue.html">SynchronousQueue</ulink>. This queue holds no events, instead transferring each pushed event to a free worker, blocking if all workers are busy. Each pipeline worker thread takes a batch of events off this queue, creating a buffer per worker, runs the batch of events through the configured filters, then runs the filtered events through any outputs. The size of the batch and number of pipeline worker threads are configurable (see <xref linkend="tuning-logstash"/>).</simpara>
<simpara>By default, Logstash uses in-memory bounded queues between pipeline stages
(input → filter and filter → output) to buffer events. If Logstash terminates
unsafely, any events that are stored in memory will be lost. To prevent data
loss, you can enable Logstash to persist in-flight events to disk. See
<xref linkend="persistent-queues"/> for more information.</simpara>
<remark> Lostash setup</remark>
</section>
</chapter>
<chapter id="setup-logstash">
<title>Setting Up and Running Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>Before reading this section, see <xref linkend="installing-logstash"/> for basic installation instructions to get you started.</simpara>
<simpara>This section includes additional information on how to set up and run Logstash, including:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="dir-layout"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="config-setting-files"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="logstash-settings-file"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="running-logstash-command-line"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="running-logstash"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="docker"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="logging"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="persistent-queues"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="shutdown"/>
</simpara>
</listitem>
</itemizedlist>
<section id="dir-layout">
<title>Logstash Directory Layout<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>This section describes the default directory structure that is created when you unpack the Logstash installation packages.</simpara>
<section id="zip-targz-layout">
<title>Directory Layout of <literal>.zip</literal> and <literal>.tar.gz</literal> Archives<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>The <literal>.zip</literal> and <literal>.tar.gz</literal> packages are entirely self-contained. All files and
directories are, by default, contained within the home directory&#8201;&#8212;&#8201;the directory
created when unpacking the archive.</simpara>
<simpara>This is very convenient because you don&#8217;t have to create any directories to start using Logstash, and uninstalling
Logstash is as easy as removing the home directory.  However, it is advisable to change the default locations of the
config and the logs directories so that you do not delete important data later on.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top"> Type </entry>
<entry align="left" valign="top"> Description </entry>
<entry align="left" valign="top"> Default Location </entry>
<entry align="left" valign="top"> Setting</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">home</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Home directory of the Logstash installation.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}</literal>- Directory created by unpacking the archive</literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">bin</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Binary scripts, including <literal>logstash</literal> to start Logstash
    and <literal>logstash-plugin</literal> to install plugins</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}/bin</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">settings</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration files, including <literal>logstash.yml</literal> and <literal>jvm.options</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}/config</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.settings</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">logs</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Log files</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}/logs</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.logs</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">plugins</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}/plugins</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.plugins</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">data</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Data files used by logstash and its plugins for any persistent needs.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>{extract.path}/data</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.data</literal></literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="deb-layout">
<title>Directory Layout of Debian and RPM Packages<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>The Debian package and the RPM package each place config files, logs, and the settings files in the appropriate
locations for the system:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top"> Type </entry>
<entry align="left" valign="top"> Description </entry>
<entry align="left" valign="top"> Default Location </entry>
<entry align="left" valign="top"> Setting</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">home</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Home directory of the Logstash installation.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">bin</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Binary scripts including <literal>logstash</literal> to start Logstash
    and <literal>logstash-plugin</literal> to install plugins</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/bin</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">settings</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration files, including <literal>logstash.yml</literal>, <literal>jvm.options</literal>, and <literal>startup.options</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/etc/logstash</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.settings</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">conf</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Logstash pipeline configuration files</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/etc/logstash/conf.d</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.config</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">logs</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Log files</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/var/log/logstash</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.logs</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">plugins</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/plugins</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.plugins</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">data</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Data files used by logstash and its plugins for any persistent needs.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/var/lib/logstash</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.data</literal></literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="docker-layout">
<title>Directory Layout of Docker Images<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>The Docker images are created from the <literal>.tar.gz</literal> packages, and follow a
similar directory layout.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top"> Type </entry>
<entry align="left" valign="top"> Description </entry>
<entry align="left" valign="top"> Default Location </entry>
<entry align="left" valign="top"> Setting</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">home</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Home directory of the Logstash installation.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">bin</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Binary scripts, including <literal>logstash</literal> to start Logstash
    and <literal>logstash-plugin</literal> to install plugins</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/bin</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">settings</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration files, including <literal>logstash.yml</literal> and <literal>jvm.options</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/config</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.settings</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">conf</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Logstash pipeline configuration files</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/pipeline</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.config</literal></literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">plugins</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>/usr/share/logstash/plugins</literal></literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal><literal>path.plugins</literal></literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note><simpara>Logstash Docker containers do not create log files by default. They log
to standard output.</simpara></note>
</section>
</section>
<section id="config-setting-files">
<title>Logstash Configuration Files<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>Logstash has two types of configuration files: <emphasis>pipeline configuration files</emphasis>, which define the Logstash processing
pipeline, and <emphasis>settings files</emphasis>, which specify options that control Logstash startup and execution.</simpara>
<section id="_pipeline_configuration_files">
<title>Pipeline Configuration Files<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and
rpm, you place the pipeline configuration files in the <literal>/etc/logstash/conf.d</literal> directory. Logstash tries to load only
files with <literal>.conf</literal> extension in the <literal>/etc/logstash/conf.d directory</literal> and ignores all other files.</simpara>
<simpara>See <xref linkend="configuration"/> for more info.</simpara>
</section>
<section id="_settings_files">
<title>Settings Files<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc">Edit me</ulink></title>
<simpara>The settings files are already defined in the Logstash installation. Logstash includes the following settings files:</simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>logstash.yml</literal></emphasis>
</term>
<listitem>
<simpara>
  Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command
  line. Any flags that you set at the command line override the corresponding settings in the <literal>logstash.yml</literal> file. See <xref linkend="logstash-settings-file"/> for more info.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>jvm.options</literal></emphasis>
</term>
<listitem>
<simpara>
  Contains JVM configuration flags. Specify each flag on a separate line. You can also use this file to set the locale
  for Logstash.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>startup.options</literal> (Linux)</emphasis>
</term>
<listitem>
<simpara>
  Contains options used by the <literal>system-install</literal> script in <literal>/usr/share/logstash/bin</literal> to build the appropriate startup
  script for your system. When you install the Logstash package, the <literal>system-install</literal> script executes at the end of the
  installation process and uses the settings specified in <literal>startup.options</literal> to set options such as the user, group,
  service name, and service description. By default, Logstash services are installed under the user <literal>logstash</literal>. The <literal>startup.options</literal> file makes it easier for you to install multiple instances of the Logstash service. You can copy
  the file and change the values for specific settings. Note that the <literal>startup.options</literal> file is not read at startup. If
  you want to change the Logstash startup script (for example, to change the Logstash user or read from a different
  configuration path), you must re-run the <literal>system-install</literal> script (as root) to pass in the new settings.
</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section id="logstash-settings-file">
<title>Settings File<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/settings-file.asciidoc">Edit me</ulink></title>
<simpara>You can set options in the Logstash settings file, <literal>logstash.yml</literal>, to control Logstash execution. For example,
you can specify pipeline settings, the location of configuration files, logging options, and other settings.
Most of the settings in the <literal>logstash.yml</literal> file are also available as <link linkend="command-line-flags">command-line flags</link>
when you run Logstash. Any flags that you set at the command line override the corresponding settings in the
<literal>logstash.yml</literal> file.</simpara>
<simpara>The <literal>logstash.yml</literal> file is written in <ulink url="http://yaml.org/">YAML</ulink>. Its location varies by platform (see
<xref linkend="dir-layout"/>). You can specify settings in hierarchical form or use flat keys. For example, to use
hierarchical form to set the pipeline batch size and batch delay, you specify:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">pipeline:
  batch:
    size: 125
    delay: 5</programlisting>
<simpara>To express the same values as flat keys, you specify:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">pipeline.batch.size: 125
pipeline.batch.delay: 5</programlisting>
<simpara>The <literal>logstash.yml</literal> file also supports bash-style interpolation of environment variables in
setting values.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">pipeline:
  batch:
    size: ${BATCH_SIZE}
    delay: ${BATCH_DELAY:5}
node:
  name: "node_${LS_NODE_NAME}"
path:
   queue: "/tmp/${QUEUE_DIR:queue}"</programlisting>
<simpara>Note that the <literal>${VAR_NAME:default_value}</literal> notation is supported, setting a default batch delay
of <literal>5</literal> and a default <literal>path.queue</literal> of <literal>/tmp/queue</literal> in the above example.</simpara>
<simpara>Modules may also be specified in the <literal>logstash.yml</literal> file. The modules definition will have
this format:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">modules:
  - name: MODULE_NAME1
    var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE
    var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE
    var.PLUGIN_TYPE2.PLUGIN_NAME2.KEY1: VALUE
    var.PLUGIN_TYPE3.PLUGIN_NAME3.KEY1: VALUE
  - name: MODULE_NAME2
    var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE
    var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE</programlisting>
<important><simpara>If the <link linkend="command-line-flags">command-line flag</link> <literal>--modules</literal> is used, any modules defined in the <literal>logstash.yml</literal> file will be ignored.</simpara></important>
<simpara>The <literal>logstash.yml</literal> file includes the following settings:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top"> Setting </entry>
<entry align="left" valign="top"> Description </entry>
<entry align="left" valign="top"> Default value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>node.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A descriptive name for the node.</simpara></entry>
<entry align="left" valign="top"><simpara>Machine&#8217;s hostname</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.data</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The directory that Logstash and its plugins use for any persistent needs.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>LOGSTASH_HOME/data</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pipeline.workers</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The number of workers that will, in parallel, execute the filter and output stages of the pipeline.
  If you find that events are backing up, or that the
  CPU is not saturated, consider increasing this number to better utilize machine processing power.</simpara></entry>
<entry align="left" valign="top"><simpara>Number of the host&#8217;s CPU cores</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pipeline.batch.size</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of events an individual worker thread will collect from inputs
  before attempting to execute its filters and outputs.
  Larger batch sizes are generally more efficient, but come at the cost of increased memory
  overhead. You may have to increase the JVM heap size by setting the <literal>LS_HEAP_SIZE</literal>
  variable to effectively use the option.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>125</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pipeline.batch.delay</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When creating pipeline event batches, how long in milliseconds to wait for
  each event before dispatching an undersized batch to pipeline workers.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>5</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pipeline.unsafe_shutdown</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, forces Logstash to exit during shutdown even if there are still inflight events
  in memory. By default, Logstash will refuse to quit until all received events
  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.config</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,
  config files are read from the directory in alphabetical order.</simpara></entry>
<entry align="left" valign="top"><simpara>Platform-specific. See <xref linkend="dir-layout"/>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as
  the config file.</simpara></entry>
<entry align="left" valign="top"><simpara>None</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.test_and_exit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, checks that the configuration is valid and then exits. Note that grok patterns are not checked for
  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this
  setting with <literal>log.level: debug</literal>, Logstash will log the combined config file, annotating
  each config block with the source file it came from.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.reload.automatic</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, periodically checks if the configuration has changed and reloads the configuration whenever it is changed.
  This can also be triggered manually through the SIGHUP signal.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.reload.interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>How often in seconds Logstash checks the config files for changes.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>3s</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.debug</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, shows the fully compiled configuration as a debug log message. You must also set <literal>log.level: debug</literal>.
  WARNING: The log message will include any <emphasis>password</emphasis> options passed to plugin configs as plaintext, and may result
  in plaintext passwords appearing in your logs!</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.support_escapes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, quoted strings will process the following escape sequences: <literal>\n</literal> becomes a literal newline (ASCII 10). <literal>\r</literal> becomes a literal carriage return (ASCII 13). <literal>\t</literal> becomes a literal tab (ASCII 9). <literal>\\</literal> becomes a literal backslash <literal>\</literal>. <literal>\"</literal> becomes a literal double quotation mark. <literal>\'</literal> becomes a literal quotation mark.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>modules</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When configured, <literal>modules</literal> must be in the nested YAML structure described above this table.</simpara></entry>
<entry align="left" valign="top"><simpara>None</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.type</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The internal queuing model to use for event buffering. Specify <literal>memory</literal> for legacy in-memory based queuing, or <literal>persisted</literal> for disk-based ACKed queueing (<link linkend="persistent-queues">persistent queues</link>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>memory</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.queue</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The directory path where the data files will be stored when persistent queues are enabled (<literal>queue.type: persisted</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>path.data/queue</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.page_capacity</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The size of the page data files used when persistent queues are enabled (<literal>queue.type: persisted</literal>). The queue data consists of append-only data files separated into pages.</simpara></entry>
<entry align="left" valign="top"><simpara>250mb</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.max_events</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of unread events in the queue when persistent queues are enabled (<literal>queue.type: persisted</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara>0 (unlimited)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.max_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both <literal>queue.max_events</literal> and <literal>queue.max_bytes</literal> are specified, Logstash uses whichever criteria is reached first.</simpara></entry>
<entry align="left" valign="top"><simpara>1024mb (1g)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.checkpoint.acks</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (<literal>queue.type: persisted</literal>). Specify <literal>queue.checkpoint.acks: 0</literal> to set this value to unlimited.</simpara></entry>
<entry align="left" valign="top"><simpara>1024</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.checkpoint.writes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum number of written events before forcing a checkpoint when persistent queues are enabled (<literal>queue.type: persisted</literal>). Specify <literal>queue.checkpoint.writes: 0</literal> to set this value to unlimited.</simpara></entry>
<entry align="left" valign="top"><simpara>1024</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.checkpoint.interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The interval in milliseconds when a checkpoint is forced on the head page when persistent queues are enabled (<literal>queue.type: persisted</literal>). Specify <literal>queue.checkpoint.interval: 0</literal> for no periodic checkpoint.</simpara></entry>
<entry align="left" valign="top"><simpara>1000</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>queue.drain</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When enabled, Logstash waits until the persistent queue is drained before shutting down.</simpara></entry>
<entry align="left" valign="top"><simpara>false</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>dead_letter_queue.enable</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Flag to instruct Logstash to enable the DLQ feature supported by plugins.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>false</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>dead_letter_queue.max_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum size of each dead letter queue. Entries will be dropped if they
  would increase the size of the dead letter queue beyond this setting.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>1024mb</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.dead_letter_queue</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The directory path where the data files will be stored for the dead-letter queue.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>path.data/dead_letter_queue</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>http.host</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The bind address for the metrics REST endpoint.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>"127.0.0.1"</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>http.port</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The bind port for the metrics REST endpoint.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>9600</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>log.level</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The log level. Valid options are:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>fatal</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>error</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>warn</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>info</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>debug</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>trace</literal>
</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara><literal>info</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>log.format</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The log format. Set to <literal>json</literal> to log in JSON format, or <literal>plain</literal> to use <literal>Object#.inspect</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>plain</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.logs</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The directory where Logstash will write its log to.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>LOGSTASH_HOME/logs</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>path.plugins</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Where to find custom plugins. You can specify this setting multiple times to include
  multiple paths. Plugins are expected to be in a specific directory hierarchy:
  <literal>PATH/logstash/TYPE/NAME.rb</literal> where <literal>TYPE</literal> is <literal>inputs</literal>, <literal>filters</literal>, <literal>outputs</literal>, or <literal>codecs</literal>,
  and <literal>NAME</literal> is the name of the plugin.</simpara></entry>
<entry align="left" valign="top"><simpara>Platform-specific. See <xref linkend="dir-layout"/>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="running-logstash-command-line">
<title>Running Logstash from the Command Line<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash-command-line.asciidoc">Edit me</ulink></title>
<simpara>To run Logstash from the command line, use the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash [options]</programlisting>
<simpara>Where <literal>options</literal> are <link linkend="command-line-flags">command-line</link> flags that you can
specify to control Logstash execution. The location of the <literal>bin</literal> directory
varies by platform. See <xref linkend="dir-layout"/> to find the location of <literal>bin\logstash</literal> on
your system.</simpara>
<simpara>The following example runs Logstash and loads the Logstash config defined in
the <literal>mypipeline.conf</literal> file:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f mypipeline.conf</programlisting>
<simpara>Any flags that you set at the command line override the corresponding settings
in the Logstash <link linkend="logstash-settings-file">settings file</link>, but the settings file
itself is not changed. It remains as-is for subsequent Logstash runs.</simpara>
<simpara>Specifying command line options is useful when you are testing Logstash.
However, in a production environment, we recommend that you use the Logstash
<link linkend="logstash-settings-file">settings file</link> to control Logstash execution. Using
the settings file makes it easier for you to specify multiple options, and it
provides you with a single, versionable file that you can use to start up
Logstash consistently for each run.</simpara>
<section id="command-line-flags">
<title>Command-Line Flags<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash-command-line.asciidoc">Edit me</ulink></title>
<simpara>Logstash has the following flags. You can use the <literal>--help</literal> flag to display this information.</simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>--node.name NAME</literal></emphasis>
</term>
<listitem>
<simpara>
  Specify the name of this Logstash instance. If no value is given it will default to the current
  hostname.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-f, --path.config CONFIG_PATH</literal></emphasis>
</term>
<listitem>
<simpara>
Load the Logstash config from a specific file or directory. If a directory is given, all
files in that directory will be concatenated in lexicographical order and then parsed as a
single config file. Specifying this flag multiple times is not supported. If you specify
this flag multiple times, Logstash uses the last occurrence (for example, <literal>-f foo -f bar</literal>
is the same as <literal>-f bar</literal>).
</simpara>
<simpara>You can specify wildcards (<link linkend="glob-support">globs</link>) and any matched files will
be loaded in the order described above. For example, you can use the wildcard feature to
load specific files by name:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --debug -f '/tmp/{one,two,three}'</programlisting>
<simpara>With this command, Logstash concatenates three config files, <literal>/tmp/one</literal>, <literal>/tmp/two</literal>, and
<literal>/tmp/three</literal>, and parses them into a single config.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-e, --config.string CONFIG_STRING</literal></emphasis>
</term>
<listitem>
<simpara>
  Use the given string as the configuration data. Same syntax as the config file. If no
  input is specified, then the following is used as the default input:
  <literal>input { stdin { type =&gt; stdin } }</literal> and if no output is specified, then the
  following is used as the default output: <literal>output { stdout { codec =&gt; rubydebug } }</literal>.
  If you wish to use both defaults, please use the empty string for the <literal>-e</literal> flag.
  The default is nil.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--modules</literal></emphasis>
</term>
<listitem>
<simpara>
  Launch the named module.  Works in conjunction with the <literal>-M</literal> option to assign values to
  default variables for the specified module.  If <literal>--modules</literal> is used on the command line,
  any modules in <literal>logstash.yml</literal> will be ignored, as will any settings there.  This flag is
  mutually exclusive to the <literal>-f</literal> and <literal>-e</literal> flags.  Only one of <literal>-f</literal>, <literal>-e</literal>, or <literal>--modules</literal> may
  be specified.  Multiple modules can be specified by separating them with a comma, or by
  invoking the <literal>--modules</literal> flag multiple times.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-M, --modules.variable</literal></emphasis>
</term>
<listitem>
<simpara>
  Assign a value to a configurable option for a module.  The format for assigning variables is
  <literal>-M "MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.KEY_NAME=value"</literal> for Logstash variables. For other
  settings, it will be <literal>-M "MODULE_NAME.KEY_NAME.SUB_KEYNAME=value"</literal>.  The <literal>-M</literal> flag can be used
  as many times as is necessary. If no <literal>-M</literal> options are specified, then the default value for
  that setting will be used.  The <literal>-M</literal> flag is only used in conjunction with the <literal>--modules</literal>
  flag.  It will be ignored if the <literal>--modules</literal> flag is absent.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-w, --pipeline.workers COUNT</literal></emphasis>
</term>
<listitem>
<simpara>
  Sets the number of pipeline workers to run. This option sets the number of workers that will,
  in parallel, execute the filter and output stages of the pipeline. If you find that events are
  backing up, or that  the CPU is not saturated, consider increasing this number to better utilize
  machine processing power. The default is the number of the host&#8217;s CPU cores.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-b, --pipeline.batch.size SIZE</literal></emphasis>
</term>
<listitem>
<simpara>
  Size of batches the pipeline is to work in. This option defines the maximum number of events an
  individual worker thread will collect from inputs before attempting to execute its filters and outputs.
  The default is 125 events. Larger batch sizes are generally more efficient, but come at the cost of
  increased memory overhead. You may have to increase the JVM heap size by setting the <literal>LS_HEAP_SIZE</literal>
  variable to effectively use the option.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-u, --pipeline.batch.delay DELAY_IN_MS</literal></emphasis>
</term>
<listitem>
<simpara>
  When creating pipeline batches, how long to wait while polling for the next event. This option defines
  how long in milliseconds to wait while polling for the next event before dispatching an undersized batch
  to filters and workers. The default is 250ms.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--pipeline.unsafe_shutdown</literal></emphasis>
</term>
<listitem>
<simpara>
  Force Logstash to exit during shutdown even if there are still inflight events
  in memory. By default, Logstash will refuse to quit until all received events
  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--path.data PATH</literal></emphasis>
</term>
<listitem>
<simpara>
  This should point to a writable directory. Logstash will use this directory whenever it needs to store
  data. Plugins will also have access to this path. The default is the <literal>data</literal> directory under
  Logstash home.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-p, --path.plugins PATH</literal></emphasis>
</term>
<listitem>
<simpara>
  A path of where to find custom plugins. This flag can be given multiple times to include
  multiple paths. Plugins are expected to be in a specific directory hierarchy:
  <literal>PATH/logstash/TYPE/NAME.rb</literal> where <literal>TYPE</literal> is <literal>inputs</literal>, <literal>filters</literal>, <literal>outputs</literal>, or <literal>codecs</literal>,
  and <literal>NAME</literal> is the name of the plugin.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-l, --path.logs PATH</literal></emphasis>
</term>
<listitem>
<simpara>
  Directory to write Logstash internal logs to.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--log.level LEVEL</literal></emphasis>
</term>
<listitem>
<simpara>
 Set the log level for Logstash. Possible values are:
</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>fatal</literal>: log very severe error messages that will usually be followed by the application aborting
</simpara>
</listitem>
<listitem>
<simpara>
<literal>error</literal>: log errors
</simpara>
</listitem>
<listitem>
<simpara>
<literal>warn</literal>: log warnings
</simpara>
</listitem>
<listitem>
<simpara>
<literal>info</literal>: log verbose info (this is the default)
</simpara>
</listitem>
<listitem>
<simpara>
<literal>debug</literal>: log debugging info (for developers)
</simpara>
</listitem>
<listitem>
<simpara>
<literal>trace</literal>: log finer-grained messages beyond debugging info
</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--config.debug</literal></emphasis>
</term>
<listitem>
<simpara>
  Show the fully compiled configuration as a debug log message (you must also have <literal>--log.level=debug</literal> enabled).
  WARNING: The log message will include any <emphasis>password</emphasis> options passed to plugin configs as plaintext, and may result
  in plaintext passwords appearing in your logs!
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-i, --interactive SHELL</literal></emphasis>
</term>
<listitem>
<simpara>
  Drop to shell instead of running as normal. Valid shells are "irb" and "pry".
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-V, --version</literal></emphasis>
</term>
<listitem>
<simpara>
  Emit the version of Logstash and its friends, then exit.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-t, --config.test_and_exit</literal></emphasis>
</term>
<listitem>
<simpara>
  Check configuration for valid syntax and then exit. Note that grok patterns are not checked for
  correctness with this flag. Logstash can read multiple config files from a directory. If you combine this
  flag with <literal>--log.level=debug</literal>, Logstash will log the combined config file, annotating
  each config block with the source file it came from.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-r, --config.reload.automatic</literal></emphasis>
</term>
<listitem>
<simpara>
  Monitor configuration changes and reload whenever the configuration is changed.
  NOTE: Use SIGHUP to manually reload the config. The default is false.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--config.reload.interval RELOAD_INTERVAL</literal></emphasis>
</term>
<listitem>
<simpara>
  How frequently to poll the configuration location for changes. The default value is "3s".
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--http.host HTTP_HOST</literal></emphasis>
</term>
<listitem>
<simpara>
  Web API binding host. This option specifies the bind address for the metrics REST endpoint. The default is "127.0.0.1".
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--http.port HTTP_PORT</literal></emphasis>
</term>
<listitem>
<simpara>
  Web API http port. This option specifies the bind port for the metrics REST endpoint. The default is 9600-9700.
  This setting accepts a range of the format 9600-9700. Logstash will pick up the first available port.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--log.format FORMAT</literal></emphasis>
</term>
<listitem>
<simpara>
   Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text
   (using Ruby&#8217;s Object#inspect). The default is "plain".
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>--path.settings SETTINGS_DIR</literal></emphasis>
</term>
<listitem>
<simpara>
  Set the directory containing the <literal>logstash.yml</literal> <link linkend="logstash-settings-file">settings file</link> as well
  as the log4j logging configuration. This can also be set through the LS_SETTINGS_DIR environment variable.
  The default is the <literal>config</literal> directory under Logstash home.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>-h, --help</literal></emphasis>
</term>
<listitem>
<simpara>
  Print help
</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section id="running-logstash">
<title>Running Logstash as a Service on Debian or RPM<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash.asciidoc">Edit me</ulink></title>
<simpara>Logstash is not started automatically after installation. How to start and stop Logstash depends on whether your system
uses systemd, upstart, or SysV.</simpara>
<simpara>Here are some common operating systems and versions, and the corresponding
startup styles they use.  This list is intended to be informative, not exhaustive.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Distribution</simpara></entry>
<entry align="left" valign="top"><simpara>Service System</simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Ubuntu 16.04 and newer</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-systemd">systemd</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Ubuntu 12.04 through 15.10</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-upstart">upstart</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Debian 8 "jessie" and newer</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-systemd">systemd</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Debian 7 "wheezy" and older</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-sysv">sysv</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>CentOS (and RHEL) 7 and newer</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-systemd">systemd</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>CentOS (and RHEL) 6</simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="running-logstash-upstart">upstart</link></simpara></entry>
<entry align="left" valign="top"><simpara></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<section id="running-logstash-systemd">
<title>Running Logstash by Using Systemd<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash.asciidoc">Edit me</ulink></title>
<simpara>Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the
<literal>systemctl</literal> command to start and stop services. Logstash places the systemd unit files in <literal>/etc/systemd/system</literal> for both deb and rpm. After installing the package, you can start up Logstash with:</simpara>
<programlisting language="sh" linenumbering="unnumbered">sudo systemctl start logstash.service</programlisting>
</section>
<section id="running-logstash-upstart">
<title>Running Logstash by Using Upstart<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash.asciidoc">Edit me</ulink></title>
<simpara>For systems that use upstart, you can start Logstash with:</simpara>
<programlisting language="sh" linenumbering="unnumbered">sudo initctl start logstash</programlisting>
<simpara>The auto-generated configuration file for upstart systems is <literal>/etc/init/logstash.conf</literal>.</simpara>
</section>
<section id="running-logstash-sysv">
<title>Running Logstash by Using SysV<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/running-logstash.asciidoc">Edit me</ulink></title>
<simpara>For systems that use SysV, you can start Logstash with:</simpara>
<programlisting language="sh" linenumbering="unnumbered">sudo /etc/init.d/logstash start</programlisting>
<simpara>The auto-generated configuration file for SysV systems is <literal>/etc/init.d/logstash</literal>.</simpara>
</section>
</section>
<section id="docker">
<title>Running Logstash on Docker<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Docker images for Logstash are available from the Elastic Docker
registry. The base image is <ulink url="https://hub.docker.com/_/centos/">centos:7</ulink>.</simpara>
<simpara>A list of all published Docker images and tags can be found at
<ulink url="https://www.docker.elastic.co">www.docker.elastic.co</ulink>. The source code can be
found on <ulink url="https://github.com/elastic/logstash-docker/tree/6.x">GitHub</ulink>.</simpara>
<section id="_image_types">
<title>Image types<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>The images are available in two different configurations or "flavors". The
<literal>x-pack</literal> flavor, which is the default, ships with X-Pack features
pre-installed. The <literal>oss</literal> flavor does not include X-Pack, and contains only
open source Logstash.</simpara>
<note><simpara><ulink url="https://www.elastic.co/guide/en/x-pack/current/index.html">X-Pack</ulink> is
pre-installed in the default image. With X-Pack installed, Logstash expects to
connect to an Elasticsearch cluster that is also running X-Pack, in order to
publish data for the <ulink url="https://www.elastic.co/guide/en/x-pack/6.x/xpack-monitoring.html">Monitoring</ulink> component.</simpara></note>
</section>
<section id="_pulling_the_image">
<title>Pulling the image<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Obtaining Logstash for Docker is as simple as issuing a <literal>docker
pull</literal> command against the Elastic Docker registry.</simpara>
<simpara>However, version 6.1.0 of Logstash has not yet been
released, so no Docker image is currently available for this version.</simpara>
</section>
</section>
<section id="_configuring_logstash_for_docker">
<title>Configuring Logstash for Docker<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Logstash differentiates between two types of configuration:
<link linkend="config-setting-files">Settings and Pipeline Configuration</link>.</simpara>
<section id="_pipeline_configuration">
<title>Pipeline Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>It is essential to place your pipeline configuration where it can be
found by Logstash. By default, the container will look in
<literal>/usr/share/logstash/pipeline/</literal> for pipeline configuration files.</simpara>
<simpara>In this example we use a bind-mounted volume to provide the
configuration via the <literal>docker run</literal> command:</simpara>
<programlisting language="sh" linenumbering="unnumbered">docker run --rm -it -v ~/pipeline/:/usr/share/logstash/pipeline/ docker.elastic.co/logstash/logstash:6.1.0</programlisting>
<simpara>Every file in the host directory <literal>~/pipeline/</literal> will then be parsed
by Logstash as pipeline configuration.</simpara>
<simpara>If you don&#8217;t provide configuration to Logstash, it will run with a
minimal config that listens for messages from the
<link linkend="plugins-inputs-beats">Beats input plugin</link> and echoes any that are
received to <literal>stdout</literal>. In this case, the startup logs will be similar
to the following:</simpara>
<programlisting language="text" linenumbering="unnumbered">Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties.
[2016-10-26T05:11:34,992][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=&gt;"0.0.0.0:5044"}
[2016-10-26T05:11:35,068][INFO ][logstash.pipeline        ] Starting pipeline {"id"=&gt;"main", "pipeline.workers"=&gt;4, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;5, "pipeline.max_inflight"=&gt;500}
[2016-10-26T05:11:35,078][INFO ][org.logstash.beats.Server] Starting server on port: 5044
[2016-10-26T05:11:35,078][INFO ][logstash.pipeline        ] Pipeline main started
[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}</programlisting>
<simpara>This is the default configuration for the image, defined in
<literal>/usr/share/logstash/pipeline/logstash.conf</literal>.  If this is the
behaviour that you are observing, ensure that your pipeline
configuration is being picked up correctly, and that you are replacing
either <literal>logstash.conf</literal> or the entire <literal>pipeline</literal> directory.</simpara>
</section>
<section id="_settings">
<title>Settings<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>The image provides several methods for configuring settings. The conventional
approach is to provide a custom <literal>logstash.yml</literal> file, but it&#8217;s
also possible to use environment variables to define settings.</simpara>
<section id="docker-bind-mount-settings">
<title>Bind-mounted settings files<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Settings files can also be provided through bind-mounts. Logstash
expects to find them at <literal>/usr/share/logstash/config/</literal>.</simpara>
<simpara>It&#8217;s possible to provide an entire directory containing all needed
files:</simpara>
<programlisting language="sh" linenumbering="unnumbered">docker run --rm -it -v ~/settings/:/usr/share/logstash/config/ docker.elastic.co/logstash/logstash:6.1.0</programlisting>
<simpara>Alternatively, a single file can be mounted:</simpara>
<programlisting language="sh" linenumbering="unnumbered">docker run --rm -it -v ~/settings/logstash.yml:/usr/share/logstash/config/logstash.yml docker.elastic.co/logstash/logstash:6.1.0</programlisting>
<note><simpara>Bind-mounted configuration files will retain the same permissions and
ownership within the container that they have on the host system. Be sure
to set permissions such that the files will be readable and, ideally, not
writeable by the container&#8217;s <literal>logstash</literal> user (UID 1000).</simpara></note>
</section>
<section id="_custom_images">
<title>Custom Images<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Bind-mounted configuration is not the only option, naturally. If you
prefer the <emphasis>Immutable Infrastructure</emphasis> approach, you can prepare a
custom image containing your configuration by using a <literal>Dockerfile</literal>
like this one:</simpara>
<programlisting language="dockerfile" linenumbering="unnumbered">FROM docker.elastic.co/logstash/logstash:6.1.0
RUN rm -f /usr/share/logstash/pipeline/logstash.conf
ADD pipeline/ /usr/share/logstash/pipeline/
ADD config/ /usr/share/logstash/config/</programlisting>
<simpara>Be sure to replace or delete <literal>logstash.conf</literal> in your custom image, so
that you don&#8217;t retain the example config from the base image.</simpara>
</section>
<section id="docker-env-config">
<title>Environment variable configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Under Docker, Logstash settings can be configured via environment
variables. When the container starts, a helper process checks the environment
for variables that can be mapped to Logstash settings. Settings that are found
in the environment are merged into <literal>logstash.yml</literal> as the container starts up.</simpara>
<simpara>For compatibility with container orchestration systems, these environment
variables are written in all capitals, with underscores as word
separators</simpara>
<simpara>Some example translations are shown here:</simpara>
<table tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><title>Example Docker Environment Variables</title><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<emphasis role="strong">Environment Variable</emphasis>
</simpara>
</entry>
<entry>
<simpara>
<emphasis role="strong">Logstash Setting</emphasis>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>PIPELINE_WORKERS</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>pipeline.workers</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>LOG_LEVEL</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>log.level</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>XPACK_MONITORING_ENABLED</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>xpack.monitoring.enabled</literal>
</simpara>
</entry>
</row>
</tbody></tgroup></table>
<simpara>In general, any setting listed in the <link linkend="logstash-settings-file">settings documentation</link> can be configured with this technique.</simpara>
<note><simpara>Defining settings with environment variables causes <literal>logstash.yml</literal> to
be modified in place. This behaviour is likely undesirable if <literal>logstash.yml</literal> was
bind-mounted from the host system. Thus, it is not reccomended to
combine the bind-mount technique with the environment variable technique. It
is best to choose a single method for defining Logstash settings.</simpara></note>
</section>
</section>
<section id="_docker_defaults">
<title>Docker defaults<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>The following settings have different default values when using the Docker
images:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<literal>http.host</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>0.0.0.0</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>path.config</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>/usr/share/logstash/pipeline</literal>
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>In the <literal>x-pack</literal> image, the following additional defaults are also set:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<literal>xpack.monitoring.elasticsearch.url</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>http://elasticsearch:9200</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>xpack.monitoring.elasticsearch.username</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>logstash_system</literal>
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>xpack.monitoring.elasticsearch.password</literal>
</simpara>
</entry>
<entry>
<simpara>
<literal>changeme</literal>
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>These settings are defined in the default <literal>logstash.yml</literal>. They can be overridden
with a <link linkend="docker-bind-mount-settings">custom <literal>logstash.yml</literal></link> or via
<link linkend="docker-env-config">environment variables</link>.</simpara>
<important><simpara>If replacing <literal>logstash.yml</literal> with a custom version, be sure to copy the
above defaults to the custom file if you want to retain them. If not, they will
be "masked" by the new file.</simpara></important>
</section>
<section id="_logging_configuration">
<title>Logging Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc">Edit me</ulink></title>
<simpara>Under Docker, Logstash logs go to standard output by default. To
change this behaviour, use any of the techniques above to replace the
file at <literal>/usr/share/logstash/config/log4j2.properties</literal>.</simpara>
</section>
</section>
<section id="logging">
<title>Logging<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc">Edit me</ulink></title>
<simpara>Logstash emits internal logs during its operation, which are placed in <literal>LS_HOME/logs</literal> (or <literal>/var/log/logstash</literal> for
DEB/RPM). The default logging level is <literal>INFO</literal>. Logstash&#8217;s logging framework is based on
<ulink url="http://logging.apache.org/log4j/2.x/">Log4j 2 framework</ulink>, and much of its functionality is exposed directly to users.</simpara>
<simpara>When debugging problems, particularly problems with plugins, it can be helpful to increase the logging level to <literal>DEBUG</literal>
to emit more verbose messages. Previously, you could only set a log level that applied to the entire Logstash product.
Starting with 5.0, you can configure logging for a particular subsystem in Logstash. For example, if you are
debugging issues with Elasticsearch Output, you can increase log levels just for that component. This way
you can reduce noise due to excessive logging and focus on the problem area effectively.</simpara>
<section id="_log_file_location">
<title>Log file location<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc">Edit me</ulink></title>
<simpara>You can specify the log file location using <literal>--path.logs</literal> setting.</simpara>
</section>
<section id="_log4j_2_configuration">
<title>Log4j 2 Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc">Edit me</ulink></title>
<simpara>Logstash ships with a <literal>log4j2.properties</literal> file with out-of-the-box settings. You  can modify this file directly to change the
rotation policy, type, and other <ulink url="https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers">log4j2 configuration</ulink>.
You must restart Logstash to apply any changes that you make to this file.</simpara>
</section>
<section id="_slowlog">
<title>Slowlog<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc">Edit me</ulink></title>
<simpara>Slow-log for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
through the pipeline. Just like the normal application log, you can find slow-logs in your <literal>--path.logs</literal> directory.
Slowlog is configured in the <literal>logstash.yml</literal> settings file with the following options:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">slowlog.threshold.warn (default: -1)
slowlog.threshold.info (default: -1)
slowlog.threshold.debug (default: -1)
slowlog.threshold.trace (default: -1)</programlisting>
<simpara>By default, these values are set to <literal>-1nanos</literal> to represent an infinite threshold where no slowlog will be invoked. These <literal>slowlog.threshold</literal>
fields are configured using a time-value format which enables a wide range of trigger intervals. The positive numeric ranges
can be specified using the following time units: <literal>nanos</literal> (nanoseconds), <literal>micros</literal> (microseconds), <literal>ms</literal> (milliseconds), <literal>s</literal> (second), <literal>m</literal> (minute),
<literal>h</literal> (hour), <literal>d</literal> (day).</simpara>
<simpara>Here is an example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">slowlog.threshold.warn: 2s
slowlog.threshold.info: 1s
slowlog.threshold.debug: 500ms
slowlog.threshold.trace: 100ms</programlisting>
<simpara>In the above configuration, events that take longer than two seconds to be processed within a filter will be logged.
The logs will include the full event and filter configuration that are responsible for the slowness.</simpara>
</section>
<section id="_logging_apis">
<title>Logging APIs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc">Edit me</ulink></title>
<simpara>You could modify the <literal>log4j2.properties</literal> file and restart your Logstash, but that is both tedious and leads to unnecessary
downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective
immediately and do not need a restart.</simpara>
<note><simpara>By default, the logging API attempts to bind to <literal>tcp:9600</literal>. If this port is already in use by another Logstash
instance, you need to launch Logstash with the <literal>--http.port</literal> flag specified to bind to a different port. See
<xref linkend="command-line-flags"/> for more information.</simpara></note>
<simpara>To update logging levels, take the subsystem/module you are interested in and prepend
<literal>logger.</literal> to it. For example:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'
{
    "logger.logstash.outputs.elasticsearch" : "DEBUG"
}
'</programlisting>
<simpara>While this setting is in effect, Logstash will begin to emit DEBUG-level logs for <emphasis>all</emphasis> the Elasticsearch outputs
specified in your configuration. Please note this new setting is transient and will not survive a restart.</simpara>
<simpara>Persistent changes should be added to <literal>log4j2.properties</literal>. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
logger.elasticsearchoutput.level = debug</programlisting>
<simpara>To retrieve a list of logging subsystems available at runtime, you can do a <literal>GET</literal> request to <literal>_node/logging</literal></simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/logging?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
...
  "loggers" : {
    "logstash.agent" : "INFO",
    "logstash.api.service" : "INFO",
    "logstash.basepipeline" : "INFO",
    "logstash.codecs.plain" : "INFO",
    "logstash.codecs.rubydebug" : "INFO",
    "logstash.filters.grok" : "INFO",
    "logstash.inputs.beats" : "INFO",
    "logstash.instrument.periodicpoller.jvm" : "INFO",
    "logstash.instrument.periodicpoller.os" : "INFO",
    "logstash.instrument.periodicpoller.persistentqueue" : "INFO",
    "logstash.outputs.stdout" : "INFO",
    "logstash.pipeline" : "INFO",
    "logstash.plugins.registry" : "INFO",
    "logstash.runner" : "INFO",
    "logstash.shutdownwatcher" : "INFO",
    "org.logstash.Event" : "INFO",
    "slowlog.logstash.codecs.plain" : "TRACE",
    "slowlog.logstash.codecs.rubydebug" : "TRACE",
    "slowlog.logstash.filters.grok" : "TRACE",
    "slowlog.logstash.inputs.beats" : "TRACE",
    "slowlog.logstash.outputs.stdout" : "TRACE"
  }
}</programlisting>
</section>
</section>
<section id="shutdown">
<title>Shutting Down Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/shutdown.asciidoc">Edit me</ulink></title>
<simpara>To shut down Logstash, use one of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>
On systemd, use:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">systemctl stop logstash</programlisting>
</listitem>
<listitem>
<simpara>
On upstart, use:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">initctl stop logstash</programlisting>
</listitem>
<listitem>
<simpara>
On sysv, use:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">/etc/init.d/logstash stop</programlisting>
</listitem>
<listitem>
<simpara>
If you have the PID, use:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">kill -TERM {logstash_pid}</programlisting>
</listitem>
</itemizedlist>
<section id="_what_happens_during_a_controlled_shutdown">
<title>What Happens During a Controlled Shutdown?<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/shutdown.asciidoc">Edit me</ulink></title>
<simpara>When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:</simpara>
<itemizedlist>
<listitem>
<simpara>
Stop all input, filter and output plugins
</simpara>
</listitem>
<listitem>
<simpara>
Process all in-flight events
</simpara>
</listitem>
<listitem>
<simpara>
Terminate the Logstash process
</simpara>
</listitem>
</itemizedlist>
<simpara>The following conditions affect the shutdown process:</simpara>
<itemizedlist>
<listitem>
<simpara>
An input plugin receiving data at a slow pace.
</simpara>
</listitem>
<listitem>
<simpara>
A slow filter, like a Ruby filter executing <literal>sleep(10000)</literal> or an Elasticsearch filter that is executing a very heavy
query.
</simpara>
</listitem>
<listitem>
<simpara>
A disconnected output plugin that is waiting to reconnect to flush in-flight events.
</simpara>
</listitem>
</itemizedlist>
<simpara>These situations make the duration and success of the shutdown process unpredictable.</simpara>
<simpara>Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
worker threads.</simpara>
<simpara>To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the <literal>--pipeline.unsafe_shutdown</literal> flag when
you start Logstash.</simpara>
<warning><simpara>Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason may result in data loss (unless you&#8217;ve
enabled Logstash to use <link linkend="persistent-queues">persistent queues</link>). Shut down
Logstash safely whenever possible.</simpara></warning>
</section>
<section id="shutdown-stall-example">
<title>Stall Detection Example<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/shutdown.asciidoc">Edit me</ulink></title>
<simpara>In this example, slow filter execution prevents the pipeline from performing a clean shutdown. Because Logstash is
started with the <literal>--pipeline.unsafe_shutdown</literal> flag, the shutdown results in the loss of 20 events.</simpara>
<informalexample>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -e 'input { generator { } } filter { ruby { code =&gt; "sleep 10000" } }
  output { stdout { codec =&gt; dots } }' -w 1 --pipeline.unsafe_shutdown
Pipeline main started
^CSIGINT received. Shutting down the agent. {:level=&gt;:warn}
stopping pipeline {:id=&gt;"main", :level=&gt;:warn}
Received shutdown signal, but pipeline is still waiting for in-flight events
to be processed. Sending another ^C will force quit Logstash, but this may cause
data loss. {:level=&gt;:warn}
{"inflight_count"=&gt;125, "stalling_thread_info"=&gt;{["LogStash::Filters::Ruby",
{"code"=&gt;"sleep 10000"}]=&gt;[{"thread_id"=&gt;19, "name"=&gt;"[main]&gt;worker0",
"current_call"=&gt;"(ruby filter code):1:in `sleep'"}]}} {:level=&gt;:warn}
The shutdown process appears to be stalled due to busy or blocked plugins.
Check the logs for more information. {:level=&gt;:error}
{"inflight_count"=&gt;125, "stalling_thread_info"=&gt;{["LogStash::Filters::Ruby",
{"code"=&gt;"sleep 10000"}]=&gt;[{"thread_id"=&gt;19, "name"=&gt;"[main]&gt;worker0",
"current_call"=&gt;"(ruby filter code):1:in `sleep'"}]}} {:level=&gt;:warn}
{"inflight_count"=&gt;125, "stalling_thread_info"=&gt;{["LogStash::Filters::Ruby",
{"code"=&gt;"sleep 10000"}]=&gt;[{"thread_id"=&gt;19, "name"=&gt;"[main]&gt;worker0",
"current_call"=&gt;"(ruby filter code):1:in `sleep'"}]}} {:level=&gt;:warn}
Forcefully quitting logstash.. {:level=&gt;:fatal}</programlisting>
</informalexample>
<simpara>When <literal>--pipeline.unsafe_shutdown</literal> isn&#8217;t enabled, Logstash continues to run and produce these reports periodically.</simpara>
</section>
</section>
</chapter>
<chapter id="setup-xpack" role="xpack">
<title>Setting Up X-Pack</title>
<simpara>X-Pack is an Elastic Stack extension that bundles security, alerting,
monitoring, reporting, machine learning, and graph capabilities into one
easy-to-install package.</simpara>
<simpara>X-Pack also provides a monitoring UI for Logstash. To access this
functionality, you must <link linkend="installing-xpack-log">install X-Pack in Logstash</link>.</simpara>
<simpara>X-Pack security enables you to secure your Elasticsearch cluster. If other products in
the stack are connected to the cluster, they need to be secured as well,
or at least communicate with the cluster in a secured way. For more information,
see <xref linkend="ls-security"/>.</simpara>
<section id="installing-xpack-log" role="xpack">
<title>Installing X-Pack in Logstash</title>
<titleabbrev>Installing X-Pack</titleabbrev>
<simpara>After you install Logstash, you can optionally obtain and install X-Pack.
For more information about how to obtain X-Pack,
see <ulink url="https://www.elastic.co/products/x-pack">https://www.elastic.co/products/x-pack</ulink>.</simpara>
<simpara>To use X-Pack you need:</simpara>
<itemizedlist>
<listitem>
<simpara>
Elasticsearch 6.1.0 - <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/_installation.html">Installing Elasticsearch</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
Kibana 6.1.0 -  <ulink url="https://www.elastic.co/guide/en/kibana/6.x/setup.html">Getting Kibana Up and Running</ulink>
</simpara>
</listitem>
</itemizedlist>
<simpara>You must install X-Pack on Elasticsearch, Kibana, and Logstash, using the version of
X-Pack that matches of the version the product. See the
<ulink url="https://www.elastic.co/support/matrix#matrix_compatibility">Elastic Support Matrix</ulink>
for more information about product compatibility.</simpara>
<important><simpara>If you are installing X-Pack for the first time on an existing
cluster, you must perform a full cluster restart. Installing X-Pack enables
security and security must be enabled on ALL nodes in a cluster for the cluster
to operate correctly. When upgrading you can usually perform a rolling upgrade.</simpara></important>
<simpara>The following diagram provides an overview of the steps that are required to
set up X-Pack on Logstash:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="setup/images/LogstashFlow.jpg"/>
  </imageobject>
  <textobject><phrase>Installation overview on Logstash</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>To install X-Pack on Logstash:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/installing-xpack-es.html">Install X-Pack on Elasticsearch</ulink>.
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/installing-xpack-kb.html">Install X-Pack on Kibana</ulink>.
</simpara>
</listitem>
<listitem>
<simpara>
Optional: If you want to install X-Pack on a machine that doesn&#8217;t have
internet access:
</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>
Manually download the X-Pack zip file:
<ulink url="https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.1.0.zip">
<literal>https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.1.0.zip</literal></ulink>
(<ulink url="https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.1.0.zip.sha1">sha1</ulink>)
</simpara>
<note><simpara>The plugins for Elasticsearch, Kibana, and Logstash are included in the same zip file. If
you have already downloaded this file to install X-Pack on one of those other
products, you can reuse the same file.</simpara></note>
</listitem>
<listitem>
<simpara>
Transfer the zip file to a temporary directory on the offline machine. (Do NOT
put the file in the Elasticsearch plugins directory.)
</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>
Run <literal>bin/logstash-plugin install</literal> from the Logstash installation directory.
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin install x-pack</programlisting>
<simpara>The plugin install scripts require direct internet access to download and
install X-Pack. If your server doesn’t have internet access, specify the
location of the X-Pack zip file that you downloaded to a temporary directory.</simpara>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash-plugin install file:///path/to/file/x-pack-6.1.0.zip</programlisting>
</listitem>
<listitem>
<simpara>
Update Logstash to use the new password for the built-in <literal>logstash_system</literal>
user, which you set up along with the other built-in users when you installed
X-Pack on Elasticsearch. You must configure the
<literal>xpack.monitoring.elasticsearch.password</literal> setting in the <literal>logstash.yml</literal>
configuration file with the new password for the <literal>logstash_system</literal> user.
</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.username: logstash_system
xpack.monitoring.elasticsearch.password: logstashpassword</programlisting>
<simpara>For more information,
see <ulink url="https://www.elastic.co/guide/en/x-pack/6.x/setting-up-authentication.html">Setting Up User Authentication</ulink>.</simpara>
</listitem>
<listitem>
<simpara>
<link linkend="configuration">Configure and start Logstash</link>.
</simpara>
</listitem>
</orderedlist>
</section>
<section id="settings-xpack" role="xpack">
<title>X-Pack Settings in Logstash</title>
<titleabbrev>X-Pack Settings</titleabbrev>
<simpara>You configure settings for X-Pack features in the <literal>elasticsearch.yml</literal>,
<literal>kibana.yml</literal>, and <literal>logstash.yml</literal> configuration files.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">X-Pack Feature   </entry>
<entry align="left" valign="top">Elasticsearch Settings                  </entry>
<entry align="left" valign="top">Kibana Settings                                </entry>
<entry align="left" valign="top">Logstash Settings</entry>
</row>
</thead>
<tfoot>
<row>
<entry align="left" valign="top"><simpara>Watcher</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/notification-settings.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tfoot>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Development Tools</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/dev-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Graph</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/graph-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Machine learning</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/ml-settings.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/ml-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Management</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="http://www.elastic.co/guide/en/logstash/6.x/configuring-centralized-pipelines.html#configuration-management-settings">Yes</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Monitoring</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/monitoring-settings.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/monitoring-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="http://www.elastic.co/guide/en/logstash/6.x/configuring-logstash.html#monitoring-settings">Yes</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reporting</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/reporting-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Security</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/security-settings.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://www.elastic.co/guide/en/kibana/6.x/security-settings-kb.html">Yes</ulink></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>There are also <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/license-settings.html">X-Pack license settings</ulink> in the
<literal>elasticsearch.yml</literal> file.</simpara>
<simpara>For more Logstash configuration settings, see <xref linkend="logstash-settings-file"/>.</simpara>
</section>
<section id="configuring-centralized-pipelines" role="xpack">
<title>Configuring Centralized Pipeline Management</title>
<simpara>To configure
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/logstash-centralized-pipeline-management.html">centralized pipeline management</ulink>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<link linkend="installing-xpack-log">Install X-Pack</link> in the Logstash
installation directory.
</simpara>
</listitem>
<listitem>
<simpara>
Specify
<link linkend="configuration-management-settings">configuration management settings</link> in the
<literal>logstash.yml</literal> file. At a
minimum, set:
</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>xpack.management.enabled: true</literal> to enable centralized configuration
management.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>xpack.management.elasticsearch.url</literal> to specify the Elasticsearch
instance that will store the Logstash pipeline configurations and metadata.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>xpack.management.pipeline.id</literal> to register the pipelines that you want to
centrally manage.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Restart Logstash.
</simpara>
</listitem>
<listitem>
<simpara>
If your Elasticsearch cluster is protected with basic authentication, assign
the <literal>logstash_admin</literal> role to any users who will use centralized pipeline
management. See <xref linkend="ls-security"/>.
</simpara>
</listitem>
</orderedlist>
<important><simpara>After you&#8217;ve configured Logstash to use centralized pipeline
management, you can no longer specify local pipeline configurations. This means
that the <literal>pipelines.yml</literal> file and settings like <literal>path.config</literal> and
<literal>config.string</literal> are inactive when this feature is enabled.</simpara></important>
<section id="configuration-management-settings" role="xpack">
<title>Configuration Management Settings in Logstash</title>
<titleabbrev>Configuration Management Settings</titleabbrev>
<simpara>You can set the following <literal>xpack.management</literal> settings in <literal>logstash.yml</literal> to
enable
<link linkend="logstash-centralized-pipeline-management">centralized pipeline management</link>.
For more information about configuring Logstash, see <xref linkend="logstash-settings-file"/>.</simpara>
<simpara>The following example shows basic settings that assume Elasticsearch and Kibana are
installed on the localhost with basic AUTH enabled, but no SSL. If you&#8217;re using
SSL, you need to specify additional SSL settings.</simpara>
<programlisting language="shell" linenumbering="unnumbered">xpack.management.enabled: true
xpack.management.elasticsearch.url: "http://localhost:9200/"
xpack.management.elasticsearch.username: logstash_admin_user
xpack.management.elasticsearch.password: t0p.s3cr3t
xpack.management.logstash.poll_interval: 5s
xpack.management.pipeline.id: ["apache", "cloudwatch_logs"]</programlisting>
<variablelist>
<varlistentry>
<term>
<literal>xpack.management.enabled</literal>
</term>
<listitem>
<simpara>
Set to <literal>true</literal> to enable X-Pack centralized configuration management for
Logstash.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.logstash.poll_interval</literal>
</term>
<listitem>
<simpara>
How often the Logstash instance polls for pipeline changes from Elasticsearch.
The default is 5s.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.pipeline.id</literal>
</term>
<listitem>
<simpara>
Specify a comma-separated list of pipeline IDs to register for centralized
pipeline management. After changing this setting, you need to restart Logstash
to pick up changes.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.url</literal>
</term>
<listitem>
<simpara>
The Elasticsearch instance that will store the Logstash pipeline configurations and
metadata. This might be the same Elasticsearch instance specified in the <literal>outputs</literal>
section in your Logstash configuration, or a different one. Defaults to
<literal>http://localhost:9200</literal>.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.username</literal> and <literal>xpack.management.elasticsearch.password</literal>
</term>
<listitem>
<simpara>
If your Elasticsearch cluster is protected with basic authentication, these settings
provide the username and password that the Logstash instance uses to
authenticate for accessing the configuration data. The username you specify here
should have the <literal>logstash_admin</literal> role, which provides access to <literal>.logstash-*</literal>
indices for managing configurations.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.ssl.ca</literal>
</term>
<listitem>
<simpara>
Optional setting that enables you to specify a path to the <literal>.pem</literal> file for the
certificate authority for your Elasticsearch instance.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.ssl.truststore.path</literal>
</term>
<listitem>
<simpara>
Optional setting that provides the path to the Java keystore (JKS) to validate
the server’s certificate.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.ssl.truststore.password</literal>
</term>
<listitem>
<simpara>
Optional setting that provides the password to the truststore.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.ssl.keystore.path</literal>
</term>
<listitem>
<simpara>
Optional setting that provides the path to the Java keystore (JKS) to validate
the client’s certificate.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.management.elasticsearch.ssl.keystore.password</literal>
</term>
<listitem>
<simpara>
Optional setting that provides the password to the keystore.
</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section id="configuring-logstash" role="xpack">
<title>Configuring Monitoring for Logstash Nodes</title>
<titleabbrev>Configuring Monitoring</titleabbrev>
<simpara>To monitor Logstash nodes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<link linkend="installing-xpack-log">Install X-Pack</link> in the Logstash
installation directory on each node you want to monitor.
</simpara>
</listitem>
<listitem>
<simpara>
Configure your Logstash nodes to send metrics to your Elasticsearch cluster by setting
the <literal>xpack.monitoring.elasticsearch.url</literal> in <literal>logstash.yml</literal>. If security is
enabled, you also need to specify the credentials for the built-in
<literal>logstash_system</literal> user. For more information about these settings, see
<xref linkend="monitoring-settings"/>.
</simpara>
<important><simpara>To visualize Logstash as part of the Stack (shown below in Step 5),
send metrics to your <emphasis>production</emphasis> cluster.
Sending metrics to a dedicated monitoring cluster
will show the Logstash metrics under the <emphasis>monitoring</emphasis> cluster.</simpara></important>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.url:
["http://es-prod-node-1:9200", "http://es-prod-node-2:9200"] <co id="CO3-1"/>
xpack.monitoring.elasticsearch.username: "logstash_system" <co id="CO3-2"/>
xpack.monitoring.elasticsearch.password: "changeme"</programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para>
If SSL/TLS is enabled on the production cluster, you must connect through
HTTPS. As of v5.2.1, you can specify multiple Elasticsearch hosts as an array as well as
specifying a single host as a string. If multiple URLs are specified, Logstash
can round-robin requests to these production nodes.
</para>
</callout>
<callout arearefs="CO3-2">
<para>
The <literal>logstash_system</literal> user is automatically added when you
install X-Pack. Don&#8217;t forget to change the default passwords for all of the
<ulink url="https://www.elastic.co/guide/en/x-pack/6.x/setting-up-authentication.html#built-in-users">built in users</ulink>. If
X-Pack security is disabled on the production cluster, you can omit the <literal>username</literal>
and <literal>password</literal> for the agent.
</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>
If SSL/TLS is enabled on the production Elasticsearch cluster, specify the trusted
CA certificates that will be used to verify the identity of the nodes
in the cluster.
</simpara>
<simpara>To add a CA certificate to a Logstash node&#8217;s trusted certificates, you
can specify the location of the PEM encoded certificate with the
<literal>ca</literal> setting:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.ssl.ca: [ "/path/to/ca.crt" ]</programlisting>
<simpara>Alternatively, you can configure trusted certificates using a truststore
(a Java Keystore file that contains the certificates):</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.ssl.truststore.path: /path/to/file
xpack.monitoring.elasticsearch.ssl.truststore.password: password</programlisting>
<simpara>Also, optionally, you can set up client certificate using a keystore
(a Java Keystore file that contains the certificate):</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file
xpack.monitoring.elasticsearch.ssl.keystore.password: password</programlisting>
<simpara>Set sniffing to <literal>true</literal> to enable discovery of other nodes of the
elasticsearch cluster. Defaults to <literal>false</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.sniffing: false</programlisting>
</listitem>
<listitem>
<simpara>
Restart your Logstash nodes.
</simpara>
</listitem>
<listitem>
<simpara>
To verify your X-Pack monitoring configuration, point your web browser at your Kibana
host, and select <emphasis role="strong">Monitoring</emphasis> from the side navigation. Metrics reported from
your Logstash nodes should be visible in the Logstash section. When security is
enabled, to view the monitoring dashboards you must log in to Kibana as a user
who has the <literal>kibana_user</literal> and <literal>monitoring_user</literal> roles.
</simpara>
<simpara><inlinemediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/monitoring-ui.png"/>
  </imageobject>
  <textobject><phrase>Monitoring</phrase></textobject>
</inlinemediaobject></simpara>
</listitem>
</orderedlist>
<bridgehead id="monitoring-upgraded-logstash" renderas="sect3">Re-enabling Logstash Monitoring After Upgrading</bridgehead>
<simpara>When upgrading from older versions of X-Pack, the built-in <literal>logstash_system</literal>
user is disabled for security reasons. To resume monitoring,
<link linkend="monitoring-troubleshooting">change the password and re-enable the logstash_system user</link>.</simpara>
<section id="monitoring-settings" role="xpack">
<title>Monitoring Settings in Logstash</title>
<titleabbrev>Monitoring Settings</titleabbrev>
<simpara>You can set the following <literal>xpack.monitoring</literal> settings in <literal>logstash.yml</literal> to
control how monitoring data is collected from your Logstash nodes. However, the
defaults work best in most circumstances. For more information about configuring
Logstash, see <xref linkend="logstash-settings-file"/>.</simpara>
<variablelist>
<varlistentry>
<term>
<literal>xpack.monitoring.enabled</literal>
</term>
<listitem>
<simpara>
Monitoring is enabled by default. Set to <literal>false</literal> to disable X-Pack monitoring.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.collection.interval</literal>
</term>
<listitem>
<simpara>
Controls how often data samples are collected and shipped on the Logstash side.
Defaults to <literal>10s</literal>.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.url</literal>
</term>
<listitem>
<simpara>
The Elasticsearch instances that you want to ship your Logstash metrics to. This might be
the same Elasticsearch instance specified in the <literal>outputs</literal> section in your Logstash
configuration, or a different one. This is <emphasis role="strong">not</emphasis> the URL of your dedicated
monitoring cluster. Even if you are using a dedicated monitoring cluster, the
Logstash metrics must be routed through your production cluster. You can specify
a single host as a string, or specify multiple hosts as an array. Defaults to
<literal>http://localhost:9200</literal>.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.username</literal> and <literal>xpack.monitoring.elasticsearch.password</literal>
</term>
<listitem>
<simpara>
If your Elasticsearch is protected with basic authentication, these settings provide the
username and password that the Logstash instance uses to authenticate for
shipping monitoring data.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.ssl.ca</literal>
</term>
<listitem>
<simpara>
Optional setting that enables you to specify a path to the <literal>.pem</literal> file for the
certificate authority for your Elasticsearch instance.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.ssl.truststore.path</literal>
</term>
<listitem>
<simpara>
Optional settings that provide the paths to the Java keystore (JKS) to validate
the server’s certificate.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.ssl.truststore.password</literal>
</term>
<listitem>
<simpara>
Optional settings that provide the password to the truststore.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.ssl.keystore.path</literal>
</term>
<listitem>
<simpara>
Optional settings that provide the paths to the Java keystore (JKS) to validate
the client’s certificate.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>xpack.monitoring.elasticsearch.ssl.keystore.password</literal>
</term>
<listitem>
<simpara>
Optional settings that provide the password to the keystore.
</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section id="ls-security" role="xpack">
<title>Configuring Security in Logstash</title>
<titleabbrev>Configuring Security</titleabbrev>
<simpara>The Logstash Elasticsearch plugins (
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-outputs-elasticsearch.html">output</ulink>,
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-inputs-elasticsearch.html">input</ulink>,
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-elasticsearch.html">filter</ulink>
and <ulink url="http://www.elastic.co/guide/en/logstash/6.x/monitoring-logstash.html">monitoring</ulink>)
support authentication and encryption over HTTP.</simpara>
<simpara>To use Logstash with a secured cluster, you need to configure authentication
credentials for Logstash. Logstash throws an exception and the processing
pipeline is halted if authentication fails.</simpara>
<simpara>If encryption is enabled on the cluster, you also need to enable SSL in the
Logstash configuration.</simpara>
<simpara>If you want to monitor your Logstash instance with X-Pack monitoring, and store the
monitoring data in a secured Elasticsearch cluster, you must configure Logstash
with a username and password for a user with the appropriate permissions.</simpara>
<simpara>In addition to configuring authentication credentials for Logstash, you need
to grant authorized users permission to access the Logstash indices.</simpara>
<bridgehead id="ls-http-auth-basic" renderas="sect3">Configuring Logstash to use Basic Authentication</bridgehead>
<simpara>Logstash needs to be able to manage index templates, create indices,
and write and delete documents in the indices it creates.</simpara>
<simpara>To set up authentication credentials for Logstash:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Create a <literal>logstash_writer</literal> role that has the <literal>manage_index_templates</literal> cluster
privilege, and the <literal>write</literal>, <literal>delete</literal>, and <literal>create_index</literal> privileges  for the
Logstash indices. You can create roles from the <emphasis role="strong">Management &gt; Roles</emphasis> UI in
Kibana or through the <literal>role</literal> API:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">POST _xpack/security/role/logstash_writer
{
  "cluster": ["manage_index_templates", "monitor"],
  "indices": [
    {
      "names": [ "logstash-*" ], <co id="CO4-1"/>
      "privileges": ["write","delete","create_index"]
    }
  ]
}</programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para>
If you use a custom Logstash index pattern, specify that pattern
instead of the default <literal>logstash-*</literal> pattern.
</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>
Create a <literal>logstash_internal</literal> user and assign it the <literal>logstash_writer</literal> role.
You can create users from the <emphasis role="strong">Management &gt; Users</emphasis> UI in Kibana or through
the <literal>user</literal> API:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">POST _xpack/security/user/logstash_internal
{
  "password" : "x-pack-test-password",
  "roles" : [ "logstash_writer"],
  "full_name" : "Internal Logstash User"
}</programlisting>
</listitem>
<listitem>
<simpara>
Configure Logstash to authenticate as the <literal>logstash_internal</literal> user you just
created. You configure credentials separately for each of the Elasticsearch plugins in
your Logstash <literal>.conf</literal> file. For example:
</simpara>
<programlisting language="js" linenumbering="unnumbered">input {
  elasticsearch {
    ...
    user =&gt; logstash_internal
    password =&gt; x-pack-test-password
  }
}
filter {
  elasticsearch {
    ...
    user =&gt; logstash_internal
    password =&gt; x-pack-test-password
  }
}
output {
  elasticsearch {
    ...
    user =&gt; logstash_internal
    password =&gt; x-pack-test-password
  }
}</programlisting>
</listitem>
</orderedlist>
<bridgehead id="ls-user-access" renderas="sect3">Granting Users Access to the Logstash Indices</bridgehead>
<simpara>To access the indices Logstash creates, users need the <literal>read</literal> and
<literal>view_index_metadata</literal> privileges:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Create a <literal>logstash_reader</literal> role that has the <literal>read</literal> and <literal>view_index_metadata</literal>
privileges  for the Logstash indices. You can create roles from the
<emphasis role="strong">Management &gt; Roles</emphasis> UI in Kibana or through the <literal>role</literal> API:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">POST _xpack/security/role/logstash_reader
{
  "indices": [
    {
      "names": [ "logstash-*" ], <co id="CO5-1"/>
      "privileges": ["read","view_index_metadata"]
    }
  ]
}</programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>
If you use a custom Logstash index pattern, specify that pattern
instead of the default <literal>logstash-*</literal> pattern.
</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>
Assign your Logstash users the <literal>logstash_reader</literal> role. If the Logstash user
will be using
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/logstash-centralized-pipeline-management.html">centralized pipeline management</ulink>,
also assign the <literal>logstash_admin</literal> role. You can create and manage users from the
<emphasis role="strong">Management &gt; Users</emphasis> UI in Kibana or through the <literal>user</literal> API:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">POST _xpack/security/user/logstash_user
{
  "password" : "x-pack-test-password",
  "roles" : [ "logstash_reader", "logstash_admin"], <co id="CO6-1"/>
  "full_name" : "Kibana User for Logstash"
}</programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para>
<literal>logstash_admin</literal> is a built-in role that provides access to <literal>.logstash-*</literal>
indices for managing configurations.
</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<bridgehead id="ls-http-auth-pki" renderas="sect3">Configuring the Elasticsearch Output to use PKI Authentication</bridgehead>
<simpara>The <literal>elasticsearch</literal> output supports PKI authentication. To use an X.509
client-certificate for authentication, you configure the <literal>keystore</literal> and
<literal>keystore_password</literal> options in your Logstash <literal>.conf</literal> file:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  elasticsearch {
    ...
    keystore =&gt; /path/to/keystore.jks
    keystore_password =&gt; realpassword
    truststore =&gt;  /path/to/truststore.jks <co id="CO7-1"/>
    truststore_password =&gt;  realpassword
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO7-1">
<para>
If you use a separate truststore, the truststore path and password are
also required.
</para>
</callout>
</calloutlist>
<bridgehead id="ls-http-ssl" renderas="sect3">Configuring Logstash to use TLS Encryption</bridgehead>
<simpara>If TLS encryption is enabled on the Elasticsearch cluster, you need to
configure the <literal>ssl</literal> and <literal>cacert</literal> options in your Logstash <literal>.conf</literal> file:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  elasticsearch {
    ...
    ssl =&gt; true
    cacert =&gt; '/path/to/cert.pem' <co id="CO8-1"/>
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO8-1">
<para>
The path to the local <literal>.pem</literal> file that contains the Certificate
    Authority&#8217;s certificate.
</para>
</callout>
</calloutlist>
<bridgehead id="ls-monitoring-user" renderas="sect3">Configuring Credentials for Logstash Monitoring</bridgehead>
<simpara>If you plan to ship Logstash <ulink url="http://www.elastic.co/guide/en/logstash/6.x/monitoring-logstash.html">monitoring</ulink>
data to a secure cluster, you need to configure the username and password that
Logstash uses to authenticate for shipping monitoring data.</simpara>
<simpara>X-Pack security comes preconfigured with a <literal>logstash_system</literal> user for this purpose.
This user has the minimum permissions necessary for the monitoring function, and
<emphasis>should not</emphasis> be used for any other purpose - it is specifically <emphasis>not intended</emphasis>
for use within a Logstash pipeline.</simpara>
<simpara>By default, the <literal>logstash_system</literal> user does not have a password. The user will
not be enabled until you set a password. Set the password through the change
password API:</simpara>
<programlisting language="js" linenumbering="unnumbered">PUT _xpack/security/user/logstash_system/_password
{
  "password": "t0p.s3cr3t"
}</programlisting>
<remark> CONSOLE</remark>
<simpara>Then configure the user and password in the <literal>logstash.yml</literal> configuration file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.monitoring.elasticsearch.username: logstash_system
xpack.monitoring.elasticsearch.password: t0p.s3cr3t</programlisting>
<simpara>If you initially installed an older version of X-Pack, and then upgraded, the
<literal>logstash_system</literal> user may have defaulted to <literal>disabled</literal> for security reasons.
You can enable the user through the <literal>user</literal> API:</simpara>
<programlisting language="js" linenumbering="unnumbered">PUT _xpack/security/user/logstash_system/_enable</programlisting>
<remark> CONSOLE</remark>
<bridgehead id="ls-pipeline-management-user" renderas="sect3">Configuring Credentials for Centralized Pipeline Management</bridgehead>
<simpara>If you plan to use Logstash
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/logstash-centralized-pipeline-management.html">centralized pipeline management</ulink>,
you need to configure the username and password that Logstash uses for managing
configurations.</simpara>
<simpara>You configure the user and password in the <literal>logstash.yml</literal> configuration file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">xpack.management.elasticsearch.username: logstash_admin_user <co id="CO9-1"/>
xpack.management.elasticsearch.password: t0p.s3cr3t</programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>
The user you specify here must have the built-in <literal>logstash_admin</literal> role as
well as the <literal>logstash_writer</literal> role that you created earlier.
</para>
</callout>
</calloutlist>
<remark> Breaking Changes</remark>
</section>
</chapter>
<chapter id="breaking-changes">
<title>Breaking Changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></title>
<simpara>This section discusses the changes that you need to be aware of when migrating to Logstash 6.0.0 from the previous major releases.</simpara>
<simpara>See also:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="breaking-changes-xls"/>
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_changes_in_logstash_core" renderas="sect2">Changes in Logstash Core<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<simpara>These changes can impact any instance of Logstash and are plugin agnostic, but only if you are using the features that are impacted.</simpara>
<bridgehead id="_application_settings" renderas="sect3">Application Settings<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
The setting <literal>config.reload.interval</literal> has been changed to use time value strings such as <literal>5m</literal>, <literal>10s</literal> etc.
  Previously, users had to convert this to a millisecond time value themselves.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_rpm_deb_package_changes" renderas="sect3">RPM/Deb package changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
For <literal>rpm</literal> and <literal>deb</literal> release artifacts, config files that match the <literal>*.conf</literal> glob pattern must be in the conf.d folder,
  or the files will not be loaded.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_command_line_interface_behavior" renderas="sect3">Command Line Interface behavior<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
The <literal>-e</literal> and <literal>-f</literal> CLI options are now mutually exclusive. This also applies to the corresponding long form options <literal>config.string</literal> and
  <literal>path.config</literal>. This means any configurations  provided via <literal>-e</literal> will no longer be appended to the configurations provided via <literal>-f</literal>.
</simpara>
</listitem>
<listitem>
<simpara>
Configurations provided with <literal>-f</literal> or <literal>config.path</literal> will not be appended with <literal>stdin</literal> input and <literal>stdout</literal> output automatically.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_plugin_changes" renderas="sect2">Plugin Changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<bridgehead id="_elasticsearch_output_changes" renderas="sect3">Elasticsearch output changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
The default <literal>document_type</literal> has changed from <literal>logs</literal> to <literal>doc</literal> for consistency with Beats.
  Furthermore, users are advised that Elasticsearch 6.0 deprecates doctypes, and 7.0 will remove them.
  See <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/master/removal-of-types.html">Removal of Mapping Types</ulink> for more info.
</simpara>
</listitem>
<listitem>
<simpara>
The options <literal>flush_size</literal> and <literal>idle_flush_time</literal> are now obsolete.
</simpara>
</listitem>
<listitem>
<simpara>
Please note that the <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.0/mapping-all-field.html">_all</ulink> field is deprecated in 6.0.
 The new mapping template has been updated to reflect that. If you are using a custom mapping template you may need to update it to reflect that.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_kafka_input_changes" renderas="sect3">Kafka input changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
Upgraded Kafka client support to v0.11.0.0, which only supports Kafka brokers v0.10.x or later.
</simpara>
<itemizedlist>
<listitem>
<simpara>
Please refer to <link linkend="plugins-inputs-kafka">Kafka input plugin</link> documentation for information about Kafka compatibility with Logstash.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Decorated fields are now nested under <literal>@metadata</literal> to avoid mapping conflicts with Beats.
</simpara>
<itemizedlist>
<listitem>
<simpara>
See the <literal>Metadata Fields</literal> section in the <link linkend="plugins-inputs-kafka">Kafka input plugin</link> documentation for more details.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
The <literal>ssl</literal> option is now obsolete.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_kafka_output_changes" renderas="sect3">Kafka output changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
Upgraded Kafka client support to v0.11.0.0, which only supports Kafka brokers v0.10.x or later.
</simpara>
<itemizedlist>
<listitem>
<simpara>
Please refer to <link linkend="plugins-outputs-kafka">Kafka output plugin</link> documentation for information about Kafka compatibility with Logstash.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
The options <literal>block_on_buffer_full</literal>, <literal>ssl</literal>, and <literal>timeout_ms</literal> are now obsolete.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_beats_input_changes" renderas="sect3">Beats input changes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<itemizedlist>
<listitem>
<simpara>
Logstash will no longer start when <link linkend="plugins-codecs-multiline">Multiline codec plugin</link> is used with the Beats input plugin.
</simpara>
<itemizedlist>
<listitem>
<simpara>
It is recommended to use the multiline support in Filebeat as a replacement - see <ulink url="https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html">configuration options available in Filebeat</ulink> for details.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
The options <literal>congestion_threshold</literal> and <literal>target_field_for_codec</literal> are now obsolete.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_list_of_plugins_bundled_with_logstash" renderas="sect3">List of plugins bundled with Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc">Edit me</ulink></bridgehead>
<simpara>The following plugins were removed from the 6.0 default bundle based on usage data. You can still install these plugins manually:</simpara>
<itemizedlist>
<listitem>
<simpara>
logstash-codec-oldlogstashjson
</simpara>
</listitem>
<listitem>
<simpara>
logstash-input-couchdb_changes
</simpara>
</listitem>
<listitem>
<simpara>
logstash-input-irc
</simpara>
</listitem>
<listitem>
<simpara>
logstash-input-log4j
</simpara>
</listitem>
<listitem>
<simpara>
logstash-input-lumberjack
</simpara>
</listitem>
<listitem>
<simpara>
logstash-filter-uuid
</simpara>
</listitem>
<listitem>
<simpara>
logstash-output-xmpp
</simpara>
</listitem>
<listitem>
<simpara>
logstash-output-irc
</simpara>
</listitem>
<listitem>
<simpara>
logstash-output-statsd
</simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter id="breaking-changes-xls" role="xpack">
<title>X-Pack Breaking Changes</title>
<simpara>This section summarizes the changes that you need to be aware of when migrating
your application from one version of X-Pack to another.</simpara>
<simpara>See also:</simpara>
<itemizedlist>
<listitem>
<simpara>
<link linkend="breaking-changes">Breaking Changes in Logstash</link>
<remark> * {ref}/breaking-changes-xes.html[{xpack} Breaking Changes in {es}]</remark>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/breaking-changes-xpackkb.html"> X-Pack Breaking Changes in Kibana</ulink>
</simpara>
</listitem>
</itemizedlist>
<simpara>There are no breaking changes in Logstash X-Pack 6.x features.</simpara>
<remark> Upgrading Logstash</remark>
</chapter>
<chapter id="upgrading-logstash">
<title>Upgrading Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<important>
<simpara>Before upgrading Logstash:</simpara>
<itemizedlist>
<listitem>
<simpara>
Consult the <link linkend="breaking-changes">breaking changes</link> docs.
</simpara>
</listitem>
<listitem>
<simpara>
Test upgrades in a development environment before upgrading your production cluster.
</simpara>
</listitem>
</itemizedlist>
</important>
<simpara>If you are installing Logstash with other components in the Elastic Stack, also see the
<ulink url="http://www.elastic.co/guide/en/elastic-stack/6.x/index.html">Elastic Stack installation and upgrade documentation</ulink>.</simpara>
<simpara>See the following topics for information about upgrading Logstash:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="upgrading-using-package-managers"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="upgrading-using-direct-download"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="upgrading-logstash-pqs"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="upgrading-logstash-6.0"/>
</simpara>
</listitem>
</itemizedlist>
<section id="upgrading-using-package-managers">
<title>Upgrading Using Package Managers<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>This procedure uses <link linkend="package-repositories">package managers</link> to upgrade Logstash.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Shut down your Logstash pipeline, including any inputs that send events to Logstash.
</simpara>
</listitem>
<listitem>
<simpara>
Using the directions in the <emphasis>Package Repositories</emphasis> section, update your repository links to point to the 5.x repositories
instead of the previous version.
</simpara>
</listitem>
<listitem>
<simpara>
Run the <literal>apt-get upgrade logstash</literal> or <literal>yum update logstash</literal> command as appropriate for your operating system.
</simpara>
</listitem>
<listitem>
<simpara>
Test your configuration file with the <literal>logstash --config.test_and_exit -f &lt;configuration-file&gt;</literal> command. Configuration options for
some Logstash plugins have changed in the 5.x release.
</simpara>
</listitem>
<listitem>
<simpara>
Restart your Logstash pipeline after updating your configuration file.
</simpara>
</listitem>
</orderedlist>
</section>
<section id="upgrading-using-direct-download">
<title>Upgrading Using a Direct Download<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>This procedure downloads the relevant Logstash binaries directly from Elastic.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Shut down your Logstash pipeline, including any inputs that send events to Logstash.
</simpara>
</listitem>
<listitem>
<simpara>
Download the <ulink url="https://www.elastic.co/downloads/logstash">Logstash installation file</ulink> that matches your host environment.
</simpara>
</listitem>
<listitem>
<simpara>
Unpack the installation file into your Logstash directory.
</simpara>
</listitem>
<listitem>
<simpara>
Test your configuration file with the <literal>logstash --config.test_and_exit -f &lt;configuration-file&gt;</literal> command. Configuration options for
some Logstash plugins have changed in the 5.x release.
</simpara>
</listitem>
<listitem>
<simpara>
Restart your Logstash pipeline after updating your configuration file.
</simpara>
</listitem>
</orderedlist>
</section>
<section id="upgrading-logstash-pqs">
<title>Upgrading with Persistent Queues Enabled<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>Upgrading Logstash with persistent queues enabled is supported. The persistent
queue directory is self-contained and can be read by a new Logstash instance
running the same pipeline. You can safely shut down the original Logstash
instance, spin up a new instance, and set <literal>path.queue</literal> in the <literal>logstash.yml</literal>
<link linkend="logstash-settings-file">settings file</link> to point to the original queue directory.
You can also use a mounted drive to make this workflow easier.</simpara>
<simpara>Keep in mind that only one Logstash instance can write to <literal>path.queue</literal>. You
cannot have the original instance and the new instance writing to the queue at
the same time.</simpara>
</section>
<section id="upgrading-logstash-6.0">
<title>Upgrading Logstash to 6.0<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>Before upgrading Logstash, remember to read the <link linkend="breaking-changes">breaking changes</link>.</simpara>
<simpara>If you are installing Logstash with other components in the Elastic Stack, also see the
<ulink url="http://www.elastic.co/guide/en/elastic-stack/6.x/index.html">Elastic Stack installation and upgrade documentation</ulink>.</simpara>
<simpara>If you are using the default mapping templates in Logstash, to continue using these after migrating Elasticsearch to 6.0, you must override the existing template with the 6.x template.
 This can be done by starting a pipeline with the <literal>overwrite_template =&gt; true</literal> option in the Elasticsearch output definition in the Logstash config.</simpara>
<simpara>Note that multiple doctypes are no longer supported in Elasticsearch 6.0. Please refer to
 <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/removal-of-types.html">Removal of mapping types</ulink> and <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/breaking-changes.html">Breaking changes</ulink> for more information.</simpara>
<section id="_when_to_upgrade">
<title>When to Upgrade<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>Fresh installations can and should start with the same version across the Elastic Stack.</simpara>
<simpara>Elasticsearch 6.0 does not require Logstash 6.0. An Elasticsearch 6.0 cluster will happily receive data from a
Logstash 5.x instance via the default HTTP communication layer. This provides some flexibility to decide when to upgrade
Logstash relative to an Elasticsearch upgrade. It may or may not be convenient for you to upgrade them together, and it
is not required to be done at the same time as long as Elasticsearch is upgraded first.</simpara>
<simpara>You should upgrade in a timely manner to get the performance improvements that come with Logstash 6.0, but do so in
the way that makes the most sense for your environment.</simpara>
</section>
<section id="_when_not_to_upgrade">
<title>When Not to Upgrade<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc">Edit me</ulink></title>
<simpara>If any Logstash plugin that you require is not compatible with Logstash 6.0, then you should wait until it is ready
before upgrading.</simpara>
<simpara>Although we make great efforts to ensure compatibility, Logstash 6.0 is not completely backwards compatible. As noted
in the Elastic Stack upgrade guide, Logstash 6.0 should not be upgraded before Elasticsearch 6.0. This is both
practical and because some Logstash 6.0 plugins may attempt to use features of Elasticsearch 6.0 that did not exist
in earlier versions.</simpara>
<remark> Configuring Logstash</remark>
</section>
</section>
</chapter>
<chapter id="configuration">
<title>Configuring Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>To configure Logstash, you create a config file that specifies which plugins you want to use and settings for each plugin.
You can reference event fields in a configuration and use conditionals to process events when they meet certain
criteria. When you run logstash, you use the <literal>-f</literal> to specify your config file.</simpara>
<simpara>Let&#8217;s step through creating a simple config file and using it to run Logstash. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input { stdin { } }
output {
  elasticsearch { hosts =&gt; ["localhost:9200"] }
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Then, run logstash and specify the configuration file with the <literal>-f</literal> flag.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">bin/logstash -f logstash-simple.conf</programlisting>
<simpara>Et voilà! Logstash reads  the specified configuration file and outputs to both Elasticsearch and stdout. Before we
move on to some <link linkend="config-examples">more complex examples</link>, let&#8217;s take a closer look at what&#8217;s in a config file.</simpara>
<section id="configuration-file-structure">
<title>Structure of a Config File<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>A Logstash config file has a separate section for each type of plugin you want to add to the event processing pipeline. For example:</simpara>
<programlisting language="js" linenumbering="unnumbered"># This is a comment. You should use comments to describe
# parts of your configuration.
input {
  ...
}

filter {
  ...
}

output {
  ...
}</programlisting>
<simpara>Each section contains the configuration options for one or more plugins. If you specify
multiple filters, they are applied in the order of their appearance in the configuration file.</simpara>
<bridgehead id="plugin_configuration" renderas="sect2">Plugin Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>The configuration of a plugin consists of the plugin name followed
by a block of settings for that plugin. For example, this input section configures two file inputs:</simpara>
<programlisting language="js" linenumbering="unnumbered">input {
  file {
    path =&gt; "/var/log/messages"
    type =&gt; "syslog"
  }

  file {
    path =&gt; "/var/log/apache/access.log"
    type =&gt; "apache"
  }
}</programlisting>
<simpara>In this example, two settings are configured for each of the file inputs: <emphasis>path</emphasis> and <emphasis>type</emphasis>.</simpara>
<simpara>The settings you can configure vary according to the plugin type. For information about each plugin, see <link linkend="input-plugins">Input Plugins</link>, <link linkend="output-plugins">Output Plugins</link>, <link linkend="filter-plugins">Filter Plugins</link>, and <link linkend="codec-plugins">Codec Plugins</link>.</simpara>
<bridgehead id="plugin-value-types" renderas="sect2">Value Types<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A plugin can require that the value for a setting be a
certain type, such as boolean, list, or hash. The following value
types are supported.</simpara>
<section id="array">
<title>Array<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>This type is now mostly deprecated in favor of using a standard type like <literal>string</literal> with the plugin defining the <literal>:list =&gt; true</literal> property for better type checking. It is still needed to handle lists of hashes or mixed types where type checking is not desired.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  users =&gt; [ {id =&gt; 1, name =&gt; bob}, {id =&gt; 2, name =&gt; jane} ]</programlisting>
<bridgehead id="list" renderas="sect3">Lists<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Not a type in and of itself, but a property types can have.
This makes it possible to type check multiple values.
Plugin authors can enable list checking by specifying <literal>:list =&gt; true</literal> when declaring an argument.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  path =&gt; [ "/var/log/messages", "/var/log/*.log" ]
  uris =&gt; [ "http://elastic.co", "http://example.net" ]</programlisting>
<simpara>This example configures <literal>path</literal>, which is a <literal>string</literal> to be a list that contains an element for each of the three strings. It also will configure the <literal>uris</literal> parameter to be a list of URIs, failing if any of the URIs provided are not valid.</simpara>
<bridgehead id="boolean" renderas="sect3">Boolean<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A boolean must be either <literal>true</literal> or <literal>false</literal>. Note that the <literal>true</literal> and <literal>false</literal> keywords
are not enclosed in quotes.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  ssl_enable =&gt; true</programlisting>
<bridgehead id="bytes" renderas="sect3">Bytes<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A bytes field is a string field that represents a valid unit of bytes. It is a
convenient way to declare specific sizes in your plugin options. Both SI (k M G T P E Z Y)
and Binary (Ki Mi Gi Ti Pi Ei Zi Yi) units are supported. Binary units are in
base-1024 and SI units are in base-1000. This field is case-insensitive
and accepts space between the value and the unit. If no unit is specified, the integer string
represents the number of bytes.</simpara>
<simpara>Examples:</simpara>
<programlisting language="js" linenumbering="unnumbered">  my_bytes =&gt; "1113"   # 1113 bytes
  my_bytes =&gt; "10MiB"  # 10485760 bytes
  my_bytes =&gt; "100kib" # 102400 bytes
  my_bytes =&gt; "180 mb" # 180000000 bytes</programlisting>
<bridgehead id="codec" renderas="sect3">Codec<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A codec is the name of Logstash codec used to represent the data. Codecs can be
used in both inputs and outputs.</simpara>
<simpara>Input codecs provide a convenient way to decode your data before it enters the input.
Output codecs provide a convenient way to encode your data before it leaves the output.
Using an input or output codec eliminates the need for a separate filter in your Logstash pipeline.</simpara>
<simpara>A list of available codecs can be found at the <link linkend="codec-plugins">Codec Plugins</link> page.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  codec =&gt; "json"</programlisting>
<bridgehead id="hash" renderas="sect3">Hash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A hash is a collection of key value pairs specified in the format <literal>"field1" =&gt; "value1"</literal>.
Note that multiple key value entries are separated by spaces rather than commas.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">match =&gt; {
  "field1" =&gt; "value1"
  "field2" =&gt; "value2"
  ...
}</programlisting>
<bridgehead id="number" renderas="sect3">Number<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Numbers must be valid numeric values (floating point or integer).</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  port =&gt; 33</programlisting>
<bridgehead id="password" renderas="sect3">Password<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A password is a string with a single value that is not logged or printed.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  my_password =&gt; "password"</programlisting>
<bridgehead id="uri" renderas="sect3">URI<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A URI can be anything from a full URL like <emphasis>http://elastic.co/</emphasis> to a simple identifier
like <emphasis>foobar</emphasis>. If the URI contains a password such as <emphasis>http://user:pass@example.net</emphasis> the password
portion of the URI will not be logged or printed.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  my_uri =&gt; "http://foo:bar@example.net"</programlisting>
<bridgehead id="path" renderas="sect3">Path<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A path is a string that represents a valid operating system path.</simpara>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  my_path =&gt; "/tmp/logstash"</programlisting>
<bridgehead id="string" renderas="sect3">String<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>A string must be a single character sequence. Note that string values are
enclosed in quotes, either double or single.</simpara>
<section id="_escape_sequences">
<title>Escape Sequences<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>By default, escape sequences are not enabled. If you wish to use escape
sequences in quoted strings, you will need to set
<literal>config.support_escapes: true</literal> in your <literal>logstash.yml</literal>. When <literal>true</literal>, quoted
strings (double and single) will have this transformation:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Text</simpara></entry>
<entry align="left" valign="top"><simpara>Result</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\r</simpara></entry>
<entry align="left" valign="top"><simpara>carriage return (ASCII 13)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\n</simpara></entry>
<entry align="left" valign="top"><simpara>new line (ASCII 10)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\t</simpara></entry>
<entry align="left" valign="top"><simpara>tab (ASCII 9)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\\</simpara></entry>
<entry align="left" valign="top"><simpara>backslash (ASCII 92)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\"</simpara></entry>
<entry align="left" valign="top"><simpara>double quote (ASCII 34)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>\'</simpara></entry>
<entry align="left" valign="top"><simpara>single quote (ASCII 39)</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Example:</simpara>
<programlisting language="js" linenumbering="unnumbered">  name =&gt; "Hello world"
  name =&gt; 'It\'s a beautiful day'</programlisting>
<bridgehead id="comments" renderas="sect2">Comments<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Comments are the same as in perl, ruby, and python. A comment starts with a <emphasis>#</emphasis> character, and does not need to be at the beginning of a line. For example:</simpara>
<programlisting language="js" linenumbering="unnumbered"># this is a comment

input { # comments can appear at the end of a line, too
  # ...
}</programlisting>
</section>
</section>
</section>
<section id="event-dependent-configuration">
<title>Accessing Event Data and Fields in the Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>The logstash agent is a processing pipeline with 3 stages: inputs &#8594; filters &#8594;
outputs. Inputs generate events, filters modify them, outputs ship them
elsewhere.</simpara>
<simpara>All events have properties. For example, an apache access log would have things
like status code (200, 404), request path ("/", "index.html"), HTTP verb
(GET, POST), client IP address, etc. Logstash calls these properties "fields."</simpara>
<simpara>Some of the configuration options in Logstash require the existence of fields in
order to function.  Because inputs generate events, there are no fields to
evaluate within the input block&#8212;they do not exist yet!</simpara>
<simpara>Because of their dependency on events and fields, the following configuration
options will only work within filter and output blocks.</simpara>
<important><simpara>Field references, sprintf format and conditionals, described below,
will not work in an input block.</simpara></important>
<bridgehead id="logstash-config-field-references" renderas="sect3">Field References<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>It is often useful to be able to refer to a field by name. To do this,
you can use the Logstash field reference syntax.</simpara>
<simpara>The syntax to access a field is <literal>[fieldname]</literal>. If you are referring to a
<emphasis role="strong">top-level field</emphasis>, you can omit the <literal>[]</literal> and simply use <literal>fieldname</literal>.
To refer to a <emphasis role="strong">nested field</emphasis>, you specify
the full path to that field: <literal>[top-level field][nested field]</literal>.</simpara>
<simpara>For example, the following event has five top-level fields (agent, ip, request, response,
ua) and three nested fields (status, bytes, os).</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "agent": "Mozilla/5.0 (compatible; MSIE 9.0)",
  "ip": "192.168.24.44",
  "request": "/index.html"
  "response": {
    "status": 200,
    "bytes": 52353
  },
  "ua": {
    "os": "Windows 7"
  }
}</programlisting>
<simpara>To reference the <literal>os</literal> field, you specify <literal>[ua][os]</literal>. To reference a top-level
field such as <literal>request</literal>, you can simply specify the field name.</simpara>
<bridgehead id="sprintf" renderas="sect3">sprintf format<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>The field reference format is also used in what Logstash calls <emphasis>sprintf format</emphasis>. This format
enables you to refer to field values from within other strings. For example, the
statsd output has an <emphasis>increment</emphasis> setting that enables you to keep a count of
apache logs by status code:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  statsd {
    increment =&gt; "apache.%{[response][status]}"
  }
}</programlisting>
<simpara>Similarly, you can convert the timestamp in the <literal>@timestamp</literal> field into a string. Instead of specifying a field name inside the curly braces, use the <literal>+FORMAT</literal> syntax where <literal>FORMAT</literal> is a <ulink url="http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html">time format</ulink>.</simpara>
<simpara>For example, if you want to use the file output to write to logs based on the
event&#8217;s date and hour and the <literal>type</literal> field:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  file {
    path =&gt; "/var/log/%{type}.%{+yyyy.MM.dd.HH}"
  }
}</programlisting>
<bridgehead id="conditionals" renderas="sect3">Conditionals<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Sometimes you only want to filter or output an event under
certain conditions. For that, you can use a conditional.</simpara>
<simpara>Conditionals in Logstash look and act the same way they do in programming
languages. Conditionals support <literal>if</literal>, <literal>else if</literal> and <literal>else</literal> statements
and can be nested.</simpara>
<simpara>The conditional syntax is:</simpara>
<programlisting language="js" linenumbering="unnumbered">if EXPRESSION {
  ...
} else if EXPRESSION {
  ...
} else {
  ...
}</programlisting>
<simpara>What&#8217;s an expression? Comparison tests, boolean logic, and so on!</simpara>
<simpara>You can use the following comparison operators:</simpara>
<itemizedlist>
<listitem>
<simpara>
equality: <literal>==</literal>,  <literal>!=</literal>,  <literal>&lt;</literal>,  <literal>&gt;</literal>,  <literal>&lt;=</literal>, <literal>&gt;=</literal>
</simpara>
</listitem>
<listitem>
<simpara>
regexp: <literal>=~</literal>, <literal>!~</literal> (checks a pattern on the right against a string value on the left)
</simpara>
</listitem>
<listitem>
<simpara>
inclusion: <literal>in</literal>, <literal>not in</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The supported boolean operators are:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>and</literal>, <literal>or</literal>, <literal>nand</literal>, <literal>xor</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The supported unary operators are:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>!</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Expressions can be long and complex. Expressions can contain other expressions,
you can negate expressions with <literal>!</literal>, and you can group them with parentheses <literal>(...)</literal>.</simpara>
<simpara>For example, the following conditional uses the mutate filter to remove the field <literal>secret</literal> if the field
<literal>action</literal> has a value of <literal>login</literal>:</simpara>
<programlisting language="js" linenumbering="unnumbered">filter {
  if [action] == "login" {
    mutate { remove_field =&gt; "secret" }
  }
}</programlisting>
<simpara>You can specify multiple expressions in a single condition:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  # Send production errors to pagerduty
  if [loglevel] == "ERROR" and [deployment] == "production" {
    pagerduty {
    ...
    }
  }
}</programlisting>
<simpara>You can use the <literal>in</literal> operator to test whether a field contains a specific string, key, or (for lists) element:</simpara>
<programlisting language="js" linenumbering="unnumbered">filter {
  if [foo] in [foobar] {
    mutate { add_tag =&gt; "field in field" }
  }
  if [foo] in "foo" {
    mutate { add_tag =&gt; "field in string" }
  }
  if "hello" in [greeting] {
    mutate { add_tag =&gt; "string in field" }
  }
  if [foo] in ["hello", "world", "foo"] {
    mutate { add_tag =&gt; "field in list" }
  }
  if [missing] in [alsomissing] {
    mutate { add_tag =&gt; "shouldnotexist" }
  }
  if !("foo" in ["hello", "world"]) {
    mutate { add_tag =&gt; "shouldexist" }
  }
}</programlisting>
<simpara>You use the <literal>not in</literal> conditional the same way. For example,
you could use <literal>not in</literal> to only route events to Elasticsearch
when <literal>grok</literal> is successful:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  if "_grokparsefailure" not in [tags] {
    elasticsearch { ... }
  }
}</programlisting>
<simpara>You can check for the existence of a specific field, but there&#8217;s currently no way to differentiate between a field that
doesn&#8217;t exist versus a field that&#8217;s simply false. The expression <literal>if [foo]</literal> returns <literal>false</literal> when:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>[foo]</literal> doesn&#8217;t exist in the event,
</simpara>
</listitem>
<listitem>
<simpara>
<literal>[foo]</literal> exists in the event, but is false, or
</simpara>
</listitem>
<listitem>
<simpara>
<literal>[foo]</literal> exists in the event, but is null
</simpara>
</listitem>
</itemizedlist>
<simpara>For more complex examples, see <link linkend="using-conditionals">Using Conditionals</link>.</simpara>
<bridgehead id="metadata" renderas="sect3">The @metadata field<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>In Logstash 1.5 and later, there is a special field called <literal>@metadata</literal>.  The contents
of <literal>@metadata</literal> will not be part of any of your events at output time, which
makes it great to use for conditionals, or extending and building event fields
with field reference and sprintf formatting.</simpara>
<simpara>The following configuration file will yield events from STDIN.  Whatever is
typed will become the <literal>message</literal> field in the event.  The <literal>mutate</literal> events in the
filter block will add a few fields, some nested in the <literal>@metadata</literal> field.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input { stdin { } }

filter {
  mutate { add_field =&gt; { "show" =&gt; "This data will be in the output" } }
  mutate { add_field =&gt; { "[@metadata][test]" =&gt; "Hello" } }
  mutate { add_field =&gt; { "[@metadata][no_show]" =&gt; "This data will not be in the output" } }
}

output {
  if [@metadata][test] == "Hello" {
    stdout { codec =&gt; rubydebug }
  }
}</programlisting>
<simpara>Let&#8217;s see what comes out:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">$ bin/logstash -f ../test.conf
Pipeline main started
asdf
{
    "@timestamp" =&gt; 2016-06-30T02:42:51.496Z,
      "@version" =&gt; "1",
          "host" =&gt; "example.com",
          "show" =&gt; "This data will be in the output",
       "message" =&gt; "asdf"
}</programlisting>
<simpara>The "asdf" typed in became the <literal>message</literal> field contents, and the conditional
successfully evaluated the contents of the <literal>test</literal> field nested within the
<literal>@metadata</literal> field.  But the output did not show a field called <literal>@metadata</literal>, or
its contents.</simpara>
<simpara>The <literal>rubydebug</literal> codec allows you to reveal the contents of the <literal>@metadata</literal> field
if you add a config flag, <literal>metadata =&gt; true</literal>:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">    stdout { codec =&gt; rubydebug { metadata =&gt; true } }</programlisting>
<simpara>Let&#8217;s see what the output looks like with this change:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">$ bin/logstash -f ../test.conf
Pipeline main started
asdf
{
    "@timestamp" =&gt; 2016-06-30T02:46:48.565Z,
     "@metadata" =&gt; {
           "test" =&gt; "Hello",
        "no_show" =&gt; "This data will not be in the output"
    },
      "@version" =&gt; "1",
          "host" =&gt; "example.com",
          "show" =&gt; "This data will be in the output",
       "message" =&gt; "asdf"
}</programlisting>
<simpara>Now you can see the <literal>@metadata</literal> field and its sub-fields.</simpara>
<important><simpara>Only the <literal>rubydebug</literal> codec allows you to show the contents of the
<literal>@metadata</literal> field.</simpara></important>
<simpara>Make use of the <literal>@metadata</literal> field any time you need a temporary field but do not
want it to be in the final output.</simpara>
<simpara>Perhaps one of the most common use cases for this new field is with the <literal>date</literal>
filter and having a temporary timestamp.</simpara>
<simpara>This configuration file has been simplified, but uses the timestamp format
common to Apache and Nginx web servers.  In the past, you&#8217;d have to delete
the timestamp field yourself, after using it to overwrite the <literal>@timestamp</literal>
field.  With the <literal>@metadata</literal> field, this is no longer necessary:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input { stdin { } }

filter {
  grok { match =&gt; [ "message", "%{HTTPDATE:[@metadata][timestamp]}" ] }
  date { match =&gt; [ "[@metadata][timestamp]", "dd/MMM/yyyy:HH:mm:ss Z" ] }
}

output {
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Notice that this configuration puts the extracted date into the
<literal>[@metadata][timestamp]</literal> field in the <literal>grok</literal> filter.  Let&#8217;s feed this
configuration a sample date string and see what comes out:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">$ bin/logstash -f ../test.conf
Pipeline main started
02/Mar/2014:15:36:43 +0100
{
    "@timestamp" =&gt; 2014-03-02T14:36:43.000Z,
      "@version" =&gt; "1",
          "host" =&gt; "example.com",
       "message" =&gt; "02/Mar/2014:15:36:43 +0100"
}</programlisting>
<simpara>That&#8217;s it!  No extra fields in the output, and a cleaner config file because you
do not have to delete a "timestamp" field after conversion in the <literal>date</literal> filter.</simpara>
<simpara>Another use case is the CouchDB Changes input plugin (See
<ulink url="https://github.com/logstash-plugins/logstash-input-couchdb_changes">https://github.com/logstash-plugins/logstash-input-couchdb_changes</ulink>).
This plugin automatically captures CouchDB document field metadata into the
<literal>@metadata</literal> field within the input plugin itself.  When the events pass through
to be indexed by Elasticsearch, the Elasticsearch output plugin allows you to
specify the <literal>action</literal> (delete, update, insert, etc.) and the <literal>document_id</literal>, like
this:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">output {
  elasticsearch {
    action =&gt; "%{[@metadata][action]}"
    document_id =&gt; "%{[@metadata][_id]}"
    hosts =&gt; ["example.com"]
    index =&gt; "index_name"
    protocol =&gt; "http"
  }
}</programlisting>
</section>
<section id="environment-variables">
<title>Using Environment Variables in the Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<section id="_overview">
<title>Overview<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
You can set environment variable references in the configuration for Logstash plugins by using <literal>${var}</literal>.
</simpara>
</listitem>
<listitem>
<simpara>
At Logstash startup, each reference will be replaced by the value of the environment variable.
</simpara>
</listitem>
<listitem>
<simpara>
The replacement is case-sensitive.
</simpara>
</listitem>
<listitem>
<simpara>
References to undefined variables raise a Logstash configuration error.
</simpara>
</listitem>
<listitem>
<simpara>
You can give a default value by using the form <literal>${var:default value}</literal>. Logstash uses the default value if the
environment variable is undefined.
</simpara>
</listitem>
<listitem>
<simpara>
You can add environment variable references in any plugin option type : string, number, boolean, array, or hash.
</simpara>
</listitem>
<listitem>
<simpara>
Environment variables are immutable. If you update the environment variable, you&#8217;ll have to restart Logstash to pick up the updated value.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="_examples">
<title>Examples<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>The following examples show you how to use environment variables to set the values of some commonly used
configuration options.</simpara>
<section id="_setting_the_tcp_port">
<title>Setting the TCP Port<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>Here&#8217;s an example that uses an environment variable to set the TCP port:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  tcp {
    port =&gt; "${TCP_PORT}"
  }
}</programlisting>
<simpara>Now let&#8217;s set the value of <literal>TCP_PORT</literal>:</simpara>
<programlisting language="shell" linenumbering="unnumbered">export TCP_PORT=12345</programlisting>
<simpara>At startup, Logstash uses the following configuration:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  tcp {
    port =&gt; 12345
  }
}</programlisting>
<simpara>If the <literal>TCP_PORT</literal> environment variable is not set, Logstash returns a configuration error.</simpara>
<simpara>You can fix this problem by specifying a default value:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  tcp {
    port =&gt; "${TCP_PORT:54321}"
  }
}</programlisting>
<simpara>Now, instead of returning a configuration error if the variable is undefined, Logstash uses the default:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  tcp {
    port =&gt; 54321
  }
}</programlisting>
<simpara>If the environment variable is defined, Logstash uses the value specified for the variable instead of the default.</simpara>
</section>
<section id="_setting_the_value_of_a_tag">
<title>Setting the Value of a Tag<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>Here&#8217;s an example that uses an environment variable to set the value of a tag:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">filter {
  mutate {
    add_tag =&gt; [ "tag1", "${ENV_TAG}" ]
  }
}</programlisting>
<simpara>Let&#8217;s set the value of <literal>ENV_TAG</literal>:</simpara>
<programlisting language="shell" linenumbering="unnumbered">export ENV_TAG="tag2"</programlisting>
<simpara>At startup, Logstash uses the following configuration:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">filter {
  mutate {
    add_tag =&gt; [ "tag1", "tag2" ]
  }
}</programlisting>
</section>
<section id="_setting_a_file_path">
<title>Setting a File Path<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>Here&#8217;s an example that uses an environment variable to set the path to a log file:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">filter {
  mutate {
    add_field =&gt; {
      "my_path" =&gt; "${HOME}/file.log"
    }
  }
}</programlisting>
<simpara>Let&#8217;s set the value of <literal>HOME</literal>:</simpara>
<programlisting language="shell" linenumbering="unnumbered">export HOME="/path"</programlisting>
<simpara>At startup, Logstash uses the following configuration:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">filter {
  mutate {
    add_field =&gt; {
      "my_path" =&gt; "/path/file.log"
    }
  }
}</programlisting>
</section>
</section>
</section>
<section id="config-examples">
<title>Logstash Configuration Examples<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></title>
<simpara>The following examples illustrate how you can configure Logstash to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.</simpara>
<tip><simpara>If you need help building grok patterns, try out the
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/xpack-grokdebugger.html">Grok Debugger</ulink>. The Grok Debugger is an
X-Pack feature under the Basic License and is therefore <emphasis role="strong">free to use</emphasis>.</simpara></tip>
<bridgehead id="filter-example" renderas="sect3">Configuring Filters<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Filters are an in-line processing mechanism that provide the flexibility to slice and dice your data to fit your needs. Let&#8217;s take a look at some filters in action. The following configuration file sets up the <literal>grok</literal> and <literal>date</literal> filters.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input { stdin { } }

filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
  }
  date {
    match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
}

output {
  elasticsearch { hosts =&gt; ["localhost:9200"] }
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Run Logstash with this configuration:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">bin/logstash -f logstash-filter.conf</programlisting>
<simpara>Now, paste the following line into your terminal so it will be processed by the stdin input:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"</programlisting>
<simpara>You should see something returned to stdout that looks like this:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">{
        "message" =&gt; "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
     "@timestamp" =&gt; "2013-12-11T08:01:45.000Z",
       "@version" =&gt; "1",
           "host" =&gt; "cadenza",
       "clientip" =&gt; "127.0.0.1",
          "ident" =&gt; "-",
           "auth" =&gt; "-",
      "timestamp" =&gt; "11/Dec/2013:00:01:45 -0800",
           "verb" =&gt; "GET",
        "request" =&gt; "/xampp/status.php",
    "httpversion" =&gt; "1.1",
       "response" =&gt; "200",
          "bytes" =&gt; "3891",
       "referrer" =&gt; "\"http://cadenza/xampp/navi.php\"",
          "agent" =&gt; "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
}</programlisting>
<simpara>As you can see, Logstash (with help from the <literal>grok</literal> filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you&#8217;ll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it&#8217;s quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of <ulink url="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns">Logstash grok patterns</ulink> on GitHub.</simpara>
<simpara>The other filter used in this example is the <literal>date</literal> filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you&#8217;re ingesting the log data). You&#8217;ll notice that the <literal>@timestamp</literal> field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash "use this value as the timestamp for this event".</simpara>
<bridgehead id="_processing_apache_logs" renderas="sect3">Processing Apache Logs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Let&#8217;s do something that&#8217;s actually <emphasis role="strong">useful</emphasis>: process apache2 access log files! We are going to read the input from a file on the localhost, and use a <link linkend="conditionals">conditional</link> to process the event according to our needs. First, create a file called something like <emphasis>logstash-apache.conf</emphasis> with the following contents (you can change the log&#8217;s file path to suit your needs):</simpara>
<programlisting language="js" linenumbering="unnumbered">input {
  file {
    path =&gt; "/tmp/access_log"
    start_position =&gt; "beginning"
  }
}

filter {
  if [path] =~ "access" {
    mutate { replace =&gt; { "type" =&gt; "apache_access" } }
    grok {
      match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    }
  }
  date {
    match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
}

output {
  elasticsearch {
    hosts =&gt; ["localhost:9200"]
  }
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Then, create the input file you configured above (in this example, "/tmp/access_log") with the following log entries (or use some from your own webserver):</simpara>
<programlisting language="js" linenumbering="unnumbered">71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"</programlisting>
<simpara>Now, run Logstash with the -f flag to pass in the configuration file:</simpara>
<programlisting language="js" linenumbering="unnumbered">bin/logstash -f logstash-apache.conf</programlisting>
<simpara>Now you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type &#8658; "apache_access" line in the input configuration).</simpara>
<simpara>In this configuration, Logstash is only watching the apache access_log, but it&#8217;s easy enough to watch both the access_log and the error_log (actually, any file matching <literal>*log</literal>), by changing one line in the above configuration:</simpara>
<programlisting language="js" linenumbering="unnumbered">input {
  file {
    path =&gt; "/tmp/*_log"
...</programlisting>
<simpara>When you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you&#8217;ll see that the access_log is broken up into discrete fields, but the error_log isn&#8217;t. That&#8217;s because we used a <literal>grok</literal> filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn&#8217;t it be nice <emphasis role="strong">if</emphasis> we could control how a line was parsed, based on its format? Well, we can&#8230;</simpara>
<simpara>Note that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!</simpara>
<bridgehead id="using-conditionals" renderas="sect3">Using Conditionals<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>You use conditionals to control what events are processed by a filter or output. For example, you could label each event according to which file it appeared in (access_log, error_log, and other random files that end with "log").</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  file {
    path =&gt; "/tmp/*_log"
  }
}

filter {
  if [path] =~ "access" {
    mutate { replace =&gt; { type =&gt; "apache_access" } }
    grok {
      match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    }
    date {
      match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  } else if [path] =~ "error" {
    mutate { replace =&gt; { type =&gt; "apache_error" } }
  } else {
    mutate { replace =&gt; { type =&gt; "random_logs" } }
  }
}

output {
  elasticsearch { hosts =&gt; ["localhost:9200"] }
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>This example labels all events using the <literal>type</literal> field, but doesn&#8217;t actually parse the <literal>error</literal> or <literal>random</literal> files. There are so many types of error logs that how they should be labeled really depends on what logs you&#8217;re working with.</simpara>
<simpara>Similarly, you can use conditionals to direct events to particular outputs. For example, you could:</simpara>
<itemizedlist>
<listitem>
<simpara>
alert nagios of any apache events with status 5xx
</simpara>
</listitem>
<listitem>
<simpara>
record any 4xx status to Elasticsearch
</simpara>
</listitem>
<listitem>
<simpara>
record all status code hits via statsd
</simpara>
</listitem>
</itemizedlist>
<simpara>To tell nagios about any http event that has a 5xx status code, you
first need to check the value of the <literal>type</literal> field. If it&#8217;s apache, then you can
check to see if the <literal>status</literal> field contains a 5xx error. If it is, send it to nagios. If it isn&#8217;t
a 5xx error, check to see if the <literal>status</literal> field contains a 4xx error. If so, send it to Elasticsearch.
Finally, send all apache status codes to statsd no matter what the <literal>status</literal> field contains:</simpara>
<programlisting language="js" linenumbering="unnumbered">output {
  if [type] == "apache" {
    if [status] =~ /^5\d\d/ {
      nagios { ...  }
    } else if [status] =~ /^4\d\d/ {
      elasticsearch { ... }
    }
    statsd { increment =&gt; "apache.%{status}" }
  }
}</programlisting>
<bridgehead id="_processing_syslog_messages" renderas="sect3">Processing Syslog Messages<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc">Edit me</ulink></bridgehead>
<simpara>Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won&#8217;t need a functioning syslog instance; we&#8217;ll fake it from the command line so you can get a feel for what happens.</simpara>
<simpara>First, let&#8217;s make a simple configuration file for Logstash + syslog, called <emphasis>logstash-syslog.conf</emphasis>.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  tcp {
    port =&gt; 5000
    type =&gt; syslog
  }
  udp {
    port =&gt; 5000
    type =&gt; syslog
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match =&gt; { "message" =&gt; "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field =&gt; [ "received_at", "%{@timestamp}" ]
      add_field =&gt; [ "received_from", "%{host}" ]
    }
    date {
      match =&gt; [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}

output {
  elasticsearch { hosts =&gt; ["localhost:9200"] }
  stdout { codec =&gt; rubydebug }
}</programlisting>
<simpara>Run Logstash with this new configuration:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">bin/logstash -f logstash-syslog.conf</programlisting>
<simpara>Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we&#8217;ll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">telnet localhost 5000</programlisting>
<simpara>Copy and paste the following lines as samples. (Feel free to try some of your own, but keep in mind they might not parse if the <literal>grok</literal> filter is not correct for your data).</simpara>
<programlisting language="ruby" linenumbering="unnumbered">Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)
Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.</programlisting>
<simpara>Now you should see the output of Logstash in your original shell as it processes and parses messages!</simpara>
<programlisting language="ruby" linenumbering="unnumbered">{
                 "message" =&gt; "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)",
              "@timestamp" =&gt; "2013-12-23T22:30:01.000Z",
                "@version" =&gt; "1",
                    "type" =&gt; "syslog",
                    "host" =&gt; "0:0:0:0:0:0:0:1:52617",
        "syslog_timestamp" =&gt; "Dec 23 14:30:01",
         "syslog_hostname" =&gt; "louis",
          "syslog_program" =&gt; "CRON",
              "syslog_pid" =&gt; "619",
          "syslog_message" =&gt; "(www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)",
             "received_at" =&gt; "2013-12-23 22:49:22 UTC",
           "received_from" =&gt; "0:0:0:0:0:0:0:1:52617",
    "syslog_severity_code" =&gt; 5,
    "syslog_facility_code" =&gt; 1,
         "syslog_facility" =&gt; "user-level",
         "syslog_severity" =&gt; "notice"
}</programlisting>
</section>
<section id="multiple-pipelines">
<title>Multiple Pipelines<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/multiple-pipelines.asciidoc">Edit me</ulink></title>
<simpara>If you need to run more than one pipeline in the same process, Logstash provides a way to do this through a configuration file called <literal>pipelines.yml</literal>.
This file must be placed in the <literal>path.settings</literal> folder and follows this structure:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">- pipeline.id: my-pipeline_1
  path.config: "/etc/path/to/p1.config"
  pipeline.workers: 3
- pipeline.id: my-other-pipeline
  path.config: "/etc/different/path/p2.cfg"
  queue.type: persisted</programlisting>
<simpara>This file is formatted in YAML and contains a list of dictionaries, where each dictionary describes a pipeline, and each key/value pair specifies a setting for that pipeline. The example shows two different pipelines described by their IDs and  configuration paths. For the first pipeline, the value of <literal>pipeline.workers</literal> is set to 3, while in the other, the persistent queue feature is enabled.
The value of a setting that is not explicitly set in the <literal>pipelines.yml</literal> file will fall back to the default specified in the <literal>logstash.yml</literal> <link linkend="logstash-settings-file">settings file</link>.</simpara>
<simpara>When you start Logstash without arguments, it will read the <literal>pipelines.yml</literal> file and instantiate all pipelines specified in the file. On the other hand, when you use <literal>-e</literal> or <literal>-f</literal>, Logstash ignores the <literal>pipelines.yml</literal> file and logs a warning about it.</simpara>
<section id="multiple-pipeline-usage">
<title>Usage Considerations<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/multiple-pipelines.asciidoc">Edit me</ulink></title>
<simpara>Using multiple pipelines is especially useful if your current configuration has event flows that don&#8217;t share the same inputs/filters and outputs and are being separated from each other using tags and conditionals.</simpara>
<simpara>Having multiple pipelines in a single instance also allows these event flows to have different performance and durability parameters (for example, different settings for pipeline workers and persistent queues). This separation means that a blocked output in one pipeline won&#8217;t exert backpressure in the other.</simpara>
<simpara>That said, it&#8217;s important to take into account resource competition between the pipelines, given that the default values are tuned for a single pipeline. So, for example, consider reducing the number of pipeline workers used by each pipeline, because each pipeline will use 1 worker per CPU core by default.</simpara>
<simpara>Persistent queues and dead letter queues are isolated per pipeline, with their locations namespaced by the <literal>pipeline.id</literal> value.</simpara>
</section>
</section>
<section id="reloading-config">
<title>Reloading the Config File<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/reloading-config.asciidoc">Edit me</ulink></title>
<simpara>Starting with Logstash 2.3, you can set Logstash to detect and reload configuration
changes automatically.</simpara>
<simpara>To enable automatic config reloading, start Logstash with the <literal>--config.reload.automatic</literal> (or <literal>-r</literal>)
command-line option specified. For example:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash –f apache.config --config.reload.automatic</programlisting>
<note><simpara>The <literal>--config.reload.automatic</literal> option is not available when you specify the <literal>-e</literal> flag to pass
in  configuration settings from the command-line.</simpara></note>
<simpara>By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
use the <literal>--config.reload.interval &lt;interval&gt;</literal> option,  where <literal>interval</literal> specifies how often Logstash
checks the config files for changes.</simpara>
<simpara>If Logstash is already running without auto-reload enabled, you can force Logstash to
reload the config file and restart the pipeline by sending a SIGHUP (signal hangup) to the
process running Logstash. For example:</simpara>
<programlisting language="shell" linenumbering="unnumbered">kill -1 14175</programlisting>
<simpara>Where 14175 is the ID of the process running Logstash.</simpara>
<section id="_how_automatic_config_reloading_works">
<title>How Automatic Config Reloading Works<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/reloading-config.asciidoc">Edit me</ulink></title>
<simpara>When Logstash detects a change in a config file, it stops the current pipeline by stopping
all inputs, and it attempts to create a new pipeline that uses the updated configuration.
After validating the syntax of the new configuration, Logstash verifies that all inputs
and outputs can be initialized (for example, that all required ports are open). If the checks
are successful, Logstash swaps the existing pipeline with the new pipeline. If the checks
fail, the old pipeline continues to function, and the errors are propagated to the console.</simpara>
<simpara>During automatic config reloading, the JVM is not restarted. The creating and swapping of
pipelines all happens within the same process.</simpara>
<simpara>Changes to <link linkend="plugins-filters-grok">grok</link> pattern files are also reloaded, but only when
a change in the config file triggers a reload (or the pipeline is restarted).</simpara>
</section>
</section>
<section id="multiline">
<title>Managing Multiline Events<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc">Edit me</ulink></title>
<simpara>Several use cases generate events that span multiple lines of text. In order to correctly handle these multiline events,
Logstash needs to know how to tell which lines are part of a single event.</simpara>
<simpara>Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log
processing is to implement the processing as early in the pipeline as possible.</simpara>
<simpara>The <xref linkend="plugins-codecs-multiline"/> codec is the preferred tool for handling multiline events
in the Logstash pipeline. The multiline codec merges lines from a single input using
a simple set of rules.</simpara>
<important><simpara>If you are using a Logstash input plugin that supports multiple hosts, such as
the <xref linkend="plugins-inputs-beats"/> input plugin, you should not use the
<xref linkend="plugins-codecs-multiline"/> codec to handle multiline events. Doing so may result in the
mixing of streams and corrupted event data. In this situation, you need to handle multiline
events before sending the event data to Logstash.</simpara></important>
<simpara>The most important aspects of configuring the multiline codec are the following:</simpara>
<itemizedlist>
<listitem>
<simpara>
The <literal>pattern</literal> option specifies a regular expression. Lines that match the specified regular expression are considered
either continuations of a previous line or the start of a new multiline event. You can use
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/plugins-filters-grok.html">grok</ulink> regular expression templates with this configuration option.
</simpara>
</listitem>
<listitem>
<simpara>
The <literal>what</literal> option takes two values: <literal>previous</literal> or <literal>next</literal>. The <literal>previous</literal> value specifies that lines that match the
value in the <literal>pattern</literal> option are part of the previous line. The <literal>next</literal> value specifies that lines that match the value
in the <literal>pattern</literal> option are part of the following line.* The <literal>negate</literal> option applies the multiline codec to lines that
<emphasis>do not</emphasis> match the regular expression specified in the <literal>pattern</literal> option.
</simpara>
</listitem>
</itemizedlist>
<simpara>See the full documentation for the <xref linkend="plugins-codecs-multiline"/> codec plugin for more information
on configuration options.</simpara>
<section id="_examples_of_multiline_codec_configuration">
<title>Examples of Multiline Codec Configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc">Edit me</ulink></title>
<simpara>The examples in this section cover the following use cases:</simpara>
<itemizedlist>
<listitem>
<simpara>
Combining a Java stack trace into a single event
</simpara>
</listitem>
<listitem>
<simpara>
Combining C-style line continuations into a single event
</simpara>
</listitem>
<listitem>
<simpara>
Combining multiple lines from time-stamped events
</simpara>
</listitem>
</itemizedlist>
<section id="_java_stack_traces">
<title>Java Stack Traces<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc">Edit me</ulink></title>
<simpara>Java stack traces consist of multiple lines, with each line after the initial line beginning with whitespace, as in
this example:</simpara>
<programlisting language="java" linenumbering="unnumbered">Exception in thread "main" java.lang.NullPointerException
        at com.example.myproject.Book.getTitle(Book.java:16)
        at com.example.myproject.Author.getBookTitles(Author.java:25)
        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)</programlisting>
<simpara>To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  stdin {
    codec =&gt; multiline {
      pattern =&gt; "^\s"
      what =&gt; "previous"
    }
  }
}</programlisting>
<simpara>This configuration merges any line that begins with whitespace up to the previous line.</simpara>
</section>
<section id="_line_continuations">
<title>Line Continuations<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc">Edit me</ulink></title>
<simpara>Several programming languages use the <literal>\</literal> character at the end of a line to denote that the line continues, as in this
example:</simpara>
<programlisting language="c" linenumbering="unnumbered">printf ("%10.10ld  \t %10.10ld \t %s\
  %f", w, x, y, z );</programlisting>
<simpara>To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  stdin {
    codec =&gt; multiline {
      pattern =&gt; "\\$"
      what =&gt; "next"
    }
  }
}</programlisting>
<simpara>This configuration merges any line that ends with the <literal>\</literal> character with the following line.</simpara>
</section>
<section id="_timestamps">
<title>Timestamps<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc">Edit me</ulink></title>
<simpara>Activity logs from services such as Elasticsearch typically begin with a timestamp, followed by information on the
specific activity, as in this example:</simpara>
<programlisting language="shell" linenumbering="unnumbered">[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/
(/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]</programlisting>
<simpara>To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  file {
    path =&gt; "/var/log/someapp.log"
    codec =&gt; multiline {
      pattern =&gt; "^%{TIMESTAMP_ISO8601} "
      negate =&gt; true
      what =&gt; previous
    }
  }
}</programlisting>
<simpara>This configuration uses the <literal>negate</literal> option to specify that any line that does not begin with a timestamp belongs to
the previous line.</simpara>
</section>
</section>
</section>
<section id="glob-support">
<title>Glob Pattern Support<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/glob-support.asciidoc">Edit me</ulink></title>
<simpara>Logstash supports the following patterns wherever glob patterns are allowed:</simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>*</literal></emphasis>
</term>
<listitem>
<simpara>
Match any file. You can also use an <literal>*</literal> to restrict other values in the glob.
For example, <literal>*conf</literal> matches all files that end in <literal>conf</literal>. <literal>*apache*</literal> matches
any files with <literal>apache</literal> in the name. This pattern does not match hidden files
(dot files) on Unix-like operating systems. To match dot files, use a pattern
like <literal>{*,.*}</literal>.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>**</literal></emphasis>
</term>
<listitem>
<simpara>
Match directories recursively.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>?</literal></emphasis>
</term>
<listitem>
<simpara>
Match any one character.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>[set]</literal></emphasis>
</term>
<listitem>
<simpara>
Match any one character in a set. For example, <literal>[a-z]</literal>. Also supports set negation
(<literal>[^a-z]</literal>).
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>{p,q}</literal></emphasis>
</term>
<listitem>
<simpara>
Match either literal <literal>p</literal> or literal <literal>q</literal>. The matching literal can be more than one
character, and you can specify more than two literals. This pattern is the equivalent
to using alternation with the vertical bar in regular expressions (<literal>foo|bar</literal>).
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>\</literal></emphasis>
</term>
<listitem>
<simpara>
Escape the next metacharacter. This means that you cannot use a backslash in Windows
as part of a glob. The pattern <literal>c:\foo*</literal> will not work, so use <literal>foo*</literal> instead.
</simpara>
</listitem>
</varlistentry>
</variablelist>
<bridgehead id="example-glob-patterns" renderas="sect3">Example Patterns<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/glob-support.asciidoc">Edit me</ulink></bridgehead>
<simpara>Here are some common examples of glob patterns:</simpara>
<variablelist>
<varlistentry>
<term>
<literal>"/path/to/*.conf"</literal>
</term>
<listitem>
<simpara>
Matches config files ending in <literal>.conf</literal> in the specified path.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>"/var/log/*.log"</literal>
</term>
<listitem>
<simpara>
Matches log files ending in <literal>.log</literal> in the specified path.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>"/var/log/**/*.log</literal>
</term>
<listitem>
<simpara>
Matches log files ending in <literal>.log</literal> in subdirectories under the specified path.
</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>"/path/to/logs/{app1,app2,app3}/data.log"</literal>
</term>
<listitem>
<simpara>
Matches app log files in the <literal>app1</literal>, <literal>app2</literal>, and <literal>app3</literal> subdirectories under the
specified path.
</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section id="ingest-converter">
<title>Converting Ingest Node Pipelines<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/ingest-convert.asciidoc">Edit me</ulink></title>
<simpara>After implementing <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/ingest.html">ingest</ulink> pipelines to parse your data, you
might decide that you want to take advantage of the richer transformation
capabilities in Logstash. For example, you may need to use Logstash instead of
ingest pipelines if you want to:</simpara>
<itemizedlist>
<listitem>
<simpara>
Ingest from more inputs. Logstash can natively ingest data from many other
sources like TCP, UDP, syslog, and relational databases.
</simpara>
</listitem>
<listitem>
<simpara>
Use multiple outputs. Ingest node was designed to only support Elasticsearch
as an output, but you may want to use more than one output. For example, you may
want to archive your incoming data to S3 as well as indexing it in
Elasticsearch.
</simpara>
</listitem>
<listitem>
<simpara>
Take advantage of the richer transformation capabilities in Logstash, such as
external lookups.
</simpara>
</listitem>
<listitem>
<simpara>
Use the persistent queue feature to handle spikes when ingesting data (from
Beats and other sources).
</simpara>
</listitem>
</itemizedlist>
<simpara>To make it easier for you to migrate your configurations, Logstash provides an
ingest pipeline conversion tool. The conversion tool takes the ingest pipeline
definition as input and, when possible, creates the equivalent Logstash
configuration as output.</simpara>
<simpara>See <xref linkend="ingest-converter-limitations"/> for a full list of tool limitations.</simpara>
<section id="ingest-converter-run">
<title>Running the tool<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/ingest-convert.asciidoc">Edit me</ulink></title>
<simpara>You&#8217;ll find the conversion tool in the <literal>bin</literal> directory of your Logstash
installation. See <xref linkend="dir-layout"/> to find the location of <literal>bin</literal> on your system.</simpara>
<simpara>To run the conversion tool, use the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/ingest-convert.sh --input INPUT_FILE_URI --output OUTPUT_FILE_URI [--append-stdio]</programlisting>
<simpara>Where:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>INPUT_FILE_URI</literal> is a file URI that specifies the full path to the JSON file
that defines the ingest node pipeline.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>OUTPUT_FILE_URI</literal> is the file URI of the Logstash DSL file that will be
generated by the tool.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>--append-stdio</literal> is an optional flag that adds stdin and stdout sections to
the config instead of adding the default Elasticsearch output.
</simpara>
</listitem>
</itemizedlist>
<simpara>This command expects a file URI, so make sure you use forward slashes and
specify the full path to the file.</simpara>
<simpara>For example:</simpara>
<programlisting language="text" linenumbering="unnumbered">bin/ingest-convert.sh --input file:///tmp/ingest/apache.json --output file:///tmp/ingest/apache.conf</programlisting>
</section>
<section id="ingest-converter-limitations">
<title>Limitations<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/ingest-convert.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Painless script conversion is not supported.
</simpara>
</listitem>
<listitem>
<simpara>
Only a subset of available processors are
<link linkend="ingest-converter-supported-processors">supported</link> for conversion. For
processors that are not supported, the tool produces a warning and continues
with a best-effort conversion.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="ingest-converter-supported-processors">
<title>Supported Processors<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/ingest-convert.asciidoc">Edit me</ulink></title>
<simpara>The following ingest node processors are currently supported for conversion by
the tool:</simpara>
<itemizedlist>
<listitem>
<simpara>
Append
</simpara>
</listitem>
<listitem>
<simpara>
Convert
</simpara>
</listitem>
<listitem>
<simpara>
Date
</simpara>
</listitem>
<listitem>
<simpara>
GeoIP
</simpara>
</listitem>
<listitem>
<simpara>
Grok
</simpara>
</listitem>
<listitem>
<simpara>
Gsub
</simpara>
</listitem>
<listitem>
<simpara>
Json
</simpara>
</listitem>
<listitem>
<simpara>
Lowercase
</simpara>
</listitem>
<listitem>
<simpara>
Rename
</simpara>
</listitem>
<listitem>
<simpara>
Set
</simpara>
</listitem>
</itemizedlist>
<remark> Centralized configuration managements</remark>
</section>
</section>
</chapter>
<chapter id="config-management">
<title>Managing Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/ingest-convert.asciidoc">Edit me</ulink></title>
<simpara>Logstash provides configuration management features to make it easier for you to
manage updates to your configuration over time.</simpara>
<simpara>The topics in this section describe Logstash configuration management features
only. For information about other config management tools, such as Puppet and
Chef, see the documentation for those projects. Also take a look at the
<ulink url="https://forge.puppet.com/elastic/logstash">Logstash Puppet module documentation</ulink>.</simpara>
<section id="logstash-centralized-pipeline-management" role="xpack">
<title>Centralized Pipeline Management</title>
<note><simpara>Centralized pipeline management is an X-Pack feature that
requires a paid X-Pack license. See the
<ulink url="https://www.elastic.co/subscriptions">Elastic Subscriptions</ulink> page for information
about obtaining a license.</simpara></note>
<simpara>The pipeline management feature in X-Pack centralizes the creation and
management of Logstash configuration pipelines. From within the pipeline
management UI in Kibana, you can control multiple Logstash instances. You can
add, edit, and delete pipeline configurations. On the Logstash side, you simply
need to enable configuration management and register Logstash to use the
centrally managed pipeline configurations.</simpara>
<simpara>The pipeline configurations, along with some metadata, are stored in
Elasticsearch. Any changes that you make to a pipeline definition in the UI are
picked up and loaded automatically by all Logstash instances registered to use
the pipeline. The changes are applied immediately; you do not have to restart
Logstash to pick up the changes, as long as Logtash is already registered to
use the pipeline.</simpara>
<section id="_managing_pipelines">
<title>Managing Pipelines</title>
<simpara>Before using the pipeline management UI, you must:</simpara>
<itemizedlist>
<listitem>
<simpara>
<link linkend="configuring-centralized-pipelines">Configure centralized pipeline management</link>.
</simpara>
</listitem>
<listitem>
<simpara>
If Kibana is protected with basic authentication, make sure your Kibana user has
the <literal>logstash_admin</literal> role.
</simpara>
</listitem>
</itemizedlist>
<simpara>To centrally manage Logstash pipelines:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Open Kibana in your browser and go to the Management tab. If you&#8217;ve set up
configuration management correctly, you&#8217;ll see an area for managing Logstash.
Click the <emphasis role="strong">Pipelines</emphasis> link.
</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="management/images/centralized_config.png"/>
  </imageobject>
  <textobject><phrase>management/images/centralized_config.png</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>
To add a new pipeline, click the <emphasis role="strong">Add</emphasis> button and specify values for the
following fields:
</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
Pipeline ID
</simpara>
</entry>
<entry>
<simpara>
A name that uniquely identifies the pipeline. This is the ID that you used when
you
<link linkend="configuring-centralized-pipelines">configured centralized pipeline management</link>
and specified a list of pipeline IDs in the <literal>xpack.management.pipeline.id</literal>
setting.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
Description
</simpara>
</entry>
<entry>
<simpara>
A description of the pipeline configuration. This information is for your use.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
Pipeline
</simpara>
</entry>
<entry>
<simpara>
The pipeline configuration. You can treat the editor in the pipeline management
UI like any other editor. You don&#8217;t have to worry about whitespace or indentation.
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="management/images/new_pipeline.png"/>
  </imageobject>
  <textobject><phrase>management/images/new_pipeline.png</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>
Click <emphasis role="strong">Save</emphasis>.
</simpara>
</listitem>
</orderedlist>
<simpara>The pipeline runs on all Logstash instances that are registered to use the
pipeline. There is no validation done at the UI level. The UI will save the new
configuration, and Logstash will attempt to load it. You need to check the local
Logstash logs for configuration errors. If you&#8217;re using the Logstash monitoring
feature in X-Pack, you can also navigate to the Monitoring tab to check the
status of your Logstash nodes.</simpara>
<simpara>You can specify multiple pipeline configurations that run in parallel on the
same Logstash node.</simpara>
<simpara>If you edit a pipeline configuration and save the changes, Logstash reloads
the configuration in the background and continues processing events.</simpara>
<simpara>If you delete a pipeline (for example, <literal>apache</literal>) from the UI, Logstash will
attempt to stop the pipeline if it&#8217;s running. Logstash will wait until all
events have been fully processed by the pipeline. Before deleting a pipeline,
make sure you understand your data sources because stopping a pipeline may
lead to data loss.</simpara>
<remark> Working with Logstash Modules</remark>
</section>
</section>
</chapter>
<chapter id="logstash-modules">
<title>Working with Logstash Modules</title>
<simpara>Logstash modules provide a quick, end-to-end solution for ingesting data and
visualizing it with purpose-built dashboards.</simpara>
<simpara>Each module comes pre-packaged with Logstash configurations, Kibana dashboards,
and other meta files that make it easier for you to set up the Elastic Stack for
specific use cases or data sources.</simpara>
<simpara>You can think of modules as providing three essential functions that make it
easier for you to get started. When you run a module, it will:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Create the Elasticsearch index.
</simpara>
</listitem>
<listitem>
<simpara>
Set up the Kibana dashboards, including the index pattern, searches, and
visualizations required to visualize your data in Kibana.
</simpara>
</listitem>
<listitem>
<simpara>
Run the Logstash pipeline with the configurations required to read and parse
the data.
</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/logstash-module-overview.png"/>
  </imageobject>
  <textobject><phrase>Logstash modules overview</phrase></textobject>
</mediaobject>
</informalfigure>
<bridgehead id="running-logstash-modules" renderas="sect2">Running modules</bridgehead>
<simpara>To run a module and set up dashboards, you specify the following options:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules MODULE_NAME --setup [-M "CONFIG_SETTING=VALUE"]</programlisting>
<remark>TODO: For 6.0, show how to run mutliple modules</remark>
<simpara>Where:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>--modules</literal> runs the Logstash module specified by <literal>MODULE_NAME</literal>.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>-M "CONFIG_SETTING=VALUE"</literal> is optional and overrides the specified
configuration setting. You can specify multiple overrides. Each override must
start with <literal>-M</literal>. See <xref linkend="overriding-logstash-module-settings"/> for more info.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>--setup</literal> creates an index pattern in Elasticsearch and imports Kibana
dashboards and visualizations. Running <literal>--setup</literal> is a one-time setup step. Omit
this option for subsequent runs of the module to avoid overwriting existing
Kibana dashboards.
</simpara>
</listitem>
</itemizedlist>
<simpara>For example, the following command runs the Netflow module with the default
settings, and sets up the netflow index pattern and dashboards:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules netflow --setup</programlisting>
<simpara>The following command runs the Netflow module and overrides the Elasticsearch
<literal>host</literal> setting. Here it&#8217;s assumed that you&#8217;ve already run the setup step.</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules netflow -M "netflow.var.elasticsearch.host=es.mycloud.com"</programlisting>
<bridgehead id="configuring-logstash-modules" renderas="sect2">Configuring modules</bridgehead>
<simpara>To configure a module, you can either
<link linkend="setting-logstash-module-config">specify configuration settings</link> in the
<literal>logstash.yml</literal> <link linkend="logstash-settings-file">settings file</link>, or use command-line overrides to
<link linkend="overriding-logstash-module-settings">specify settings at the command line</link>.</simpara>
<bridgehead id="setting-logstash-module-config" renderas="sect3">Specify module settings in <literal>logstash.yml</literal></bridgehead>
<simpara>To specify module settings in the <literal>logstash.yml</literal>
<link linkend="logstash-settings-file">settings file</link> file, you add a module definition to
the modules array. Each module definition begins with a dash (-) and is followed
by <literal>name: module_name</literal> then a series of name/value pairs that specify module
settings. For example:</simpara>
<programlisting language="shell" linenumbering="unnumbered">modules:
- name: netflow
  var.elasticsearch.hosts: "es.mycloud.com"
  var.elasticsearch.username: "foo"
  var.elasticsearch.password: "password"
  var.kibana.host: "kb.mycloud.com"
  var.kibana.username: "foo"
  var.kibana.password: "password"
  var.input.tcp.port: 5606</programlisting>
<simpara>For a list of available module settings, see the documentation for the module.</simpara>
<bridgehead id="overriding-logstash-module-settings" renderas="sect3">Specify module settings at the command line</bridgehead>
<simpara>You can override module settings by specifying one or more configuration
overrides when you start Logstash. To specify an override, you use the <literal>-M</literal>
command line option:</simpara>
<programlisting language="shell" linenumbering="unnumbered">-M MODULE_NAME.var.PLUGINTYPE1.PLUGINNAME1.KEY1=VALUE</programlisting>
<simpara>Notice that the fully-qualified setting name includes the module name.</simpara>
<simpara>You can specify multiple overrides. Each override must start with <literal>-M</literal>.</simpara>
<simpara>The following command runs the Netflow module and overrides both the
Elasticsearch <literal>host</literal> setting and the <literal>udp.port</literal> setting:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules netflow -M "netflow.var.input.udp.port=3555" -M "netflow.var.elasticsearch.hosts=my-es-cloud"</programlisting>
<simpara>Any settings defined in the command line are ephemeral and will not persist across
subsequent runs of Logstash. If you want to persist a configuration, you need to
set it in the <literal>logstash.yml</literal> <link linkend="logstash-settings-file">settings file</link>.</simpara>
<simpara>Settings that you specify at the command line are merged with any settings
specified in the <literal>logstash.yml</literal> file. If an option is set in both
places, the value specified at the command line takes precedence.</simpara>
<section id="connecting-to-cloud">
<title>Using Elastic Cloud</title>
<simpara>Logstash comes with two settings that simplify using modules with <ulink url="https://cloud.elastic.co/">Elastic Cloud</ulink>.
The Elasticsearch and Kibana hostnames in Elastic Cloud may be hard to set
in the Logstash config or on the commandline, so a Cloud ID can be used instead.</simpara>
<section id="_cloud_id">
<title>Cloud ID</title>
<simpara>The Cloud ID, which can be found in the Elastic Cloud web console, is used by
Logstash to build the Elasticsearch and Kibana hosts settings.
It is a base64 encoded text value of about 120 characters made up of upper and
lower case letters and numbers.
If you have several Cloud IDs, you can add a label, which is ignored
internally, to help you tell them apart. To add a label you should prefix your
Cloud ID with a label and a <literal>:</literal> separator in this format "&lt;label&gt;:&lt;cloud-id&gt;"</simpara>
<simpara><literal>cloud.id</literal> will overwrite these settings:</simpara>
<screen>var.elasticsearch.hosts
var.kibana.host</screen>
</section>
<section id="_cloud_auth">
<title>Cloud Auth</title>
<simpara>This is optional. Construct this value by following this format "&lt;username&gt;:&lt;password&gt;".
Use your Cloud username for the first part. Use your Cloud password for the second part,
which is given once in the Cloud UI when you create a cluster.
As your Cloud password is changeable, if you change it in the Cloud UI remember to change it here too.</simpara>
<simpara><literal>cloud.auth</literal> when specified will overwrite these settings:</simpara>
<screen>var.elasticsearch.username
var.elasticsearch.password
var.kibana.username
var.kibana.password</screen>
<simpara>Example:</simpara>
<simpara>These settings can be specified in the <literal>logstash.yml</literal> <link linkend="logstash-settings-file">settings file</link>.
They should be added separately from any module configuration settings you may have added before.</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># example with a label
cloud.id: "staging:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy"
cloud.auth: "elastic:YOUR_PASSWORD"</programlisting>
<programlisting language="yaml" linenumbering="unnumbered"># example without a label
cloud.id: "dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy"
cloud.auth: "elastic:YOUR_PASSWORD"</programlisting>
<simpara>These settings can be also specified at the command line, like this:</simpara>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash --modules netflow -M "netflow.var.input.udp.port=3555" --cloud.id &lt;cloud-id&gt; --cloud.auth &lt;cloud.auth&gt;</programlisting>
</section>
</section>
<section id="arcsight-module" role="xpack">
<title>Logstash ArcSight Module</title>
<titleabbrev>ArcSight Module</titleabbrev>
<note><simpara>The Logstash ArcSight module is an
<ulink url="https://www.elastic.co/products/x-pack">X-Pack</ulink> feature under the Basic License
and is therefore free to use. Please contact
<ulink url="mailto:arcsight@elastic.co">arcsight@elastic.co</ulink> for questions or more
information.</simpara></note>
<simpara>The Logstash ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack.
With a single command, the module taps directly into the ArcSight Smart Connector or the Event Broker,
parses and indexes the security events into Elasticsearch, and installs a suite of Kibana dashboards
to get you exploring your data immediately.</simpara>
<section id="arcsight-architecture">
<title>Deployment Architecture</title>
<simpara>The Logstash ArcSight module understands CEF (Common Event Format), and can
accept, enrich, and index these events for analysis on the Elastic Stack. ADP
contains two core data collection components for data streaming:</simpara>
<itemizedlist>
<listitem>
<simpara>
The <emphasis>Smart Connectors (SC)</emphasis> are edge log collectors that parse and normalize
data to CEF prior to publishing to the Logstash receiver.
</simpara>
</listitem>
<listitem>
<simpara>
The <emphasis>Event Broker (EB)</emphasis> is the central hub for incoming data and is based on
open source Apache Kafka. The Logstash ArcSight module can consume directly from
EB topics.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="arcsight-getting-started-smartconnector">
<title>Getting Started With The Smart Connector</title>
<simpara>To get started, you can use a basic Elastic Stack setup that reads events from
the Smart Connector directly.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/arcsight-diagram-smart-connectors.svg"/>
  </imageobject>
  <textobject><phrase>ArcSight Smart Connector architecture</phrase></textobject>
</mediaobject>
</informalfigure>
<section id="arcsight-requirements-smartconnector">
<title>Requirements</title>
<itemizedlist>
<listitem>
<simpara>
These instructions assume you have part of the Elastic Stack (Logstash, Elasticsearch,
Kibana) already installed. The products you need are
<ulink url="https://www.elastic.co/downloads">available to download</ulink> and easy to install. The
Elastic Stack 5.6 or higher is required for this module.
</simpara>
</listitem>
<listitem>
<simpara>
The Elastic Stack is running locally with default ports exposed, namely
Elasticsearch as "localhost:9200" and Kibana as "localhost:5601". Note that you can also run
Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
</simpara>
</listitem>
<listitem>
<simpara>
Smart Connector has been configured to publish ArcSight data (to TCP port <literal>5000</literal>) using the CEF syslog
destination.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="arcsight-instructions-smartconnector">
<title>Instructions</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/installing-xpack-es.html">Install X-Pack on Elasticsearch</ulink> and then start
Elasticsearch.
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/installing-xpack-kb.html">Install X-Pack on Kibana</ulink> and then start
Kibana.
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/installing-xpack-log.html">Install X-Pack on Logstash</ulink>, which
includes the Logstash ArcSight module.
</simpara>
</listitem>
<listitem>
<simpara>
Start the Logstash ArcSight module by running the following command in the
Logstash install directory with your respective EB host and port:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules arcsight --setup
  -M "arcsight.var.inputs=smartconnector"
  -M "arcsight.var.elasticsearch.username=elastic"
  -M "arcsight.var.elasticsearch.password=YOUR_PASSWORD"
  -M "arcsight.var.kibana.username=elastic"
  -M "arcsight.var.kibana.password=YOUR_PASSWORD"</programlisting>
<tip><simpara>The command in this example is formatted for readability. Remove the line
breaks before running this command.</simpara></tip>
<simpara>The <literal>--modules arcsight</literal> option spins up an ArcSight CEF-aware Logstash
pipeline for ingestion. The <literal>--setup</literal> option creates an <literal>arcsight-*</literal> index
pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
subsequent module runs or when scaling out the Logstash deployment,
the <literal>--setup</literal> option should be omitted to avoid overwriting the existing Kibana
dashboards.</simpara>
</listitem>
<listitem>
<simpara>
Explore your data with Kibana:
</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>
Open browser @ <ulink url="http://localhost:5601">http://localhost:5601</ulink> (username:
  "elastic"; password: "YOUR_PASSWORD")
</simpara>
</listitem>
<listitem>
<simpara>
Open the <emphasis role="strong">[ArcSight] Network Overview Dashboard</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
See <xref linkend="exploring-data-arcsight"/> for additional details on data exploration.
</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>See <xref linkend="configuring-arcsight"/> if you want to specify additional options that
control the behavior of the ArcSight module.</simpara>
</section>
</section>
<section id="arcsight-getting-started-eventbroker">
<title>Getting Started With The Event Broker</title>
<simpara>To get started, you can use a basic Elastic Stack setup that reads events from
the EB event stream.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/arcsight-diagram-adp.svg"/>
  </imageobject>
  <textobject><phrase>ArcSight Event Broker architecture</phrase></textobject>
</mediaobject>
</informalfigure>
<section id="arcsight-requirements-eventbroker">
<title>Requirements</title>
<itemizedlist>
<listitem>
<simpara>
These instructions assume you have the Elastic Stack (Logstash, Elasticsearch,
Kibana) already installed. The products you need are
<ulink url="https://www.elastic.co/downloads">available to download</ulink> and easy to install. The
Elastic Stack 5.6 or higher is required for this module.
</simpara>
</listitem>
<listitem>
<simpara>
The Elastic Stack is running locally with default ports exposed, namely
Elasticsearch as "localhost:9200" and Kibana as "localhost:5601".
</simpara>
</listitem>
<listitem>
<simpara>
By default, the Logstash ArcSight module consumes from the EB "eb-cef" topic.
For additional EB settings, see <xref linkend="arcsight-module-config"/>. Consuming from a
secured EB port is not currently available.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="arcsight-instructions-eventbroker">
<title>Instructions</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/installing-xpack-es.html">Install X-Pack on Elasticsearch</ulink> and then start
Elasticsearch.
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/installing-xpack-kb.html">Install X-Pack on Kibana</ulink> and then start
Kibana.
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/installing-xpack-log.html">Install X-Pack on Logstash</ulink>, which
includes the Logstash ArcSight module. Then update the Logstash
<link linkend="plugins-inputs-kafka">Kafka input plugin</link> to an EB compatible version. In the
Logstash install directory, run:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin install x-pack
bin/logstash-plugin install --version 6.2.7 logstash-input-kafka</programlisting>
</listitem>
<listitem>
<simpara>
Start the Logstash ArcSight module by running the following command in the
Logstash install directory with your respective EB host and port:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules arcsight --setup
  -M "arcsight.var.elasticsearch.username=elastic"
  -M "arcsight.var.elasticsearch.password=YOUR_PASSWORD"
  -M "arcsight.var.kibana.username=elastic"
  -M "arcsight.var.kibana.password=YOUR_PASSWORD"</programlisting>
<tip><simpara>The command in this example is formatted for readability. Remove the line
breaks before running this command.</simpara></tip>
<simpara>The <literal>--modules arcsight</literal> option spins up an ArcSight CEF-aware Logstash
pipeline for ingestion. The <literal>--setup</literal> option creates an <literal>arcsight-*</literal> index
pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
subsequent module runs or when scaling out the Logstash deployment,
the <literal>--setup</literal> option should be omitted to avoid overwriting the existing Kibana
dashboards.</simpara>
</listitem>
<listitem>
<simpara>
Explore your data with Kibana:
</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>
Open browser @ <ulink url="http://localhost:5601">http://localhost:5601</ulink> (username:
  "elastic"; password: "YOUR_PASSWORD")
</simpara>
</listitem>
<listitem>
<simpara>
Open the <emphasis role="strong">[ArcSight] Network Overview Dashboard</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
See <xref linkend="exploring-data-arcsight"/> for additional details on data exploration.
</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>See <xref linkend="configuring-arcsight"/> if you want to specify additional options that
control the behavior of the ArcSight module.</simpara>
</section>
</section>
<section id="exploring-data-arcsight">
<title>Exploring Your Security Data</title>
<simpara>Once the Logstash ArcSight module starts receiving events, you can immediately
begin using the packaged Kibana dashboards to explore and visualize your
security data. The dashboards rapidly accelerate the time and effort required
for security analysts and operators to gain situational and behavioral insights
on network, endpoint, and DNS events flowing through the environment. You can
use the dashboards as-is, or tailor them to work better with existing use cases
and business requirements.</simpara>
<simpara>The dashboards have a navigation pane for context switching and drill downs
across three core use cases:</simpara>
<itemizedlist>
<listitem>
<simpara>
<emphasis role="strong">Network Data</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Dashboards: Network Overview, Network Suspicious Activity
</simpara>
</listitem>
<listitem>
<simpara>
Data Types: Network firewalls, intrusion systems, VPN devices
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">Endpoint Data</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Dashboards: Endpoint Overview, Endpoint OS Activity
</simpara>
</listitem>
<listitem>
<simpara>
Data Types: Operating systems, applications, host intrusion systems
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">DNS Data</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Dashboards: Microsoft DNS Overview
</simpara>
</listitem>
<listitem>
<simpara>
Data Types: Microsoft DNS devices
</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<section id="network-dashboards-arsight">
<title>Example Network Dashboards</title>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/arcsight-network-overview.png"/>
  </imageobject>
  <textobject><phrase>Network overview dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/arcsight-network-suspicious.png"/>
  </imageobject>
  <textobject><phrase>Network suspicious activity dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>These Kibana visualizations enable you to quickly understand the top devices,
endpoints, attackers, and targets. This insight, along with the ability to
instantly drill down on a particular host, port, device, or time range, offers a
holistic view across the entire environment to identify specific segments that
may require immediate attention or action. You can easily discover answers to
questions like:</simpara>
<itemizedlist>
<listitem>
<simpara>
Who are my attackers and what are they targeting?
</simpara>
</listitem>
<listitem>
<simpara>
Which of my devices or endpoints are the busiest and what services were
rendered?
</simpara>
</listitem>
<listitem>
<simpara>
How many unique attackers, techniques, signatures, or targets were triggered
at any given point in time?
</simpara>
</listitem>
<listitem>
<simpara>
What are the top sources, destinations, protocols, and behaviors that are
causing the elevated count of failures?
</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section id="configuring-arcsight">
<title>Configuring the Module</title>
<simpara>You can specify additional options for the Logstash ArcSight module in the
<literal>logstash.yml</literal> configuration file or with overrides through the command line
like in the getting started. For more information about configuring modules, see
<xref linkend="logstash-modules"/>.</simpara>
<simpara>As an example, the following settings can be appended to <literal>logstash.yml</literal> to
configure your module:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">modules:
  - name: arcsight
    var.input.eventbroker.bootstrap_servers: "eb_host:39092"
    var.input.eventbroker.topics: "eb_topic"
    var.elasticsearch.hosts: "localhost:9200"
    var.elasticsearch.username: "elastic"
    var.elasticsearch.password: "YOUR_PASSWORD"
    var.kibana.host: “localhost:5601”
    var.kibana.username: "elastic"
    var.kibana.password: "YOUR_PASSWORD"</programlisting>
<section id="arcsight-module-config">
<title>Logstash ArcSight Module Configuration Options</title>
<simpara>The ArcSight module provides the following settings for configuring the behavior
of the module. These settings include ArcSight-specific options plus common
options that are supported by all Logstash modules.</simpara>
<simpara>When you override a setting at the command line, remember to prefix the setting
with the module name, for example, <literal>arcsight.var.inputs</literal> instead of <literal>var.inputs</literal>.</simpara>
<simpara>If you don&#8217;t specify configuration settings, Logstash uses the defaults.</simpara>
<simpara><emphasis role="strong">ArcSight Module Options</emphasis></simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.inputs</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "eventbroker"
</simpara>
</listitem>
</itemizedlist>
<simpara>Set the input(s) to expose for the Logstash ArcSight module. Valid settings are
"eventbroker", "smartconnector", or "eventbroker,smartconnector" (exposes both
inputs concurrently).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.input.eventbroker.bootstrap_servers</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "localhost:39092"
</simpara>
</listitem>
</itemizedlist>
<simpara>A list of EB URLs to use for establishing the initial connection to the cluster.
This list should be in the form of <literal>host1:port1,host2:port2</literal>. These URLs are
just used for the initial connection to discover the full cluster membership
(which may change dynamically), so this list need not contain the full set of
servers (you may want more than one, though, in case a server is down).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.input.eventbroker.topics</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is ["eb-cef"]
</simpara>
</listitem>
</itemizedlist>
<simpara>A list of EB topics to subscribe to.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.input.smartconnector.port</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is 5000
</simpara>
</listitem>
</itemizedlist>
<simpara>The TCP port to listen on when receiving data from SCs.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara><emphasis role="strong">Common options</emphasis></simpara>
<simpara>The following configuration options are supported by all modules:</simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.hosts</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="uri">uri</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "localhost:9200"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the host(s) of the Elasticsearch cluster. For each host, you must specify
the hostname and port. For example, "myhost:9200". If given an <link linkend="array">array</link>,
Logstash will load balance requests across the hosts specified in the hosts
parameter. It is important to exclude <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/modules-node.html">dedicated master
nodes</ulink> from the hosts list to prevent Logstash from sending bulk requests to the
master nodes. So this parameter should only reference either data or client
nodes in Elasticsearch.</simpara>
<simpara>Any special characters present in the URLs here MUST be URL escaped! This means #
should be put in as %23 for instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.username</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "elastic"
</simpara>
</listitem>
</itemizedlist>
<simpara>The username to authenticate to a secure Elasticsearch cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.password</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "changeme"
</simpara>
</listitem>
</itemizedlist>
<simpara>The password to authenticate to a secure Elasticsearch cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.enabled</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Enable SSL/TLS secured communication to the Elasticsearch cluster. Leaving this
unspecified will use whatever scheme is specified in the URLs listed in <literal>hosts</literal>.
If no explicit protocol is specified, plain HTTP will be used. If SSL is
explicitly disabled here, the plugin will refuse to start if an HTTPS URL is
given in hosts.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.verification_mode</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "strict"
</simpara>
</listitem>
</itemizedlist>
<simpara>The hostname verification setting when communicating with Elasticsearch. Set to
<literal>disable</literal> to turn off hostname verification. Disabling this has serious security
concerns.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.certificate_authority</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use to validate SSL certificates when
communicating with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.certificate</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use for client authentication when
communicating with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.key</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to the certificate key for client authentication when communicating
with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.host</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "localhost:5601"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the hostname and port of the Kibana instance to use for importing
dashboards and visualizations. For example: "myhost:5601".</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.scheme</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "http"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the protocol to use for reaching the Kibana instance. The options are:
"http" or "https". The default is "http".</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.username</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "elastic"
</simpara>
</listitem>
</itemizedlist>
<simpara>The username to authenticate to a secured Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.password</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "changeme"
</simpara>
</listitem>
</itemizedlist>
<simpara>The password to authenticate to a secure Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.enabled</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is false
</simpara>
</listitem>
</itemizedlist>
<simpara>Enable SSL/TLS secured communication to the Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.verification_mode</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "strict"
</simpara>
</listitem>
</itemizedlist>
<simpara>The hostname verification setting when communicating with Kibana. Set to
<literal>disable</literal> to turn off hostname verification. Disabling this has serious security
concerns.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.certificate_authority</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use to validate SSL certificates when
communicating with Kibana.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.certificate</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use for client authentication when
communicating with Kibana.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.key</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to the certificate key for client authentication when communicating
with Kibana.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
</section>
<section id="netflow-module">
<title>Logstash Netflow Module</title>
<titleabbrev>Netflow Module</titleabbrev>
<simpara>The Logstash Netflow module simplifies the collection, normalization, and
visualization of network flow data. With a single command, the module parses
network flow data, indexes the events into Elasticsearch, and installs a suite
of Kibana dashboards to get you exploring your data immediately.</simpara>
<simpara>Logstash modules support Netflow Version 5 and 9.</simpara>
<section id="_what_is_flow_data">
<title>What is Flow Data?</title>
<simpara>Netflow is a type of data record streamed from capable network devices. It
contains information about connections traversing the device, and includes
source IP addresses and ports, destination IP addresses and ports, types of
service, VLANs, and other information that can be encoded into frame and
protocol headers. With Netflow data, network operators can go beyond monitoring
simply the volume of data crossing their networks. They can understand where the
traffic originated, where it is going, and what services or applications it is
part of.</simpara>
<section id="netflow-requirements">
<title>Requirements</title>
<simpara>These instructions assume you have already installed Elastic Stack
(Logstash, Elasticsearch, and Kibana) version 5.6 or higher. The products you
need are <ulink url="https://www.elastic.co/downloads">available to download</ulink> and easy to
install.</simpara>
</section>
</section>
<section id="netflow-getting-started">
<title>Getting Started</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Start the Logstash Netflow module by running the following command in the
Logstash installation directory:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules netflow --setup -M netflow.var.input.udp.port=NNNN</programlisting>
<simpara>Where <literal>NNNN</literal> is the UDP port on which Logstash will listen for network traffic
data. If you don&#8217;t specify a port, Logstash listens on port 2055 by default.</simpara>
<simpara>The <literal>--modules netflow</literal> option spins up a Netflow-aware Logstash pipeline
for ingestion.</simpara>
<simpara>The <literal>--setup</literal> option creates a <literal>netflow-*</literal> index pattern in Elasticsearch and
imports Kibana dashboards and visualizations. Running <literal>--setup</literal> is a one-time
setup step. Omit this option for subsequent runs of the module to avoid
overwriting existing Kibana dashboards.</simpara>
<simpara>The command shown here assumes that you&#8217;re running Elasticsearch and Kibana on
your localhost. If you&#8217;re not, you need to specify additional connection
options. See <xref linkend="configuring-netflow"/>.</simpara>
</listitem>
<listitem>
<simpara>
Explore your data in Kibana:
</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>
Open your browser and navigate to
<ulink url="http://localhost:5601">http://localhost:5601</ulink>. If security is enabled, you&#8217;ll
need to specify the Kibana username and password that you used when you set up
security.
</simpara>
</listitem>
<listitem>
<simpara>
Open <emphasis role="strong">Netflow: Network Overview Dashboard</emphasis>.
</simpara>
</listitem>
<listitem>
<simpara>
See <xref linkend="exploring-data-netflow"/> for additional details on data exploration.
</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section id="exploring-data-netflow">
<title>Exploring Your Data</title>
<simpara>Once the Logstash Netflow module starts processing events, you can immediately
begin using the packaged Kibana dashboards to explore and visualize your
network flow data.</simpara>
<simpara>You can use the dashboards as-is, or tailor them to work better with existing
use cases and business requirements.</simpara>
<section id="network-dashboards-netflow">
<title>Example Dashboards</title>
<simpara>On the <emphasis role="strong">Overview</emphasis> dashboard, you can see a summary of basic traffic data and set
up filters before you drill down to gain deeper insight into the data.</simpara>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/netflow-overview.png"/>
  </imageobject>
  <textobject><phrase>Netflow overview dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>For example, on the <emphasis role="strong">Conversation Partners</emphasis> dashboard, you can see the source
and destination addresses of the client and server in any conversation.</simpara>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/netflow-conversation-partners.png"/>
  </imageobject>
  <textobject><phrase>Netflow conversation partners dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>On the <emphasis role="strong">Traffic Analysis</emphasis> dashboard, you can identify high volume conversations
by viewing the traffic volume in bytes.</simpara>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/netflow-traffic-analysis.png"/>
  </imageobject>
  <textobject><phrase>Netflow traffic analysis dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>Then you can go to the <emphasis role="strong">Geo Location</emphasis> dashboard where you can visualize the
location of destinations and sources on a heat map.</simpara>
<informalfigure role="screenshot">
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/netflow-geo-location.png"/>
  </imageobject>
  <textobject><phrase>Netflow geo location dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section id="configuring-netflow">
<title>Configuring the Module</title>
<simpara>You can further refine the behavior of the Logstash Netflow module by specifying
settings in the <literal>logstash.yml</literal> settings file, or overriding settings at the
command line.</simpara>
<simpara>For example, the following configuration in the <literal>settings.yml</literal> file sets
Logstash to listen on port 9996 for network traffic data:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">modules:
  - name: netflow
    var.input.udp.port: 9996</programlisting>
<simpara>To specify the same settings at the command line, you use:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash --modules netflow -M netflow.var.input.udp.port=9996</programlisting>
<simpara>For more information about configuring modules, see
<xref linkend="logstash-modules"/>.</simpara>
<section id="netflow-module-config">
<title>Configuration Options</title>
<simpara>The Netflow module provides the following settings for configuring the behavior
of the module. These settings include Netflow-specific options plus common
options that are supported by all Logstash modules.</simpara>
<simpara>When you override a setting at the command line, remember to prefix the setting
with the module name, for example,  <literal>netflow.var.input.udp.port</literal> instead of
<literal>var.input.udp.port</literal>.</simpara>
<simpara>If you don&#8217;t specify configuration settings, Logstash uses the defaults.</simpara>
<simpara><emphasis role="strong">Netflow Options</emphasis></simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.input.udp.port:</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is 2055.
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the UDP port on which Logstash listens for network traffic data. Although
2055 is the default for this setting, some devices use ports in the range of
9995 through 9998, with 9996 being the most commonly used alternative.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara><emphasis role="strong">Common options</emphasis></simpara>
<simpara>The following configuration options are supported by all modules:</simpara>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.hosts</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="uri">uri</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "localhost:9200"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the host(s) of the Elasticsearch cluster. For each host, you must specify
the hostname and port. For example, "myhost:9200". If given an <link linkend="array">array</link>,
Logstash will load balance requests across the hosts specified in the hosts
parameter. It is important to exclude <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/modules-node.html">dedicated master
nodes</ulink> from the hosts list to prevent Logstash from sending bulk requests to the
master nodes. So this parameter should only reference either data or client
nodes in Elasticsearch.</simpara>
<simpara>Any special characters present in the URLs here MUST be URL escaped! This means #
should be put in as %23 for instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.username</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "elastic"
</simpara>
</listitem>
</itemizedlist>
<simpara>The username to authenticate to a secure Elasticsearch cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.password</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "changeme"
</simpara>
</listitem>
</itemizedlist>
<simpara>The password to authenticate to a secure Elasticsearch cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.enabled</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Enable SSL/TLS secured communication to the Elasticsearch cluster. Leaving this
unspecified will use whatever scheme is specified in the URLs listed in <literal>hosts</literal>.
If no explicit protocol is specified, plain HTTP will be used. If SSL is
explicitly disabled here, the plugin will refuse to start if an HTTPS URL is
given in hosts.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.verification_mode</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "strict"
</simpara>
</listitem>
</itemizedlist>
<simpara>The hostname verification setting when communicating with Elasticsearch. Set to
<literal>disable</literal> to turn off hostname verification. Disabling this has serious security
concerns.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.certificate_authority</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use to validate SSL certificates when
communicating with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.certificate</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use for client authentication when
communicating with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.elasticsearch.ssl.key</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to the certificate key for client authentication when communicating
with Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.host</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "localhost:5601"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the hostname and port of the Kibana instance to use for importing
dashboards and visualizations. For example: "myhost:5601".</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.scheme</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "http"
</simpara>
</listitem>
</itemizedlist>
<simpara>Sets the protocol to use for reaching the Kibana instance. The options are:
"http" or "https". The default is "http".</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.username</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "elastic"
</simpara>
</listitem>
</itemizedlist>
<simpara>The username to authenticate to a secured Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.password</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "changeme"
</simpara>
</listitem>
</itemizedlist>
<simpara>The password to authenticate to a secure Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.enabled</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is false
</simpara>
</listitem>
</itemizedlist>
<simpara>Enable SSL/TLS secured communication to the Kibana instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.verification_mode</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is "strict"
</simpara>
</listitem>
</itemizedlist>
<simpara>The hostname verification setting when communicating with Kibana. Set to
<literal>disable</literal> to turn off hostname verification. Disabling this has serious security
concerns.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.certificate_authority</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use to validate SSL certificates when
communicating with Kibana.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.certificate</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to an X.509 certificate to use for client authentication when
communicating with Kibana.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong"><literal>var.kibana.ssl.key</literal></emphasis>
</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting
</simpara>
</listitem>
</itemizedlist>
<simpara>The path to the certificate key for client authentication when communicating
with Kibana.</simpara>
</listitem>
</varlistentry>
</variablelist>
<remark> Working with Filebeat Modules</remark>
</section>
</section>
</section>
</chapter>
<chapter id="filebeat-modules">
<title>Working with Filebeat Modules<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>Filebeat comes packaged with pre-built  <ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-modules.html">modules</ulink>
that contain the configurations needed to collect, parse, enrich, and visualize
data from various log file formats. Each Filebeat module consists of one or more
filesets that contain ingest node pipelines, Elasticsearch templates, Filebeat
prospector configurations, and Kibana dashboards.</simpara>
<simpara>Filebeat modules are a great way to get started, but you might find that ingest
pipelines don&#8217;t offer the processing power that you require. If that&#8217;s the case,
you&#8217;ll need to use Logstash.</simpara>
<bridgehead id="graduating-to-Logstash" renderas="sect2">Using Logstash instead of Ingest Node<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></bridgehead>
<simpara>Logstash provides an <link linkend="ingest-converter">ingest pipeline conversion tool</link>
to help you migrate ingest pipeline definitions to Logstash configs. However,
the tool does not currently support all the processors that are available for
ingest node.</simpara>
<simpara>You can follow the steps in this section to build and run Logstash
configurations that parse the data collected by Filebeat modules. Then you&#8217;ll be
able to use the same dashboards available with Filebeat to visualize your data
in Kibana.</simpara>
<bridgehead id="_create_and_start_the_logstash_pipeline" renderas="sect3">Create and start the Logstash pipeline<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Create a Logstash pipeline configuration that reads from the Beats input and
parses the events.
</simpara>
<simpara>See <xref linkend="logstash-config-for-filebeat-modules"/> for detailed examples.</simpara>
</listitem>
<listitem>
<simpara>
Start Logstash, passing in the pipeline configuration file that parses the
log. For example:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash -f mypipeline.conf</programlisting>
<simpara>You&#8217;ll see the following message when Logstash is running and listening for
input from Beats:</simpara>
<programlisting language="shell" linenumbering="unnumbered">[2017-10-13T00:01:15,413][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=&gt;"127.0.0.1:5044"}
[2017-10-13T00:01:15,443][INFO ][logstash.pipeline        ] Pipeline started {"pipeline.id"=&gt;"main"}</programlisting>
</listitem>
</orderedlist>
<simpara>The Logstash pipeline is now ready to receive events from Filebeat. Next, you
set up and run Filebeat.</simpara>
<bridgehead id="_set_up_and_run_filebeat" renderas="sect3">Set up and run Filebeat<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>
If you haven&#8217;t already set up the Filebeat index template and sample Kibana
dashboards, run the Filebeat <literal>setup</literal> command to do that now:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">./filebeat -e setup</programlisting>
<simpara>The <literal>-e</literal> flag is optional and sends output to standard error instead of syslog.</simpara>
<simpara>A connection to Elasticsearch and Kibana is required for this one-time setup
step because Filebeat needs to create the index template in Elasticsearch and
load the sample dashboards into Kibana.</simpara>
<simpara>After the template and dashboards are loaded, you&#8217;ll see the message <literal>INFO
Kibana dashboards successfully loaded. Loaded dashboards</literal>.</simpara>
</listitem>
<listitem>
<simpara>
Configure Filebeat to send log lines to Logstash. To do this, in the
<literal>filebeat.yml</literal> config file, disable the Elasticsearch output, and enable the
Logstash output. For example:
</simpara>
<programlisting language="yaml" linenumbering="unnumbered">#output.elasticsearch:
  #hosts: ["localhost:9200"]
output.logstash:
  hosts: ["localhost:5044"]</programlisting>
</listitem>
<listitem>
<simpara>
Run the <literal>modules enable</literal> command to enable the modules that you want to run.
For example:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">./filebeat modules enable nginx</programlisting>
<simpara>You can further configure the module by editing the config file under the
Filebeat <literal>modules.d</literal> directory. For example, if the log files are not in the
location expected by the module, you can set the <literal>var.paths</literal> option.</simpara>
</listitem>
<listitem>
<simpara>
Start Filebeat. For example, to start Filebeat in the foreground, use:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">./filebeat -e</programlisting>
<note><simpara>Depending on how you&#8217;ve installed Filebeat, you might see errors
related to file ownership or permissions when you try to run Filebeat modules.
See <ulink url="https://www.elastic.co/guide/en/beats/libbeat/6.x//config-file-permissions.html">Config File Ownership and Permissions</ulink>
in the <emphasis>Beats Platform Reference</emphasis> if you encounter errors related to file
ownership or permissions.</simpara></note>
<simpara>See <ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x//filebeat-starting.html">Starting Filebeat</ulink> for more info.</simpara>
</listitem>
</orderedlist>
<bridgehead id="_visualize_the_data" renderas="sect3">Visualize the data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></bridgehead>
<simpara>To visualize the data in Kibana, launch the Kibana web interface by pointing
your browser to port 5601. For example,
<ulink url="http://127.0.0.1:5601">http://127.0.0.1:5601</ulink>.</simpara>
<section id="logstash-config-for-filebeat-modules">
<title>Configuration Examples<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>The examples in this section show you how to build Logstash pipelines that parse
data sent collected by Filebeat modules:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="parsing-apache2"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="parsing-mysql"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="parsing-nginx"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="parsing-system"/>
</simpara>
</listitem>
</itemizedlist>
<section id="parsing-apache2">
<title>Apache 2 Logs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>The Logstash pipeline configuration in this example shows how to ship and parse
access and error logs collected by the
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-module-apache2.html"><literal>apache2</literal> Filebeat module</ulink>.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "apache2" {
    if [fileset][name] == "access" {
      grok {
        match =&gt; { "message" =&gt; ["%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] \"%{WORD:[apache2][access][method]} %{DATA:[apache2][access][url]} HTTP/%{NUMBER:[apache2][access][http_version]}\" %{NUMBER:[apache2][access][response_code]} %{NUMBER:[apache2][access][body_sent][bytes]}( \"%{DATA:[apache2][access][referrer]}\")?( \"%{DATA:[apache2][access][agent]}\")?",
          "%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \\[%{HTTPDATE:[apache2][access][time]}\\] \"-\" %{NUMBER:[apache2][access][response_code]} -" ] }
        remove_field =&gt; "message"
      }
      mutate {
        add_field =&gt; { "read_timestamp" =&gt; "%{@timestamp}" }
      }
      date {
        match =&gt; [ "[apache2][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field =&gt; "[apache2][access][time]"
      }
      useragent {
        source =&gt; "[apache2][access][agent]"
        target =&gt; "[apache2][access][user_agent]"
        remove_field =&gt; "[apache2][access][agent]"
      }
      geoip {
        source =&gt; "[apache2][access][remote_ip]"
        target =&gt; "[apache2][access][geoip]"
      }
    }
    else if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{LOGLEVEL:[apache2][error][level]}\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message]}",
          "\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{DATA:[apache2][error][module]}:%{LOGLEVEL:[apache2][error][level]}\] \[pid %{NUMBER:[apache2][error][pid]}(:tid %{NUMBER:[apache2][error][tid]})?\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message1]}" ] }
        pattern_definitions =&gt; {
          "APACHE_TIME" =&gt; "%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"
        }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "[apache2][error][message1]" =&gt; "[apache2][error][message]" }
      }
      date {
        match =&gt; [ "[apache2][error][timestamp]", "EEE MMM dd H:m:s YYYY", "EEE MMM dd H:m:s.SSSSSS YYYY" ]
        remove_field =&gt; "[apache2][error][timestamp]"
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</programlisting>
</section>
<section id="parsing-mysql">
<title>MySQL Logs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>The Logstash pipeline configuration in this example shows how to ship and parse
error and slowlog logs collected by the
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-module-mysql.html"><literal>mysql</literal> Filebeat module</ulink>.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "mysql" {
    if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["%{LOCALDATETIME:[mysql][error][timestamp]} (\[%{DATA:[mysql][error][level]}\] )?%{GREEDYDATA:[mysql][error][message]}",
          "%{TIMESTAMP_ISO8601:[mysql][error][timestamp]} %{NUMBER:[mysql][error][thread_id]} \[%{DATA:[mysql][error][level]}\] %{GREEDYDATA:[mysql][error][message1]}",
          "%{GREEDYDATA:[mysql][error][message2]}"] }
        pattern_definitions =&gt; {
          "LOCALDATETIME" =&gt; "[0-9]+ %{TIME}"
        }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "[mysql][error][message1]" =&gt; "[mysql][error][message]" }
      }
      mutate {
        rename =&gt; { "[mysql][error][message2]" =&gt; "[mysql][error][message]" }
      }
      date {
        match =&gt; [ "[mysql][error][timestamp]", "ISO8601", "YYMMdd H:m:s" ]
        remove_field =&gt; "[mysql][error][time]"
      }
    }
    else if [fileset][name] == "slowlog" {
      grok {
        match =&gt; { "message" =&gt; ["^# User@Host: %{USER:[mysql][slowlog][user]}(\[[^\]]+\])? @ %{HOSTNAME:[mysql][slowlog][host]} \[(IP:[mysql][slowlog][ip])?\](\s*Id:\s* %{NUMBER:[mysql][slowlog][id]})?\n# Query_time: %{NUMBER:[mysql][slowlog][query_time][sec]}\s* Lock_time: %{NUMBER:[mysql][slowlog][lock_time][sec]}\s* Rows_sent: %{NUMBER:[mysql][slowlog][rows_sent]}\s* Rows_examined: %{NUMBER:[mysql][slowlog][rows_examined]}\n(SET timestamp=%{NUMBER:[mysql][slowlog][timestamp]};\n)?%{GREEDYMULTILINE:[mysql][slowlog][query]}"] }
        pattern_definitions =&gt; {
          "GREEDYMULTILINE" =&gt; "(.|\n)*"
        }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[mysql][slowlog][timestamp]", "UNIX" ]
      }
      mutate {
        gsub =&gt; ["[mysql][slowlog][query]", "\n# Time: [0-9]+ [0-9][0-9]:[0-9][0-9]:[0-9][0-9](\\.[0-9]+)?$", ""]
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</programlisting>
</section>
<section id="parsing-nginx">
<title>Nginx Logs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>The Logstash pipeline configuration in this example shows how to ship and parse
access and error logs collected by the
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-module-nginx.html"><literal>nginx</literal> Filebeat module</ulink>.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "nginx" {
    if [fileset][name] == "access" {
      grok {
        match =&gt; { "message" =&gt; ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""] }
        remove_field =&gt; "message"
      }
      mutate {
        add_field =&gt; { "read_timestamp" =&gt; "%{@timestamp}" }
      }
      date {
        match =&gt; [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field =&gt; "[nginx][access][time]"
      }
      useragent {
        source =&gt; "[nginx][access][agent]"
        target =&gt; "[nginx][access][user_agent]"
        remove_field =&gt; "[nginx][access][agent]"
      }
      geoip {
        source =&gt; "[nginx][access][remote_ip]"
        target =&gt; "[nginx][access][geoip]"
      }
    }
    else if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["%{DATA:[nginx][error][time]} \[%{DATA:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "@timestamp" =&gt; "read_timestamp" }
      }
      date {
        match =&gt; [ "[nginx][error][time]", "YYYY/MM/dd H:m:s" ]
        remove_field =&gt; "[nginx][error][time]"
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</programlisting>
</section>
<section id="parsing-system">
<title>System Logs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc">Edit me</ulink></title>
<simpara>The Logstash pipeline configuration in this example shows how to ship and parse
system logs collected by the
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-module-system.html"><literal>system</literal> Filebeat module</ulink>.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "system" {
    if [fileset][name] == "auth" {
      grok {
        match =&gt; { "message" =&gt; ["%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\[%{POSINT:[system][auth][pid]}\])?: \s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\[%{POSINT:[system][auth][pid]}\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\[%{POSINT:[system][auth][pid]}\])?: new user: name=%{DATA:[system][auth][user][add][name]}, UID=%{NUMBER:[system][auth][user][add][uid]}, GID=%{NUMBER:[system][auth][user][add][gid]}, home=%{DATA:[system][auth][user][add][home]}, shell=%{DATA:[system][auth][user][add][shell]}$",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\[%{POSINT:[system][auth][pid]}\])?: %{GREEDYMULTILINE:[system][auth][message]}"] }
        pattern_definitions =&gt; {
          "GREEDYMULTILINE"=&gt; "(.|\n)*"
        }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[system][auth][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      }
      geoip {
        source =&gt; "[system][auth][ssh][ip]"
        target =&gt; "[system][auth][ssh][geoip]"
      }
    }
    else if [fileset][name] == "syslog" {
      grok {
        match =&gt; { "message" =&gt; ["%{SYSLOGTIMESTAMP:[system][syslog][timestamp]} %{SYSLOGHOST:[system][syslog][hostname]} %{DATA:[system][syslog][program]}(?:\[%{POSINT:[system][syslog][pid]}\])?: %{GREEDYMULTILINE:[system][syslog][message]}"] }
        pattern_definitions =&gt; { "GREEDYMULTILINE" =&gt; "(.|\n)*" }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[system][syslog][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</programlisting>
<remark> Data resiliency</remark>
</section>
</section>
</chapter>
<chapter id="resiliency">
<title>Data Resiliency<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/resiliency.asciidoc">Edit me</ulink></title>
<simpara>As data flows through the event processing pipeline, Logstash may encounter
situations that prevent it from delivering events to the configured
output. For example, the data might contain unexpected data types, or
Logstash might terminate abnormally.</simpara>
<simpara>To guard against data loss and ensure that events flow through the
pipeline without interruption, Logstash provides the following data resiliency
features.</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="persistent-queues"/> protect against data loss by storing events in an
internal queue on disk.
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="dead-letter-queues"/> provide on-disk storage for events that Logstash is
unable to process. You can easily reprocess events in the dead letter queue by
using the <literal>dead_letter_queue</literal> input plugin.
</simpara>
</listitem>
</itemizedlist>
<remark>TODO: Make dead_letter_queue an active link after the plugin docs are published.</remark>
<simpara>These resiliency features are disabled by default. To turn on these features,
you must explicitly enable them in the Logstash <link linkend="logstash-settings-file">settings file</link>.</simpara>
<section id="persistent-queues">
<title>Persistent Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>By default, Logstash uses in-memory bounded queues between pipeline stages
(inputs → pipeline workers) to buffer events. The size of these in-memory
queues is fixed and not configurable. If Logstash experiences a temporary
machine failure, the contents of the in-memory queue will be lost. Temporary machine
failures are scenarios where Logstash or its host machine are terminated
abnormally but are capable of being restarted.</simpara>
<simpara>In order to protect against data loss during abnormal termination, Logstash has
a persistent queue feature which will store the message queue on disk.
Persistent queues provide durability of data within Logstash.</simpara>
<simpara>Persistent queues are also useful for Logstash deployments that need large buffers.
Instead of deploying and managing a message broker, such as Redis, RabbitMQ, or
Apache Kafka, to facilitate a buffered publish-subscriber model, you can enable
persistent queues to buffer events on disk and remove the message broker.</simpara>
<simpara>In summary, the benefits of enabling persistent queues are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>
Absorbs bursts of events without needing an external buffering mechanism like
Redis or Apache Kafka.
</simpara>
</listitem>
<listitem>
<simpara>
Provides an at-least-once delivery guarantee against message loss during
a normal shutdown as well as when Logstash is terminated abnormally. If Logstash
is restarted while events are in-flight, Logstash will attempt to deliver
messages stored in the persistent queue until delivery succeeds at least once.
</simpara>
<note><simpara>You must set <literal>queue.checkpoint.writes: 1</literal> explicitly to guarantee
maximum durability for all input events. See <xref linkend="durability-persistent-queues"/>.</simpara></note>
</listitem>
</itemizedlist>
<section id="persistent-queues-limitations">
<title>Limitations of Persistent Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>The following are problems not solved by the persistent queue feature:</simpara>
<itemizedlist>
<listitem>
<simpara>
Input plugins that do not use a request-response protocol cannot be protected from data loss. For example: tcp, udp, zeromq push+pull, and many other inputs do not have a mechanism to acknowledge receipt to the sender. Plugins such as beats and http, which <emphasis role="strong">do</emphasis> have an acknowledgement capability, are well protected by this queue.
</simpara>
</listitem>
<listitem>
<simpara>
It does not handle permanent machine failures such as disk corruption, disk failure, and machine loss. The data persisted to disk is not replicated.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="persistent-queues-architecture">
<title>How Persistent Queues Work<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>The queue sits between the input and filter stages in the same
process:</simpara>
<simpara>input → queue → filter + output</simpara>
<simpara>When an input has events ready to process, it writes them to the queue. When
the write to the queue is successful, the input can send an acknowledgement to
its data source.</simpara>
<simpara>When processing events from the queue, Logstash acknowledges events as
completed, within the queue, only after filters and outputs have completed.
The queue keeps a record of events that have been processed by the pipeline.
An event is recorded as processed (in this document, called "acknowledged" or
"ACKed") if, and only if, the event has been processed completely by the
Logstash pipeline.</simpara>
<simpara>What does acknowledged mean? This means the event has been handled by all
configured filters and outputs. For example, if you have only one output,
Elasticsearch, an event is ACKed when the Elasticsearch output has successfully
sent this event to Elasticsearch.</simpara>
<simpara>During a normal shutdown (<emphasis role="strong">CTRL+C</emphasis> or SIGTERM), Logstash will stop reading
from the queue and will finish processing the in-flight events being processed
by the filters and outputs. Upon restart, Logstash will resume processing the
events in the persistent queue as well as accepting new events from inputs.</simpara>
<simpara>If Logstash is abnormally terminated, any in-flight events will not have been
ACKed and will be reprocessed by filters and outputs when Logstash is
restarted. Logstash processes events in batches, so it is possible
that for any given batch, some of that batch may have been successfully
completed, but not recorded as ACKed, when an abnormal termination occurs.</simpara>
<simpara>For more details specific behaviors of queue writes and acknowledgement, see
<xref linkend="durability-persistent-queues"/>.</simpara>
</section>
<section id="configuring-persistent-queues">
<title>Configuring Persistent Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>To configure persistent queues, you can specify the following options in the
Logstash <link linkend="logstash-settings-file">settings file</link>:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>queue.type</literal>: Specify <literal>persisted</literal> to enable persistent queues. By default, persistent queues are disabled (default: <literal>queue.type: memory</literal>).
</simpara>
</listitem>
<listitem>
<simpara>
<literal>path.queue</literal>: The directory path where the data files will be stored. By default, the files are stored in <literal>path.data/queue</literal>.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>queue.page_capacity</literal>: The maximum size of a queue page in bytes. The queue data consists of append-only files called "pages". The default size is 250mb. Changing this value is unlikely to have performance benefits.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>queue.drain</literal>: Specify <literal>true</literal> if you want Logstash to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly.
<remark> Technically, I know, this isn't "maximum number of events" it's really maximum number of events not yet read by the pipeline worker. We only use this for testing and users generally shouldn't be setting this.</remark>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>queue.max_events</literal>:  The maximum number of events that are allowed in the queue. The default is 0 (unlimited). This value is used internally for the Logstash test suite.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>queue.max_bytes</literal>: The total capacity of the queue in number of bytes. The
default is 1024mb (1gb). Make sure the capacity of your disk drive is greater
than the value you specify here.
</simpara>
</listitem>
</itemizedlist>
<simpara>If both <literal>queue.max_events</literal> and
<literal>queue.max_bytes</literal> are specified, Logstash uses whichever criteria is reached
first. See <xref linkend="backpressure-persistent-queue"/> for behavior when these queue limits are reached.</simpara>
<simpara>You can also specify options that control when the checkpoint file gets updated (<literal>queue.checkpoint.acks</literal>, <literal>queue.checkpoint.writes</literal>). See <xref linkend="durability-persistent-queues"/>.</simpara>
<simpara>Example configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">queue.type: persisted
queue.max_bytes: 4gb</programlisting>
</section>
<section id="backpressure-persistent-queue">
<title>Handling Back Pressure<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>When the queue is full, Logstash puts back pressure on the inputs to stall data
flowing into Logstash. This mechanism helps Logstash control the rate of data
flow at the input stage without overwhelming outputs like Elasticsearch.</simpara>
<simpara>Use <literal>queue.max_bytes</literal> setting to configure the total capacity of the queue on
disk. The following example sets the total capacity of the queue to 8gb:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">queue.type: persisted
queue.max_bytes: 8gb</programlisting>
<simpara>With these settings specified, Logstash will buffer events on disk until the
size of the queue reaches 8gb. When the queue is full of unACKed events, and
the size limit has been reached, Logstash will no longer accept new events.</simpara>
<simpara>Each input handles back pressure independently. For example, when the
<link linkend="plugins-inputs-beats">beats</link> input encounters back pressure, it no longer
accepts new connections and waits until the persistent queue has space to accept
more events. After the filter and output stages finish processing existing
events in the queue and ACKs them, Logstash automatically starts accepting new
events.</simpara>
</section>
<section id="durability-persistent-queues">
<title>Controlling Durability<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>Durability is a property of storage writes that ensures data will be available after it&#8217;s written.</simpara>
<simpara>When the persistent queue feature is enabled, Logstash will store events on
disk. Logstash commits to disk in a mechanism called checkpointing.</simpara>
<simpara>To discuss durability, we need to introduce a few details about how the persistent queue is implemented.</simpara>
<simpara>First, the queue itself is a set of pages. There are two kinds of pages: head pages and tail pages. The head page is where new events are written. There is only one head page. When the head page is of a certain size (see <literal>queue.page_capacity</literal>), it becomes a tail page, and a new head page is created. Tail pages are immutable, and the head page is append-only.
Second, the queue records details about itself (pages, acknowledgements, etc) in a separate file called a checkpoint file.</simpara>
<simpara>When recording a checkpoint, Logstash will:</simpara>
<itemizedlist>
<listitem>
<simpara>
Call fsync on the head page.
</simpara>
</listitem>
<listitem>
<simpara>
Atomically write to disk the current state of the queue.
</simpara>
</listitem>
</itemizedlist>
<simpara>The following settings are available to let you tune durability:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>queue.checkpoint.writes</literal>: Logstash will checkpoint after this many writes into the queue. Currently, one event counts as one write, but this may change in future releases.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>queue.checkpoint.acks</literal>: Logstash will checkpoint after this many events are acknowledged. This configuration controls the durability at the processing (filter + output)
part of Logstash.
</simpara>
</listitem>
</itemizedlist>
<simpara>Disk writes have a resource cost. Tuning the above values higher or lower will trade durability for performance. For instance, if you want the strongest durability for all input events, you can set <literal>queue.checkpoint.writes: 1</literal>.</simpara>
<simpara>The process of checkpointing is atomic, which means any update to the file is saved if successful.</simpara>
<simpara>If Logstash is terminated, or if there is a hardware level failure, any data
that is buffered in the persistent queue, but not yet checkpointed, is lost.
To avoid this possibility, you can set <literal>queue.checkpoint.writes: 1</literal>, but keep in
mind that this setting can severely impact performance.</simpara>
</section>
<section id="garbage-collection">
<title>Disk Garbage Collection<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc">Edit me</ulink></title>
<simpara>On disk, the queue is stored as a set of pages where each page is one file. Each page can be at most <literal>queue.page_capacity</literal> in size. Pages are deleted (garbage collected) after all events in that page have been ACKed. If an older page has at least one event that is not yet ACKed, that entire page will remain on disk until all events in that page are successfully processed. Each page containing unprocessed events will count against the <literal>queue.max_bytes</literal> byte size.</simpara>
</section>
</section>
<section id="dead-letter-queues">
<title>Dead Letter Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<note><simpara>The dead letter queue feature is currently supported for the
<xref linkend="plugins-outputs-elasticsearch"/> output only. Additionally, The dead
letter queue is only used where the response code is either 400
or 404, both of which indicate an event that cannot be retried.
Support for additional outputs will be available in future releases of the
Logstash plugins. Before configuring Logstash to use this feature, refer to
the output plugin documentation to verify that the plugin supports the dead
letter queue feature.</simpara></note>
<simpara>By default, when Logstash encounters an event that it cannot process because the
data contains a mapping error or some other issue, the Logstash pipeline
either hangs or drops the unsuccessful event. In order to protect against data
loss in this situation, you can <link linkend="configuring-dlq">configure Logstash</link> to write
unsuccessful events to a dead letter queue instead of dropping them.</simpara>
<simpara>Each event written to the dead letter queue includes the original event, along
with metadata that describes the reason the event could not be processed,
information about the plugin that wrote the event, and the timestamp for when
the event entered the dead letter queue.</simpara>
<simpara>To process events in the dead letter queue, you simply create a Logstash
pipeline configuration that uses the
<link linkend="plugins-inputs-dead_letter_queue"><literal>dead_letter_queue</literal> input plugin</link> to read
from the queue.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/dead_letter_queue.png"/>
  </imageobject>
  <textobject><phrase>Diagram showing pipeline reading from the dead letter queue</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>See <xref linkend="processing-dlq-events"/> for more information.</simpara>
<section id="configuring-dlq">
<title>Configuring Logstash to Use Dead Letter Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<simpara>Dead letter queues are disabled by default. To enable dead letter queues, set
the <literal>dead_letter_queue_enable</literal> option in the <literal>logstash.yml</literal>
<link linkend="logstash-settings-file">settings file</link>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">dead_letter_queue.enable: true</programlisting>
<simpara>Dead letter queues are stored as files in the local directory of the Logstash
instance. By default, the dead letter queue files are stored in
<literal>path.data/dead_letter_queue</literal>. Each pipeline has a separate queue. For example,
the dead letter queue for the <literal>main</literal> pipeline is stored in
<literal>LOGSTASH_HOME/data/dead_letter_queue/main</literal> by default. The queue files are
numbered sequentially: <literal>1.log</literal>, <literal>2.log</literal>, and so on.</simpara>
<simpara>You can set <literal>path.dead_letter_queue</literal> in the <literal>logstash.yml</literal> file to
specify a different path for the files:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">path.dead_letter_queue: "path/to/data/dead_letter_queue"</programlisting>
<note><simpara>You may not use the same <literal>dead_letter_queue</literal> path for two different
Logstash instances.</simpara></note>
<section id="_file_rotation">
<title>File Rotation<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<simpara>Dead letter queues have a built-in file rotation policy that manages the file
size of the queue. When the file size reaches a preconfigured threshold, a new
file is created automatically.</simpara>
<simpara>By default, the maximum size of each dead letter queue is set to 1024mb. To
change this setting, use the <literal>dead_letter_queue.max_bytes</literal> option.  Entries
will be dropped if they would increase the size of the dead letter queue beyond
this setting.</simpara>
</section>
</section>
<section id="processing-dlq-events">
<title>Processing Events in the Dead Letter Queue<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<simpara>When you are ready to process events in the dead letter queue, you create a
pipeline that uses the
<link linkend="plugins-inputs-dead_letter_queue"><literal>dead_letter_queue</literal> input plugin</link> to read
from the dead letter queue. The pipeline configuration that you use depends, of
course, on what you need to do. For example, if the dead letter queue contains
events that resulted from a mapping error in Elasticsearch, you can create a
pipeline that reads the "dead" events, removes the field that caused the mapping
issue, and re-indexes the clean events into Elasticsearch.</simpara>
<simpara>The following example shows a simple pipeline that reads events from the dead
letter queue and writes the events, including metadata, to standard output:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">input {
  dead_letter_queue {
    path =&gt; "/path/to/data/dead_letter_queue" <co id="CO10-1"/>
    commit_offsets =&gt; true <co id="CO10-2"/>
    pipeline_id =&gt; "main" <co id="CO10-3"/>
  }
}

output {
  stdout {
    codec =&gt; rubydebug { metadata =&gt; true }
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>
The path to the top-level directory containing the dead letter queue. This
directory contains a separate folder for each pipeline that writes to the dead
letter queue. To find the path to this directory, look at the <literal>logstash.yml</literal>
<link linkend="logstash-settings-file">settings file</link>. By default, Logstash creates the
<literal>dead_letter_queue</literal> directory under the location used for persistent
storage (<literal>path.data</literal>), for example, <literal>LOGSTASH_HOME/data/dead_letter_queue</literal>.
However, if <literal>path.dead_letter_queue</literal> is set, it uses that location instead.
</para>
</callout>
<callout arearefs="CO10-2">
<para>
When <literal>true</literal>, saves the offset. When the pipeline restarts, it will continue
reading from the position where it left off rather than reprocessing all the
items in the queue. You can set <literal>commit_offsets</literal> to <literal>false</literal> when you are
exploring events in the dead letter queue and want to iterate over the events
multiple times.
</para>
</callout>
<callout arearefs="CO10-3">
<para>
The ID of the pipeline that&#8217;s writing to the dead letter queue. The default
is <literal>"main"</literal>.
</para>
</callout>
</calloutlist>
<simpara>For another example, see <xref linkend="dlq-example"/>.</simpara>
<simpara>When the pipeline has finished processing all the events in the dead letter
queue, it will continue to run and process new events as they stream into the
queue. This means that you do not need to stop your production system to handle
events in the dead letter queue.</simpara>
<note><simpara>Events emitted from the
<link linkend="plugins-inputs-dead_letter_queue"><literal>dead_letter_queue</literal> input plugin</link> plugin
will not be resubmitted to the dead letter queue if they cannot be processed
correctly.</simpara></note>
</section>
<section id="dlq-timestamp">
<title>Reading From a Timestamp<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<simpara>When you read from the dead letter queue, you might not want to process all the
events in the queue, especially if there are a lot of old events in the queue.
You can start processing events at a specific point in the queue by using the
<literal>start_timestamp</literal> option. This option configures the pipeline to start
processing events based on the timestamp of when they entered the queue:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">input {
  dead_letter_queue {
    path =&gt; "/path/to/data/dead_letter_queue"
    start_timestamp =&gt; 2017-06-06T23:40:37
    pipeline_id =&gt; "main"
  }
}</programlisting>
<simpara>For this example, the pipeline starts reading all events that were delivered to
the dead letter queue on or after June 6, 2017, at 23:40:37.</simpara>
</section>
<section id="dlq-example">
<title>Example: Processing Data That Has Mapping Errors<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc">Edit me</ulink></title>
<simpara>In this example, the user attempts to index a document that includes geo_ip data,
but the data cannot be processed because it contains a mapping error:</simpara>
<programlisting language="json" linenumbering="unnumbered">{"geoip":{"location":"home"}}</programlisting>
<simpara>Indexing fails because the Logstash output plugin expects a <literal>geo_point</literal> object in
the <literal>location</literal> field, but the value is a string. The failed event is written to
the dead letter queue, along with metadata about the error that caused the
failure:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
   "@metadata" =&gt; {
    "dead_letter_queue" =&gt; {
       "entry_time" =&gt; #&lt;Java::OrgLogstash::Timestamp:0x5b5dacd5&gt;,
        "plugin_id" =&gt; "fb80f1925088497215b8d037e622dec5819b503e-4",
      "plugin_type" =&gt; "elasticsearch",
           "reason" =&gt; "Could not index event to Elasticsearch. status: 400, action: [\"index\", {:_id=&gt;nil, :_index=&gt;\"logstash-2017.06.22\", :_type=&gt;\"doc\", :_routing=&gt;nil}, 2017-06-22T01:29:29.804Z My-MacBook-Pro-2.local {\"geoip\":{\"location\":\"home\"}}], response: {\"index\"=&gt;{\"_index\"=&gt;\"logstash-2017.06.22\", \"_type\"=&gt;\"doc\", \"_id\"=&gt;\"AVzNayPze1iR9yDdI2MD\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_argument_exception\", \"reason\"=&gt;\"illegal latitude value [266.30859375] for geoip.location\"}}}}"
    }
  },
  "@timestamp" =&gt; 2017-06-22T01:29:29.804Z,
    "@version" =&gt; "1",
       "geoip" =&gt; {
    "location" =&gt; "home"
  },
        "host" =&gt; "My-MacBook-Pro-2.local",
     "message" =&gt; "{\"geoip\":{\"location\":\"home\"}}"
}</programlisting>
<simpara>To process the failed event, you create the following pipeline that reads from
the dead letter queue and removes the mapping problem:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  dead_letter_queue {
    path =&gt; "/path/to/data/dead_letter_queue/" <co id="CO11-1"/>
  }
}
filter {
  mutate {
    remove_field =&gt; "[geoip][location]" <co id="CO11-2"/>
  }
}
output {
  elasticsearch{
    hosts =&gt; [ "localhost:9200" ] <co id="CO11-3"/>
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>
The <link linkend="plugins-inputs-dead_letter_queue"><literal>dead_letter_queue</literal> input</link> reads from the dead letter queue.
</para>
</callout>
<callout arearefs="CO11-2">
<para>
The <literal>mutate</literal> filter removes the problem field called <literal>location</literal>.
</para>
</callout>
<callout arearefs="CO11-3">
<para>
The clean event is sent to Elasticsearch, where it can be indexed because
the mapping issue is resolved.
</para>
</callout>
</calloutlist>
<remark> Transforming Data</remark>
</section>
</section>
</chapter>
<chapter id="transformation">
<title>Transforming Data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc">Edit me</ulink></title>
<simpara>With over 200 plugins in the Logstash plugin ecosystem, it&#8217;s sometimes
challenging to choose the best plugin to meet your data processing needs.
In this section, we&#8217;ve collected a list of popular plugins and organized them
according to their processing capabilities:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="core-operations"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="data-deserialization"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="field-extraction"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="lookup-enrichment"/>
</simpara>
</listitem>
</itemizedlist>
<simpara>Also see <xref linkend="filter-plugins"/> and <xref linkend="codec-plugins"/> for the full list of available
data processing plugins.</simpara>
<section id="core-operations">
<title>Performing Core Operations<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc">Edit me</ulink></title>
<simpara>The plugins described in this section are useful for core operations, such as
mutating and dropping events.</simpara>
<variablelist>
<varlistentry>
<term>
<link linkend="plugins-filters-date">date filter</link>
</term>
<listitem>
<simpara>
Parses dates from fields to use as Logstash timestamps for events.
</simpara>
<simpara>The following config parses a field called <literal>logdate</literal> to set the Logstash
timestamp:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  date {
    match =&gt; [ "logdate", "MMM dd yyyy HH:mm:ss" ]
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-drop">drop filter</link>
</term>
<listitem>
<simpara>
Drops events. This filter is typically used in combination with conditionals.
</simpara>
<simpara>The following config drops <literal>debug</literal> level log messages:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  if [loglevel] == "debug" {
    drop { }
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-fingerprint">fingerprint filter</link>
</term>
<listitem>
<simpara>
Fingerprints fields by applying a consistent hash.
</simpara>
<simpara>The following config fingerprints the <literal>IP</literal>, <literal>@timestamp</literal>, and <literal>message</literal> fields
and adds the hash to a metadata field called <literal>generated_id</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  fingerprint {
    source =&gt; ["IP", "@timestamp", "message"]
    method =&gt; "SHA1"
    key =&gt; "0123"
    target =&gt; "[@metadata][generated_id]"
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-mutate">mutate filter</link>
</term>
<listitem>
<simpara>
Performs general mutations on fields. You can rename, remove, replace, and
modify fields in your events.
</simpara>
<simpara>The following config renames the <literal>HOSTORIP</literal> field to <literal>client_ip</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  mutate {
    rename =&gt; { "HOSTORIP" =&gt; "client_ip" }
  }
}</programlisting>
<simpara>The following config strips leading and trailing whitespace from the specified
fields:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  mutate {
    strip =&gt; ["field1", "field2"]
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-ruby">ruby filter</link>
</term>
<listitem>
<simpara>
Executes Ruby code.
</simpara>
<simpara>The following config executes Ruby code that cancels 90% of the events:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  ruby {
    code =&gt; "event.cancel if rand &lt;= 0.90"
  }
}</programlisting>
</listitem>
</varlistentry>
</variablelist>
</section>
<section id="data-deserialization">
<title>Deserializing Data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc">Edit me</ulink></title>
<simpara>The plugins described in this section are useful for deserializing data into
Logstash events.</simpara>
<variablelist>
<varlistentry>
<term>
<link linkend="plugins-codecs-avro">avro codec</link>
</term>
<listitem>
<simpara>
Reads serialized Avro records as Logstash events. This plugin deserializes
individual Avro records. It is not for reading Avro files. Avro files have a
unique format that must be handled upon input.
</simpara>
<simpara>The following config deserializes input from Kafka:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  kafka {
    codec =&gt; {
      avro =&gt; {
        schema_uri =&gt; "/tmp/schema.avsc"
      }
    }
  }
}
...</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-csv">csv filter</link>
</term>
<listitem>
<simpara>
Parses comma-separated value data into individual fields. By default, the
filter autogenerates field names (column1, column2, and so on), or you can specify
a list of names. You can also change the column separator.
</simpara>
<simpara>The following config parses CSV data into the field names specified in the
<literal>columns</literal> field:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  csv {
    separator =&gt; ","
    columns =&gt; [ "Transaction Number", "Date", "Description", "Amount Debit", "Amount Credit", "Balance" ]
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-codecs-fluent">fluent codec</link>
</term>
<listitem>
<simpara>
Reads the Fluentd <literal>msgpack</literal> schema.
</simpara>
<simpara>The following config decodes logs received from <literal>fluent-logger-ruby</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  tcp {
    codec =&gt; fluent
    port =&gt; 4000
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-codecs-json">json codec</link>
</term>
<listitem>
<simpara>
Decodes (via inputs) and encodes (via outputs) JSON formatted content, creating
one event per element in a JSON array.
</simpara>
<simpara>The following config decodes the JSON formatted content in a file:</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  file {
    path =&gt; "/path/to/myfile.json"
    codec =&gt;"json"
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-codecs-protobuf">protobuf codec</link>
</term>
<listitem>
<simpara>
Reads protobuf encoded messages and converts them to Logstash events. Requires
the protobuf definitions to be compiled as Ruby files. You can compile them by
using the
<ulink url="https://github.com/codekitchen/ruby-protocol-buffers">ruby-protoc compiler</ulink>.
</simpara>
<simpara>The following config decodes events from a Kafka stream:</simpara>
<programlisting language="json" linenumbering="unnumbered">input
  kafka {
    zk_connect =&gt; "127.0.0.1"
    topic_id =&gt; "your_topic_goes_here"
    codec =&gt; protobuf {
      class_name =&gt; "Animal::Unicorn"
      include_path =&gt; ['/path/to/protobuf/definitions/UnicornProtobuf.pb.rb']
    }
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-xml">xml filter</link>
</term>
<listitem>
<simpara>
Parses XML into fields.
</simpara>
<simpara>The following config parses the whole XML document stored in the <literal>message</literal> field:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  xml {
    source =&gt; "message"
  }
}</programlisting>
</listitem>
</varlistentry>
</variablelist>
</section>
<section id="field-extraction">
<title>Extracting Fields and Wrangling Data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc">Edit me</ulink></title>
<simpara>The plugins described in this section are useful for extracting fields and
parsing unstructured data into fields.</simpara>
<variablelist>
<varlistentry>
<term>
<link linkend="plugins-filters-dissect">dissect filter</link>
</term>
<listitem>
<simpara>
Extracts unstructured event data into fields by using delimiters. The dissect
filter does not use regular expressions and is very fast. However, if the
structure of the data varies from line to line, the grok filter is more
suitable.
</simpara>
<simpara>For example, let&#8217;s say you have a log that contains the following message:</simpara>
<programlisting language="json" linenumbering="unnumbered">Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...</programlisting>
<simpara>The following config dissects the message:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  dissect {
    mapping =&gt; { "message" =&gt; "%{ts} %{+ts} %{+ts} %{src} %{prog}[%{pid}]: %{msg}" }
  }
}</programlisting>
<simpara>After the dissect filter is applied, the event will be dissected into the following
fields:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "msg"        =&gt; "Starting system activity accounting tool...",
  "@timestamp" =&gt; 2017-04-26T19:33:39.257Z,
  "src"        =&gt; "localhost",
  "@version"   =&gt; "1",
  "host"       =&gt; "localhost.localdomain",
  "pid"        =&gt; "1",
  "message"    =&gt; "Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...",
  "type"       =&gt; "stdin",
  "prog"       =&gt; "systemd",
  "ts"         =&gt; "Apr 26 12:20:02"
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-kv">kv filter</link>
</term>
<listitem>
<simpara>
Parses key-value pairs.
</simpara>
<simpara>For example, let&#8217;s say you have a log message that contains the following
key-value pairs:</simpara>
<programlisting language="json" linenumbering="unnumbered">ip=1.2.3.4 error=REFUSED</programlisting>
<simpara>The following config parses the key-value pairs into fields:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  kv { }
}</programlisting>
<simpara>After the filter is applied, the event in the example will have these fields:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>ip: 1.2.3.4</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>error: REFUSED</literal>
</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-grok">grok filter</link>
</term>
<listitem>
<simpara>
Parses unstructured event data into fields. This tool is perfect for syslog
logs, Apache and other webserver logs, MySQL logs, and in general, any log
format that is generally written for humans and not computer consumption.
Grok works by combining text patterns into something that matches your
logs.
</simpara>
<simpara>For example, let&#8217;s say you have an HTTP request log that contains
the following message:</simpara>
<programlisting language="json" linenumbering="unnumbered">55.3.244.1 GET /index.html 15824 0.043</programlisting>
<simpara>The following config parses the message into fields:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  grok {
    match =&gt; { "message" =&gt; "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
  }
}</programlisting>
<simpara>After the filter is applied, the event in the example will have these fields:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>client: 55.3.244.1</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>method: GET</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>request: /index.html</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>bytes: 15824</literal>
</simpara>
</listitem>
<listitem>
<simpara>
<literal>duration: 0.043</literal>
</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<tip><simpara>If you need help building grok patterns, try out the
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/xpack-grokdebugger.html">Grok Debugger</ulink>. The Grok Debugger is an
X-Pack feature under the Basic License and is therefore <emphasis role="strong">free to use</emphasis>.</simpara></tip>
</section>
<section id="lookup-enrichment">
<title>Enriching Data with Lookups<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc">Edit me</ulink></title>
<simpara>The plugins described in this section are useful for enriching data with
additional info, such as GeoIP and user agent info.</simpara>
<variablelist>
<varlistentry>
<term>
<link linkend="plugins-filters-dns">dns filter</link>
</term>
<listitem>
<simpara>
Performs a standard or reverse DNS lookup.
</simpara>
<simpara>The following config performs a reverse lookup on the address in the
<literal>source_host</literal> field and replaces it with the domain name:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  dns {
    reverse =&gt; [ "source_host" ]
    action =&gt; "replace"
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-elasticsearch">elasticsearch</link>
</term>
<listitem>
<simpara>
Copies fields from previous log events in Elasticsearch to current events.
</simpara>
<simpara>The following config shows a complete example of how this filter might
be used.  Whenever Logstash receives an "end" event, it uses this Elasticsearch
filter to find the matching "start" event based on some operation identifier.
Then it copies the <literal>@timestamp</literal> field from the "start" event into a new field on
the "end" event.  Finally, using a combination of the date filter and the
ruby filter, the code in the example calculates the time duration in hours
between the two events.</simpara>
<programlisting language="json" linenumbering="unnumbered">      if [type] == "end" {
         elasticsearch {
            hosts =&gt; ["es-server"]
            query =&gt; "type:start AND operation:%{[opid]}"
            fields =&gt; { "@timestamp" =&gt; "started" }
         }
         date {
            match =&gt; ["[started]", "ISO8601"]
            target =&gt; "[started]"
         }
         ruby {
            code =&gt; 'event.set("duration_hrs", (event.get("@timestamp") - event.get("started")) / 3600) rescue nil'
         }
      }</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-geoip">geoip filter</link>
</term>
<listitem>
<simpara>
Adds geographical information about the location of IP addresses. For example:
</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  geoip {
    source =&gt; "clientip"
  }
}</programlisting>
<simpara>After the geoip filter is applied, the event will be enriched with geoip fields.
For example:</simpara>
<programlisting language="json" linenumbering="unnumbered">          "geoip" =&gt; {
              "timezone" =&gt; "Europe/Moscow",
                    "ip" =&gt; "83.149.9.216",
              "latitude" =&gt; 55.7522,
        "continent_code" =&gt; "EU",
             "city_name" =&gt; "Moscow",
         "country_code2" =&gt; "RU",
          "country_name" =&gt; "Russia",
              "dma_code" =&gt; nil,
         "country_code3" =&gt; "RU",
           "region_name" =&gt; "Moscow",
              "location" =&gt; [
            [0] 37.6156,
            [1] 55.7522
        ],
           "postal_code" =&gt; "101194",
             "longitude" =&gt; 37.6156,
           "region_code" =&gt; "MOW"
    }</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-jdbc_streaming">jdbc_streaming</link>
</term>
<listitem>
<simpara>
Enriches events with database data.
</simpara>
<simpara>The following example executes a SQL query and stores the result set in a field
called <literal>country_details</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  jdbc_streaming {
    jdbc_driver_library =&gt; "/path/to/mysql-connector-java-5.1.34-bin.jar"
    jdbc_driver_class =&gt; "com.mysql.jdbc.Driver"
    jdbc_connection_string =&gt; "jdbc:mysql://localhost:3306/mydatabase"
    jdbc_user =&gt; "me"
    jdbc_password =&gt; "secret"
    statement =&gt; "select * from WORLD.COUNTRY WHERE Code = :code"
    parameters =&gt; { "code" =&gt; "country_code"}
    target =&gt; "country_details"
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-translate">translate filter</link>
</term>
<listitem>
<simpara>
Replaces field contents based on replacement values specified in a hash or file.
Currently supports these file types: YAML, JSON, and CSV.
</simpara>
<simpara>The following example takes the value of the <literal>response_code</literal> field, translates
it to a description based on the values specified in the dictionary, and then
removes the <literal>response_code</literal> field from the event:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  translate {
    field =&gt; "response_code"
    destination =&gt; "http_response"
    dictionary =&gt; {
      "200" =&gt; "OK"
      "403" =&gt; "Forbidden"
      "404" =&gt; "Not Found"
      "408" =&gt; "Request Timeout"
    }
    remove_field =&gt; "response_code"
  }
}</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>
<link linkend="plugins-filters-useragent">useragent filter</link>
</term>
<listitem>
<simpara>
Parses user agent strings into fields.
</simpara>
<simpara>The following example takes the user agent string in the <literal>agent</literal> field, parses
it into user agent fields, and adds the user agent fields to a new field called
<literal>user_agent</literal>. It also removes the original <literal>agent</literal> field:</simpara>
<programlisting language="json" linenumbering="unnumbered">filter {
  useragent {
    source =&gt; "agent"
    target =&gt; "user_agent"
    remove_field =&gt; "agent"
  }
}</programlisting>
<simpara>After the filter is applied, the event will be enriched with user agent fields.
For example:</simpara>
<programlisting language="json" linenumbering="unnumbered">        "user_agent": {
          "os": "Mac OS X 10.12",
          "major": "50",
          "minor": "0",
          "os_minor": "12",
          "os_major": "10",
          "name": "Firefox",
          "os_name": "Mac OS X",
          "device": "Other"
        }</programlisting>
</listitem>
</varlistentry>
</variablelist>
<remark> Deploying &amp; Scaling</remark>
</section>
</chapter>
<chapter id="deploying-and-scaling">
<title>Deploying and Scaling Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></title>
<simpara>The Elastic Stack is used for tons of use cases, from operational log and
metrics analytics, to enterprise and application search. Making sure your data
gets scalably, durably, and securely transported to Elasticsearch is extremely
important, especially for mission critical environments.</simpara>
<simpara>The goal of this document is to highlight the most common architecture patterns
for Logstash and how to effectively scale as your deployment grows. The focus
will be around the operational log, metrics, and security analytics use cases
because they tend to require larger scale deployments. The deploying and scaling
recommendations provided here may vary based on your own requirements.</simpara>
<bridgehead id="deploying-getting-started" renderas="sect2">Getting Started<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>For first time users, if you simply want to tail a log file to grasp the power
of the Elastic Stack, we recommend trying
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/filebeat-modules-overview.html">Filebeat Modules</ulink>. Filebeat Modules
enable you to quickly collect, parse, and index popular log types and view
pre-built Kibana dashboards within minutes.
<ulink url="https://www.elastic.co/guide/en/beats/metricbeat/6.x/metricbeat-modules.html">Metricbeat Modules</ulink> provide a similar
experience, but with metrics data. In this context, Beats will ship data
directly to Elasticsearch where <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/6.x/ingest.html">Ingest Nodes</ulink> will process
and index your data.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/deploy1.png"/>
  </imageobject>
  <textobject><phrase>static/images/deploy1.png</phrase></textobject>
</mediaobject>
</informalfigure>
<bridgehead id="_introducing_logstash" renderas="sect3">Introducing Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>What are the main benefits for integrating Logstash into your architecture?</simpara>
<itemizedlist>
<listitem>
<simpara>
Scale through ingestion spikes - Logstash has an adaptive disk-based
buffering system that will absorb incoming throughput, therefore mitigating
backpressure
</simpara>
</listitem>
<listitem>
<simpara>
Ingest from other data sources like databases, S3, or messaging queues
</simpara>
</listitem>
<listitem>
<simpara>
Emit data to multiple destinations like S3, HDFS, or write to a file
</simpara>
</listitem>
<listitem>
<simpara>
Compose more sophisticated processing pipelines with conditional dataflow logic
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="scaling-ingest" renderas="sect2">Scaling Ingest<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Beats and Logstash make ingest awesome. Together, they provide a comprehensive
solution that is scalable and resilient. What can you expect?</simpara>
<itemizedlist>
<listitem>
<simpara>
Horizontal scalability, high availability, and variable load handling
</simpara>
</listitem>
<listitem>
<simpara>
Message durability with at-least-once delivery guarantees
</simpara>
</listitem>
<listitem>
<simpara>
End-to-end secure transport with authentication and wire encryption
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_beats_and_logstash" renderas="sect3">Beats and Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Beats run across thousands of edge host servers, collecting, tailing, and
shipping logs to Logstash. Logstash serves as the centralized streaming
engine for data unification and enrichment. The
<link linkend="plugins-inputs-beats">Beats input plugin</link> exposes a secure,
acknowledgement-based endpoint for Beats to send data to Logstash.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/deploy2.png"/>
  </imageobject>
  <textobject><phrase>static/images/deploy2.png</phrase></textobject>
</mediaobject>
</informalfigure>
<note><simpara>Enabling persistent queues is strongly recommended, and these
architecture characteristics assume that they are enabled. We encourage you to
review the <xref linkend="persistent-queues"/> documentation for feature benefits and more
details on resiliency.</simpara></note>
<bridgehead id="_scalability" renderas="sect3">Scalability<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Logstash is horizontally scalable and can form groups of nodes running the same
pipeline. Logstash’s adaptive buffering capabilities will facilitate smooth
streaming even through variable throughput loads. If the Logstash layer becomes
an ingestion bottleneck, simply add more nodes to scale out. Here are a few
general recommendations:</simpara>
<itemizedlist>
<listitem>
<simpara>
Beats should <ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/load-balancing.html">load balance</ulink> across a group of
Logstash nodes.
</simpara>
</listitem>
<listitem>
<simpara>
A minimum of two Logstash nodes are recommended for high availability.
</simpara>
</listitem>
<listitem>
<simpara>
It’s common to deploy just one Beats input per Logstash node, but multiple
Beats inputs can also be deployed per Logstash node to expose independent
endpoints for different data sources.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_resiliency" renderas="sect3">Resiliency<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>When using <ulink url="https://www.elastic.co/products/beats/filebeat">Filebeat</ulink> or
<ulink url="https://www.elastic.co/products/beats/winlogbeat">Winlogbeat</ulink> for log collection
within this ingest flow, <emphasis role="strong">at-least-once delivery</emphasis> is guaranteed. Both the
communication protocols, from Filebeat or Winlogbeat to Logstash, and from
Logstash to Elasticsearch, are synchronous and support acknowledgements. The
other Beats don’t yet have support for acknowledgements.</simpara>
<simpara>Logstash persistent queues provide protection across node failures. For
disk-level resiliency in Logstash, it’s important to ensure disk redundancy.
For on-premise deployments, it&#8217;s recommended that you configure RAID. When
running in the cloud or a containerized environment, it’s recommended that you
use persistent disks with replication strategies that reflect your data SLAs.</simpara>
<note><simpara>Make sure <literal>queue.checkpoint.writes: 1</literal> is set for at-least-once
guarantees. For more details, see the
<link linkend="durability-persistent-queues">persistent queue durability</link> documentation.</simpara></note>
<bridgehead id="_processing" renderas="sect3">Processing<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Logstash will commonly extract fields with <link linkend="plugins-filters-grok">grok</link> or
<link linkend="plugins-filters-dissect">dissect</link>, augment
<link linkend="plugins-filters-geoip">geographical</link> info, and can further enrich events with
<link linkend="plugins-filters-translate">file</link>, <link linkend="plugins-filters-jdbc_streaming">database</link>,
or <link linkend="plugins-filters-elasticsearch">Elasticsearch</link> lookup datasets. Be aware
that processing complexity can affect overall throughput and CPU utilization.
Make sure to check out the other <link linkend="filter-plugins">available filter plugins</link>.</simpara>
<bridgehead id="_secure_transport" renderas="sect3">Secure Transport<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Enterprise-grade security is available across the entire delivery chain.</simpara>
<itemizedlist>
<listitem>
<simpara>
Wire encryption is recommended for both the transport from
<ulink url="https://www.elastic.co/guide/en/beats/filebeat/6.x/configuring-ssl-logstash.html">Beats to Logstash</ulink> and from
<ulink url="http://www.elastic.co/guide/en/logstash/6.x/ls-security.html">Logstash to Elasticsearch</ulink>.
</simpara>
</listitem>
<listitem>
<simpara>
There’s a wealth of security options when communicating with Elasticsearch
including basic authentication, TLS, PKI, LDAP, AD, and other custom realms.
To enable Elasticsearch security, consult the
<ulink url="https://www.elastic.co/guide/en/x-pack/6.x/xpack-security.html">X-Pack documentation</ulink>.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_monitoring" renderas="sect3">Monitoring<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>When running Logstash 5.2 or greater,
the <ulink url="https://www.elastic.co/products/x-pack/monitoring">Monitoring UI</ulink> provides
deep visibility into your deployment metrics, helping observe performance and
alleviate bottlenecks as you scale. Monitoring is an X-Pack feature under the
Basic License and is therefore <emphasis role="strong">free to use</emphasis>. To get started, consult the
<ulink url="https://www.elastic.co/guide/en/x-pack/6.x/monitoring-logstash.html">X-Pack Monitoring documentation</ulink>.</simpara>
<simpara>If external monitoring is preferred, there are <link linkend="monitoring">Monitoring APIs</link>
that return point-in-time metrics snapshots.</simpara>
<bridgehead id="adding-other-sources" renderas="sect2">Adding Other Popular Sources<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Users may have other mechanisms of collecting logging data, and it’s easy to
integrate and centralize them into the Elastic Stack. Let’s walk through a few
scenarios:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/deploy3.png"/>
  </imageobject>
  <textobject><phrase>static/images/deploy3.png</phrase></textobject>
</mediaobject>
</informalfigure>
<bridgehead id="_tcp_udp_and_http_protocols" renderas="sect3">TCP, UDP, and HTTP Protocols<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>The TCP, UDP, and HTTP protocols are common ways to feed data into Logstash.
Logstash can expose endpoint listeners with the respective
<link linkend="plugins-inputs-tcp">TCP</link>, <link linkend="plugins-inputs-udp">UDP</link>, and
<link linkend="plugins-inputs-http">HTTP</link> input plugins. The data sources enumerated below
are typically ingested through one of these three protocols.</simpara>
<note><simpara>The TCP protocol does not support application-level acknowledgements, so
connectivity issues may result in data loss.</simpara></note>
<simpara>For high availability scenarios, a third-party hardware or software load
balancer, like HAProxy, should be added to fan out traffic to a group of
Logstash nodes.</simpara>
<bridgehead id="_network_and_security_data" renderas="sect3">Network and Security Data<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Although Beats may already satisfy your data ingest use case, network and
security datasets come in a variety of forms. Let’s touch on a few other
ingestion points.</simpara>
<itemizedlist>
<listitem>
<simpara>
Network wire data - collect and analyze network traffic with
<ulink url="https://www.elastic.co/products/beats/packetbeat">Packetbeat</ulink>.
</simpara>
</listitem>
<listitem>
<simpara>
Netflow v5/v9/v10 - Logstash understands data from Netflow/IPFIX exporters
with the <link linkend="plugins-codecs-netflow">Netflow codec</link>.
</simpara>
</listitem>
<listitem>
<simpara>
Nmap - Logstash accepts and parses Nmap XML data with the
<link linkend="plugins-codecs-nmap">Nmap codec</link>.
</simpara>
</listitem>
<listitem>
<simpara>
SNMP trap - Logstash has a native <link linkend="plugins-inputs-snmptrap">SNMP trap input</link>.
</simpara>
</listitem>
<listitem>
<simpara>
CEF - Logstash accepts and parses CEF data from systems like Arcsight
SmartConnectors with the <link linkend="plugins-codecs-cef">CEF codec</link>. See this
<ulink url="https://www.elastic.co/blog/integrating-elastic-stack-with-arcsight-siem-part-1">blog series</ulink>
for more details.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_centralized_syslog_servers" renderas="sect3">Centralized Syslog Servers<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Existing syslog server technologies like rsyslog and syslog-ng generally send
syslog over to Logstash TCP or UDP endpoints for extraction, processing, and
persistence. If the data format conforms to RFC3164, it can be fed directly
to the <link linkend="plugins-inputs-syslog">Logstash syslog input</link>.</simpara>
<bridgehead id="_infrastructure_amp_application_data_and_iot" renderas="sect3">Infrastructure &amp; Application Data and IoT<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Infrastructure and application metrics can be collected with
<ulink url="https://www.elastic.co/products/beats/metricbeat">Metricbeat</ulink>, but applications
can also send webhooks to a Logstash HTTP input or have metrics polled from an
HTTP endpoint with the <link linkend="plugins-inputs-http_poller">HTTP poller input plugin</link>.</simpara>
<simpara>For applications that log with log4j2, it’s recommended to use the
SocketAppender to send JSON to the Logstash TCP input. Alternatively, log4j2
can also log to a file for collection with FIlebeat. Usage of the log4j1
SocketAppender is not recommended.</simpara>
<simpara>IoT devices like Rasberry Pis, smartphones, and connected vehicles often send
telemetry data through one of these protocols.</simpara>
<bridgehead id="integrating-with-messaging-queues" renderas="sect2">Integrating with Messaging Queues<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>If you are leveraging message queuing technologies as part of your existing
infrastructure, getting that data into the Elastic Stack is easy. For existing
users who are utilizing an external queuing layer like Redis or RabbitMQ just
for data buffering with Logstash, it’s recommended to use Logstash persistent
queues instead of an external queuing layer. This will help with overall ease
of management by removing an unnecessary layer of complexity in your ingest
architecture.</simpara>
<simpara>For users who want to integrate data from existing Kafka deployments or require
the underlying usage of ephemeral storage, Kafka can serve as a data hub where
Beats can persist to and Logstash nodes can consume from.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/deploy4.png"/>
  </imageobject>
  <textobject><phrase>static/images/deploy4.png</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The other TCP, UDP, and HTTP sources can persist to Kafka with Logstash as a
conduit to achieve high availability in lieu of a load balancer. A group of
Logstash nodes can then consume from topics with the
<link linkend="plugins-inputs-kafka">Kafka input</link> to further transform and enrich the data in
transit.</simpara>
<bridgehead id="_resiliency_and_recovery" renderas="sect3">Resiliency and Recovery<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>When Logstash consumes from Kafka, persistent queues should be enabled and will
add transport resiliency to mitigate the need for reprocessing during Logstash
node failures. In this context, it’s recommended to use the default persistent
queue disk allocation size <literal>queue.max_bytes: 1GB</literal>.</simpara>
<simpara>If Kafka is configured to retain data for an extended period of time, data can
be reprocessed from Kafka in the case of disaster recovery and reconciliation.</simpara>
<bridgehead id="_other_messaging_queue_integrations" renderas="sect3">Other Messaging Queue Integrations<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc">Edit me</ulink></bridgehead>
<simpara>Although an additional queuing layer is not required, Logstash can consume from
a myriad of other message queuing technologies like
<link linkend="plugins-inputs-rabbitmq">RabbitMQ</link> and <link linkend="plugins-inputs-redis">Redis</link>. It also
supports ingestion from hosted queuing services like
<link linkend="plugins-inputs-google_pubsub">Pub/Sub</link>, <link linkend="plugins-inputs-kinesis">Kinesis</link>, and
<link linkend="plugins-inputs-sqs">SQS</link>.</simpara>
<remark> Troubleshooting performance</remark>
</chapter>
<chapter id="performance-tuning">
<title>Performance Tuning<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></title>
<simpara>This section includes the following information about tuning Logstash
performance:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="performance-troubleshooting"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="tuning-logstash"/>
</simpara>
</listitem>
</itemizedlist>
<section id="performance-troubleshooting">
<title>Performance Troubleshooting Guide<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></title>
<simpara>You can use this troubleshooting guide to quickly diagnose and resolve Logstash performance problems. Advanced knowledge of pipeline internals is not required to understand this guide. However, the <link linkend="pipeline">pipeline documentation</link> is recommended reading if you want to go beyond this guide.</simpara>
<simpara>You may be tempted to jump ahead and change settings like <literal>pipeline.workers</literal> (<literal>-w</literal>) as a first attempt to improve performance. In our experience, changing this setting makes it more difficult to troubleshoot performance problems because you increase the number of variables in play. Instead, make one change at a time and measure the results. Starting at the end of this list is a sure-fire way to create a confusing situation.</simpara>
<bridgehead id="_performance_checklist" renderas="sect3">Performance Checklist<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>
<emphasis role="strong">Check the performance of input sources and output destinations:</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Logstash is only as fast as the services it connects to. Logstash can only consume and produce data as fast as its input and output destinations can!
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">Check system statistics:</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
CPU
</simpara>
<itemizedlist>
<listitem>
<simpara>
Note whether the CPU is being heavily used. On Linux/Unix, you can run <literal>top -H</literal> to see process statistics broken out by thread, as well as total CPU statistics.
</simpara>
</listitem>
<listitem>
<simpara>
If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning Logstash worker settings.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Memory
</simpara>
<itemizedlist>
<listitem>
<simpara>
Be aware of the fact that Logstash runs on the Java VM. This means that Logstash will always use the maximum amount of memory you allocate to it.
</simpara>
</listitem>
<listitem>
<simpara>
Look for other applications that use large amounts of memory and may be causing Logstash to swap to disk. This can happen if the total memory used by applications exceeds physical memory.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
I/O Utilization
</simpara>
<itemizedlist>
<listitem>
<simpara>
Monitor disk I/O to check for disk saturation.
</simpara>
<itemizedlist>
<listitem>
<simpara>
Disk saturation can happen if you’re using Logstash plugins (such as the file output) that may saturate your storage.
</simpara>
</listitem>
<listitem>
<simpara>
Disk saturation can also happen if you&#8217;re encountering a lot of errors that force Logstash to generate large error logs.
</simpara>
</listitem>
<listitem>
<simpara>
On Linux, you can use iostat, dstat, or something similar to monitor disk I/O.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
Monitor network I/O for network saturation.
</simpara>
<itemizedlist>
<listitem>
<simpara>
Network saturation can happen if you’re using inputs/outputs that perform a lot of network operations.
</simpara>
</listitem>
<listitem>
<simpara>
On Linux, you can use a tool like dstat or iftop to monitor your network.
</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">Check the JVM heap:</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Often times CPU utilization can go through the roof if the heap size is too low, resulting in the JVM constantly garbage collecting.
</simpara>
</listitem>
<listitem>
<simpara>
A quick way to check for this issue is to double the heap size and see if performance improves. Do not increase the heap size past the amount of physical memory. Leave at least 1GB free for the OS and other processes.
</simpara>
</listitem>
<listitem>
<simpara>
You can make more accurate measurements of the JVM heap by using either the <literal>jmap</literal> command line utility distributed with Java or by using VisualVM. For more info, see <xref linkend="profiling-the-heap"/>.
</simpara>
</listitem>
<listitem>
<simpara>
Always make sure to set the minimum (Xms) and maximum (Xmx) heap allocation size to the same value to prevent the heap from resizing at runtime, which is a very costly process.
</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>
<emphasis role="strong">Tune Logstash worker settings:</emphasis>
</simpara>
<itemizedlist>
<listitem>
<simpara>
Begin by scaling up the number of pipeline workers by using the <literal>-w</literal> flag. This will increase the number of threads available for filters and outputs. It is safe to scale this up to a multiple of CPU cores, if need be, as the threads can become idle on I/O.
</simpara>
</listitem>
<listitem>
<simpara>
You may also tune the output batch size. For many outputs, such as the Elasticsearch output, this setting will correspond to the size of I/O operations. In the case of the Elasticsearch output, this setting corresponds to the batch size.
</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section id="tuning-logstash">
<title>Tuning and Profiling Logstash Performance<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></title>
<simpara>The Logstash defaults are chosen to provide fast, safe performance for most
users. However if you notice performance issues, you may need to modify
some of the defaults. Logstash provides the following configurable options
for tuning pipeline performance: <literal>pipeline.workers</literal>, <literal>pipeline.batch.size</literal>, and <literal>pipeline.batch.delay</literal>. For more information about setting these options, see <xref linkend="logstash-settings-file"/>.</simpara>
<simpara>Make sure you&#8217;ve read the <xref linkend="performance-troubleshooting"/> before modifying these options.</simpara>
<itemizedlist>
<listitem>
<simpara>
The <literal>pipeline.workers</literal> setting determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
</simpara>
</listitem>
<listitem>
<simpara>
The <literal>pipeline.batch.size</literal> setting defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the <literal>LS_HEAP_SIZE</literal> variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues <ulink url="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">bulk requests</ulink> for each batch received. Tuning the <literal>pipeline.batch.size</literal> setting adjusts the size of bulk requests sent to Elasticsearch.
</simpara>
</listitem>
<listitem>
<simpara>
The <literal>pipeline.batch.delay</literal> setting rarely needs to be tuned. This setting adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash begins to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the <literal>pipeline.batch.delay</literal> and  <literal>pipeline.batch.size</literal> settings.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="_notes_on_pipeline_configuration_and_performance" renderas="sect3">Notes on Pipeline Configuration and Performance<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></bridgehead>
<simpara>If you plan to modify the default pipeline settings, take into account the
following suggestions:</simpara>
<itemizedlist>
<listitem>
<simpara>
The total number of inflight events is determined by the product of the  <literal>pipeline.workers</literal> and <literal>pipeline.batch.size</literal> settings. This product is referred to as the <emphasis>inflight count</emphasis>.  Keep the value of the inflight count in mind as you adjust the <literal>pipeline.workers</literal> and <literal>pipeline.batch.size</literal> settings. Pipelines that intermittently receive large events at irregular intervals require sufficient memory to handle these spikes. Configure the <literal>LS_HEAP_SIZE</literal> variable accordingly.
</simpara>
</listitem>
<listitem>
<simpara>
Measure each change to make sure it increases, rather than decreases, performance.
</simpara>
</listitem>
<listitem>
<simpara>
Ensure that you leave enough memory available to cope with a sudden increase in event size. For example, an application that generates exceptions that are represented as large blobs of text.
</simpara>
</listitem>
<listitem>
<simpara>
The number of workers may be set higher than the number of CPU cores since outputs often spend idle time in I/O wait conditions.
</simpara>
</listitem>
<listitem>
<simpara>
Threads in Java have names and you can use the <literal>jstack</literal>, <literal>top</literal>, and the VisualVM graphical tools to figure out which resources a given thread uses.
</simpara>
</listitem>
<listitem>
<simpara>
On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as <literal>[base]&lt;inputname</literal>, and pipeline workers show up as <literal>[base]&gt;workerN</literal>, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
</simpara>
</listitem>
</itemizedlist>
<bridgehead id="profiling-the-heap" renderas="sect3">Profiling the Heap<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc">Edit me</ulink></bridgehead>
<simpara>When tuning Logstash you may have to adjust the heap size. You can use the <ulink url="https://visualvm.java.net/">VisualVM</ulink> tool to profile the heap. The <emphasis role="strong">Monitor</emphasis> pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample <emphasis role="strong">Monitor</emphasis> panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/pipeline_overload.png"/>
  </imageobject>
  <textobject><phrase>static/images/pipeline_overload.png</phrase></textobject>
</mediaobject>
</informalfigure>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="static/images/pipeline_correct_load.png"/>
  </imageobject>
  <textobject><phrase>static/images/pipeline_correct_load.png</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>In the first example we see that the CPU isn’t being used very efficiently. In fact, the JVM is often times having to stop the VM for “full GCs”. Full garbage collections are a common symptom of excessive memory pressure. This is visible in the spiky pattern on the CPU chart. In the more efficiently configured example, the GC graph pattern is more smooth, and the CPU is used in a more uniform manner. You can also see that there is ample headroom between the allocated heap size, and the maximum allowed, giving the JVM GC a lot of room to work with.</simpara>
<simpara>Examining the in-depth GC statistics with a tool similar to the excellent <ulink url="https://visualvm.java.net/plugins.html">VisualGC</ulink> plugin shows that the over-allocated VM spends very little time in the efficient Eden GC, compared to the time spent in the more resource-intensive Old Gen “Full” GCs.</simpara>
<note><simpara>As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
This document is not a comprehensive guide to JVM GC tuning. Read the official <ulink url="http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html">Oracle guide</ulink> for more information on the topic. We also recommend reading <ulink url="http://www.semicomplete.com/blog/geekery/debugging-java-performance.html">Debugging Java Performance</ulink>.</simpara></note>
<remark> Monitoring overview</remark>
</section>
</chapter>
<chapter id="monitoring-logstash">
<title>Monitoring Logstash<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring.asciidoc">Edit me</ulink></title>
<simpara>When you run Logstash, it automatically captures runtime metrics that you can
use to monitor the health and performance of your Logstash deployment.</simpara>
<simpara>The metrics collected by Logstash include:</simpara>
<itemizedlist>
<listitem>
<simpara>
Logstash node info, like pipeline settings, OS info, and JVM info.
</simpara>
</listitem>
<listitem>
<simpara>
Plugin info, including a list of installed plugins.
</simpara>
</listitem>
<listitem>
<simpara>
Node stats, like JVM stats, process stats, event-related stats, and pipeline
runtime stats.
</simpara>
</listitem>
<listitem>
<simpara>
Hot threads.
</simpara>
</listitem>
</itemizedlist>
<simpara>You can use the basic <link linkend="monitoring">monitoring APIs</link> providing by Logstash
to retrieve these metrics. These APIs are available by default without
requiring any extra configuration.</simpara>
<simpara>Alternatively, you can <link linkend="installing-xpack-log">install X-Pack</link> and
configure X-Pack monitoring.</simpara>
<note><simpara>Monitoring is an X-Pack feature under the Basic License and is therefore
<emphasis role="strong">free to use</emphasis>.</simpara></note>
<simpara>You can use the <link linkend="logstash-monitoring-ui">monitoring UI</link> in X-Pack to view
the metrics and gain insight into how your Logstash deployment is running.</simpara>
<simpara>The <link linkend="logstash-pipeline-viewer">pipeline viewer</link> in X-Pack offers additional
visibility into the behavior and performance of complex pipeline configurations.
It shows a graph representation of the overall pipeline topology, data flow, and
branching logic, overlayed with important metrics, like events per second, for
each plugin in the view.</simpara>
<simpara>This documentation focuses on the X-Pack monitoring infrastructure and setup in
Logstash. For an introduction to monitoring your Elastic stack, including Elasticsearch
and Kibana, see <ulink url="https://www.elastic.co/guide/en/x-pack/6.x/xpack-monitoring.html">Monitoring the Elastic Stack</ulink>.</simpara>
<section id="logstash-monitoring-ui" role="xpack">
<title>Monitoring UI</title>
<simpara>When running Logstash 5.2 or greater, you can use the
<ulink url="https://www.elastic.co/products/x-pack/monitoring">monitoring feature in X-Pack</ulink>
to gain deep visibility into metrics about your Logstash deployment. In the
overview dashboard, you can see all events received and sent by Logstash, plus
info about memory usage and uptime:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/overviewstats.png"/>
  </imageobject>
  <textobject><phrase>Logstash monitoring overview dashboard in Kibana</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>Then you can drill down to see stats about a specific node:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/nodestats.png"/>
  </imageobject>
  <textobject><phrase>Logstash monitoring node stats dashboard in Kibana</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>Before you can use the monitoring UI, you must
<link linkend="setup-xpack">set up X-Pack</link> and configure Logstash monitoring.</simpara>
<simpara>For information about using the Monitoring UI, see
<ulink url="https://www.elastic.co/guide/en/kibana/6.x/xpack-monitoring.html">X-Pack monitoring in Kibana</ulink>.</simpara>
</section>
<section id="logstash-pipeline-viewer" role="xpack">
<title>Pipeline Viewer UI</title>
<simpara>The pipeline viewer in X-Pack provides a simple way for you to visualize and
monitor the behavior of complex Logstash pipeline configurations. Within the
pipeline viewer, you can explore a directed acyclic graph (DAG) representation
of the overall pipeline topology, data flow, and branching logic. The diagram
is overlayed with important metrics, like events per second and time spent in
milliseconds, for each plugin in the view.</simpara>
<simpara>The diagram includes visual indicators to draw your attention to potential
bottlenecks in the pipeline, making it easy for you to diagnose and fix
problems.</simpara>
<important>
<simpara>When you configure the stages in your Logstash pipeline, make sure you specify
semantic IDs. If you don&#8217;t specify IDs, Logstash generates them for you.</simpara>
<simpara>Using semantic IDs makes it easier to identify the configurations that are
causing bottlenecks. For example, you may have several grok filters running
in your pipeline. If you haven&#8217;t specified semantic IDs, you won&#8217;t be able
to tell at a glance which filters are slow. If you specify semantic IDs,
such as <literal>apacheParsingGrok</literal> and <literal>cloudwatchGrok</literal>, you&#8217;ll know exactly which
grok filters are causing bottlenecks.</simpara>
</important>
<simpara>Before using the pipeline viewer, you need to <link linkend="setup-xpack">set up X-Pack</link> and
<link linkend="monitoring-logstash">configure Logstash monitoring</link>.</simpara>
<bridgehead id="_what_types_of_problems_does_the_pipeline_viewer_show" renderas="sect3">What types of problems does the pipeline viewer show?</bridgehead>
<simpara>The pipeline viewer highlights CPU% and event latency in cases where the values
are anomalous. The purpose of these highlights is to enable users to quickly
identify processing that is disproportionately slow. This may not necessarily
mean that anything is wrong with a given plugin, since some plugins are slower
than others due to the nature of the work they do. For instance, you may find
that a grok filter that uses a complicated regexp runs a lot slower than a
mutate filter that simply adds a field. The grok filter might be highlighted in
this case, though it may not be possible to further optimize its work.</simpara>
<simpara>The exact formula used is a heuristic, and thus is subject to change.</simpara>
<bridgehead id="_view_the_pipeline_diagram" renderas="sect3">View the pipeline diagram</bridgehead>
<simpara>To view the pipeline diagram:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
In Logstash, start the Logstash pipeline that you want to monitor.
</simpara>
<simpara>Assuming that you&#8217;ve set up Logstash monitoring, Logstash will begin shipping
metrics to the monitoring cluster.</simpara>
</listitem>
<listitem>
<simpara>
Navigate to the Monitoring tab in Kibana.
</simpara>
<simpara>You should see a Logstash section.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/monitoring-ui.png"/>
  </imageobject>
  <textobject><phrase>Monitoring UI</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>
Click the <emphasis role="strong">Pipelines</emphasis> link under Logstash to see all the pipelines that are
being monitored.
</simpara>
<simpara>Each pipeline is identified by a pipeline ID (<literal>main</literal> by default). For each
pipeline, you&#8217;ll see a list of all versions of the pipeline stats that were
captured during the specified time range.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/pipeline-viewer-overview.png"/>
  </imageobject>
  <textobject><phrase>Pipeline Overview</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The version information is auto-generated by Logstash. Each time you modify a
pipeline, Logstash generates a new version hash. Viewing different versions
of the pipeline stats allows you see how changes to the pipeline over time
affect throughput and other metrics. Note that Logstash stores multiple versions
of the pipeline stats; it does not store multiple versions of the pipeline
configurations themselves.</simpara>
</listitem>
<listitem>
<simpara>
Click a pipeline version in the list to drill down and explore the pipeline
diagram.
</simpara>
<simpara>The diagram shows all the stages feeding data through the pipeline. It also shows
conditional logic.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/pipeline-diagram.png"/>
  </imageobject>
  <textobject><phrase>Pipeline Diagram</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The information displayed on each node varies depending on the plugin type.</simpara>
<simpara>Here&#8217;s an example of an <emphasis role="strong">input</emphasis> node:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/pipeline-input-detail.png"/>
  </imageobject>
  <textobject><phrase>Input node</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The <emphasis role="strong">I</emphasis> badge indicates that this is an input stage. The node shows:</simpara>
<itemizedlist>
<listitem>
<simpara>
input type - <emphasis role="strong">stdin</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
user-supplied ID - <emphasis role="strong">logfileRead</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
throughput expressed in events per second - <emphasis role="strong">0.7 e/s</emphasis>
</simpara>
</listitem>
</itemizedlist>
<simpara>Here&#8217;s an example of a <emphasis role="strong">filter</emphasis> node.</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/pipeline-filter-detail.png"/>
  </imageobject>
  <textobject><phrase>Filter node</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The filter icon indicates that this is a filter stage. The node shows:</simpara>
<itemizedlist>
<listitem>
<simpara>
filter type - <emphasis role="strong">sleep</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
user-supplied ID - <emphasis role="strong">caSleep</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
worker usage expressed as the percentage of total execution time - <emphasis role="strong">0%</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
performance - the number of milliseconds spent processing each event - <emphasis role="strong">20.00 ms/e</emphasis>
</simpara>
</listitem>
<listitem>
<simpara>
throughput - the number of events sent per second - <emphasis role="strong">0.0 e/s</emphasis>
</simpara>
</listitem>
</itemizedlist>
<simpara>Stats that are anomalously slow appear highlighted in the pipeline viewer.
This doesn&#8217;t necessarily indicate a problem, but it highlights potential
bottle necks so that you can find them quickly.</simpara>
<simpara>An <emphasis role="strong">output</emphasis> node shows the same information as a filter node, but it has an
<emphasis role="strong">O</emphasis> badge to indicate that it is an output stage:</simpara>
<informalfigure>
<mediaobject>
  <imageobject>
  <imagedata fileref="monitoring/images/pipeline-output-detail.png"/>
  </imageobject>
  <textobject><phrase>Output node</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>
Hover over a node in the diagram, and you&#8217;ll see only the related nodes that
are ancestors or descendants of the current node.
</simpara>
</listitem>
<listitem>
<simpara>
Explore the diagram and look for performance anomalies.
</simpara>
</listitem>
</orderedlist>
</section>
<section id="monitoring-troubleshooting" role="xpack">
<title>Troubleshooting X-Pack monitoring in Logstash</title>
<titleabbrev>Troubleshooting</titleabbrev>
<bridgehead id="_logstash_monitoring_not_working_after_upgrade" renderas="sect3">Logstash Monitoring Not Working After Upgrade</bridgehead>
<simpara>When upgrading from older versions, the built-in <literal>logstash_system</literal> user is
disabled for security reasons. To resume monitoring:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Change the <literal>logstash_system</literal> password:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">PUT _xpack/security/user/logstash_system/_password
{
  "password": "newpassword"
}</programlisting>
<remark>CONSOLE</remark>
</listitem>
<listitem>
<simpara>
Re-enable the <literal>logstash_system</literal> user:
</simpara>
<programlisting language="sh" linenumbering="unnumbered">PUT _xpack/security/user/logstash_system/_enable</programlisting>
<remark>CONSOLE</remark>
</listitem>
</orderedlist>
<remark> Monitoring APIs</remark>
</section>
</chapter>
<chapter id="monitoring">
<title>Monitoring APIs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>Logstash provides the following monitoring APIs to retrieve runtime metrics
about Logstash:</simpara>
<itemizedlist>
<listitem>
<simpara>
<xref linkend="node-info-api"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="plugins-api"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="node-stats-api"/>
</simpara>
</listitem>
<listitem>
<simpara>
<xref linkend="hot-threads-api"/>
</simpara>
</listitem>
</itemizedlist>
<simpara>You can use the root resource to retrieve general information about the Logstash instance, including
the host and version.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
   "host": "skywalker",
   "version": "6.1.0",
   "http_address": "127.0.0.1:9600"
}</programlisting>
<note><simpara>By default, the monitoring API attempts to bind to <literal>tcp:9600</literal>. If this port is already in use by another Logstash
instance, you need to launch Logstash with the <literal>--http.port</literal> flag specified to bind to a different port. See
<xref linkend="command-line-flags"/> for more information.</simpara></note>
<bridgehead id="monitoring-common-options" renderas="sect2">Common Options<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></bridgehead>
<simpara>The following options can be applied to all of the Logstash monitoring APIs.</simpara>
<bridgehead id="_pretty_results" renderas="sect3">Pretty Results<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></bridgehead>
<simpara>When appending <literal>?pretty=true</literal> to any request made, the JSON returned
will be pretty formatted (use it for debugging only!).</simpara>
<bridgehead id="_human_readable_output" renderas="sect3">Human-Readable Output<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></bridgehead>
<note><simpara>For Logstash 6.1.0, the <literal>human</literal> option is supported for the <xref linkend="hot-threads-api"/>
only. When you specify <literal>human=true</literal>, the results are returned in plain text instead of
JSON format. The default is false.</simpara></note>
<simpara>Statistics are returned in a format suitable for humans
(eg <literal>"exists_time": "1h"</literal> or <literal>"size": "1kb"</literal>) and for computers
(eg <literal>"exists_time_in_millis": 3600000</literal> or <literal>"size_in_bytes": 1024</literal>).
The human-readable values can be turned off by adding <literal>?human=false</literal>
to the query string. This makes sense when the stats results are
being consumed by a monitoring tool, rather than intended for human
consumption.  The default for the <literal>human</literal> flag is
<literal>false</literal>.</simpara>
<section id="node-info-api">
<title>Node Info API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The node info API retrieves information about the node.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/&lt;types&gt;'</programlisting>
<simpara>Where <literal>&lt;types&gt;</literal> is optional and specifies the types of node info you want to return.</simpara>
<simpara>You can limit the info that&#8217;s returned by combining any of the following types in a comma-separated list:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<link linkend="node-pipeline-info"><literal>pipelines</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets pipeline-specific information and settings for each pipeline.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="node-os-info"><literal>os</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets node-level info about the OS.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="node-jvm-info"><literal>jvm</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets node-level JVM info, including info about threads.
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>See <link linkend="monitoring-common-options">Common Options</link> for a list of options that can be applied to all
Logstash monitoring APIs.</simpara>
<section id="node-pipeline-info">
<title>Pipeline Info<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document that shows pipeline info, such as the number of workers,
batch size, and batch delay:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/pipelines?pretty'</programlisting>
<simpara>If you want to view additional information about a pipeline, such as stats for each configured input, filter,
or output stage, see the <xref linkend="pipeline-stats"/> section under the <xref linkend="node-stats-api"/>.</simpara>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "pipelines" : {
    "test" : {
      "workers" : 1,
      "batch_size" : 1,
      "batch_delay" : 5,
      "config_reload_automatic" : false,
      "config_reload_interval" : 3
    },
    "test2" : {
      "workers" : 8,
      "batch_size" : 125,
      "batch_delay" : 5,
      "config_reload_automatic" : false,
      "config_reload_interval" : 3
    }
  }</programlisting>
<simpara>You can see the info for a specific pipeline by including the pipeline ID. In
the following example, the ID of the pipeline is <literal>test</literal>:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/pipelines/test?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "pipelines" : {
    "test" : {
      "workers" : 1,
      "batch_size" : 1,
      "batch_delay" : 5,
      "config_reload_automatic" : false,
      "config_reload_interval" : 3
    }
  }</programlisting>
<simpara>If you specify an invalid pipeline ID, the request returns a 404 Not Found error.</simpara>
</section>
<section id="node-os-info">
<title>OS Info<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document that shows the OS name, architecture, version, and
available processors:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/os?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "os": {
    "name": "Mac OS X",
    "arch": "x86_64",
    "version": "10.12.4",
    "available_processors": 8
  }</programlisting>
</section>
<section id="node-jvm-info">
<title>JVM Info<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document that shows node-level JVM stats, such as the JVM process id, version,
VM info, memory usage, and info about garbage collectors:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/jvm?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "jvm": {
    "pid": 59616,
    "version": "1.8.0_65",
    "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
    "vm_version": "1.8.0_65",
    "vm_vendor": "Oracle Corporation",
    "start_time_in_millis": 1484251185878,
    "mem": {
      "heap_init_in_bytes": 268435456,
      "heap_max_in_bytes": 1037959168,
      "non_heap_init_in_bytes": 2555904,
      "non_heap_max_in_bytes": 0
    },
    "gc_collectors": [
      "ParNew",
      "ConcurrentMarkSweep"
    ]
  }
}</programlisting>
</section>
</section>
<section id="plugins-api">
<title>Plugins Info API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The plugins info API gets information about all Logstash plugins that are currently installed.
This API basically returns the output of running the <literal>bin/logstash-plugin list --verbose</literal> command.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/plugins?pretty'</programlisting>
<simpara>See <link linkend="monitoring-common-options">Common Options</link> for a list of options that can be applied to all
Logstash monitoring APIs.</simpara>
<simpara>The output is a JSON document.</simpara>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "total": 93,
  "plugins": [
    {
      "name": "logstash-codec-cef",
      "version": "4.1.2"
    },
    {
      "name": "logstash-codec-collectd",
      "version": "3.0.3"
    },
    {
      "name": "logstash-codec-dots",
      "version": "3.0.2"
    },
    {
      "name": "logstash-codec-edn",
      "version": "3.0.2"
    },
    .
    .
    .
  ]</programlisting>
</section>
<section id="node-stats-api">
<title>Node Stats API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The node stats API retrieves runtime stats about Logstash.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/&lt;types&gt;'</programlisting>
<simpara>Where <literal>&lt;types&gt;</literal> is optional and specifies the types of stats you want to return.</simpara>
<simpara>By default, all stats are returned. You can limit the info that&#8217;s returned by combining any of the following types in a comma-separated list:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<link linkend="jvm-stats"><literal>jvm</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets JVM stats, including stats about threads, memory usage, garbage collectors,
and uptime.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="process-stats"><literal>process</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets process stats, including stats about file descriptors, memory consumption, and CPU usage.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="event-stats"><literal>events</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets event-related statistics for the Logstash instance (regardless of how many
pipelines were created and destroyed).
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="pipeline-stats"><literal>pipelines</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets runtime stats about each Logstash pipeline.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="reload-stats"><literal>reloads</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets runtime stats about config reload successes and failures.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<link linkend="os-stats"><literal>os</literal></link>
</simpara>
</entry>
<entry>
<simpara>
Gets runtime stats about cgroups when Logstash is running in a container.
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>See <link linkend="monitoring-common-options">Common Options</link> for a list of options that can be applied to all
Logstash monitoring APIs.</simpara>
<section id="jvm-stats">
<title>JVM Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document containing JVM stats:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/jvm?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "jvm" : {
    "threads" : {
      "count" : 49,
      "peak_count" : 50
    },
    "mem" : {
      "heap_used_percent" : 14,
      "heap_committed_in_bytes" : 309866496,
      "heap_max_in_bytes" : 1037959168,
      "heap_used_in_bytes" : 151686096,
      "non_heap_used_in_bytes" : 122486176,
      "non_heap_committed_in_bytes" : 133222400,
      "pools" : {
        "survivor" : {
          "peak_used_in_bytes" : 8912896,
          "used_in_bytes" : 288776,
          "peak_max_in_bytes" : 35782656,
          "max_in_bytes" : 35782656,
          "committed_in_bytes" : 8912896
        },
        "old" : {
          "peak_used_in_bytes" : 148656848,
          "used_in_bytes" : 148656848,
          "peak_max_in_bytes" : 715849728,
          "max_in_bytes" : 715849728,
          "committed_in_bytes" : 229322752
        },
        "young" : {
          "peak_used_in_bytes" : 71630848,
          "used_in_bytes" : 2740472,
          "peak_max_in_bytes" : 286326784,
          "max_in_bytes" : 286326784,
          "committed_in_bytes" : 71630848
        }
      }
    },
    "gc" : {
      "collectors" : {
        "old" : {
          "collection_time_in_millis" : 607,
          "collection_count" : 12
        },
        "young" : {
          "collection_time_in_millis" : 4904,
          "collection_count" : 1033
        }
      }
    },
    "uptime_in_millis" : 1809643
  }</programlisting>
</section>
<section id="process-stats">
<title>Process Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document containing process stats:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/process?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "process" : {
    "open_file_descriptors" : 184,
    "peak_open_file_descriptors" : 185,
    "max_file_descriptors" : 10240,
    "mem" : {
      "total_virtual_in_bytes" : 5486125056
    },
    "cpu" : {
      "total_in_millis" : 657136,
      "percent" : 2,
      "load_average" : {
        "1m" : 2.38134765625
      }
    }
  }</programlisting>
</section>
<section id="event-stats">
<title>Event Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document containing event-related statistics
for the Logstash instance:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/events?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "events" : {
    "in" : 293658,
    "filtered" : 293658,
    "out" : 293658,
    "duration_in_millis" : 2324391,
    "queue_push_duration_in_millis" : 343816
  }</programlisting>
</section>
<section id="pipeline-stats">
<title>Pipeline Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document containing pipeline stats,
including:</simpara>
<itemizedlist>
<listitem>
<simpara>
the number of events that were input, filtered, or output by each pipeline
</simpara>
</listitem>
<listitem>
<simpara>
stats for each configured filter or output stage
</simpara>
</listitem>
<listitem>
<simpara>
info about config reload successes and failures
(when <link linkend="reloading-config">config reload</link> is enabled)
</simpara>
</listitem>
<listitem>
<simpara>
info about the persistent queue (when
<link linkend="persistent-queues">persistent queues</link> are enabled)
</simpara>
</listitem>
</itemizedlist>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/pipelines?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "pipelines" : {
    "test" : {
      "events" : {
        "duration_in_millis" : 365495,
        "in" : 216485,
        "filtered" : 216485,
        "out" : 216485,
        "queue_push_duration_in_millis" : 342466
      },
      "plugins" : {
        "inputs" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-1",
          "events" : {
            "out" : 216485,
            "queue_push_duration_in_millis" : 342466
          },
          "name" : "beats"
        } ],
        "filters" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-2",
          "events" : {
            "duration_in_millis" : 55969,
            "in" : 216485,
            "out" : 216485
          },
          "failures" : 216485,
          "patterns_per_field" : {
            "message" : 1
          },
          "name" : "grok"
        }, {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-3",
          "events" : {
            "duration_in_millis" : 3326,
            "in" : 216485,
            "out" : 216485
          },
          "name" : "geoip"
        } ],
        "outputs" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-4",
          "events" : {
            "duration_in_millis" : 278557,
            "in" : 216485,
            "out" : 216485
          },
          "name" : "elasticsearch"
        } ]
      },
      "reloads" : {
        "last_error" : null,
        "successes" : 0,
        "last_success_timestamp" : null,
        "last_failure_timestamp" : null,
        "failures" : 0
      },
      "queue" : {
        "type" : "memory"
      }
    },
    "test2" : {
      "events" : {
        "duration_in_millis" : 2222229,
        "in" : 87247,
        "filtered" : 87247,
        "out" : 87247,
        "queue_push_duration_in_millis" : 1532
      },
      "plugins" : {
        "inputs" : [ {
          "id" : "d7ea8941c0fc48ac58f89c84a9da482107472b82-1",
          "events" : {
            "out" : 87247,
            "queue_push_duration_in_millis" : 1532
          },
          "name" : "twitter"
        } ],
        "filters" : [ ],
        "outputs" : [ {
          "id" : "d7ea8941c0fc48ac58f89c84a9da482107472b82-2",
          "events" : {
            "duration_in_millis" : 139545,
            "in" : 87247,
            "out" : 87247
          },
          "name" : "elasticsearch"
        } ]
      },
      "reloads" : {
        "last_error" : null,
        "successes" : 0,
        "last_success_timestamp" : null,
        "last_failure_timestamp" : null,
        "failures" : 0
      },
      "queue" : {
        "type" : "memory"
      }
    }
  }</programlisting>
<simpara>You can see the stats for a specific pipeline by including the pipeline ID. In
the following example, the ID of the pipeline is <literal>test</literal>:</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/pipelines/test?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
    "test" : {
      "events" : {
        "duration_in_millis" : 365495,
        "in" : 216485,
        "filtered" : 216485,
        "out" : 216485,
        "queue_push_duration_in_millis" : 342466
      },
      "plugins" : {
        "inputs" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-1",
          "events" : {
            "out" : 216485,
            "queue_push_duration_in_millis" : 342466
          },
          "name" : "beats"
        } ],
        "filters" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-2",
          "events" : {
            "duration_in_millis" : 55969,
            "in" : 216485,
            "out" : 216485
          },
          "failures" : 216485,
          "patterns_per_field" : {
            "message" : 1
          },
          "name" : "grok"
        }, {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-3",
          "events" : {
            "duration_in_millis" : 3326,
            "in" : 216485,
            "out" : 216485
          },
          "name" : "geoip"
        } ],
        "outputs" : [ {
          "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-4",
          "events" : {
            "duration_in_millis" : 278557,
            "in" : 216485,
            "out" : 216485
          },
          "name" : "elasticsearch"
        } ]
      },
      "reloads" : {
        "last_error" : null,
        "successes" : 0,
        "last_success_timestamp" : null,
        "last_failure_timestamp" : null,
        "failures" : 0
      },
      "queue" : {
        "type" : "memory"
      }
    }
  }
}</programlisting>
</section>
<section id="reload-stats">
<title>Reload Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The following request returns a JSON document that shows info about config reload successes and failures.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/reloads?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "reloads": {
    "successes": 0,
    "failures": 0
  }
}</programlisting>
</section>
<section id="os-stats">
<title>OS Stats<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>When Logstash is running in a container, the following request returns a JSON document that
contains cgroup information to give you a more accurate view of CPU load, including whether
the container is being throttled.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/stats/os?pretty'</programlisting>
<simpara>Example response:</simpara>
<programlisting language="js" linenumbering="unnumbered">{
  "os" : {
    "cgroup" : {
      "cpuacct" : {
        "control_group" : "/elastic1",
        "usage_nanos" : 378477588075
                },
      "cpu" : {
        "control_group" : "/elastic1",
        "cfs_period_micros" : 1000000,
        "cfs_quota_micros" : 800000,
        "stat" : {
          "number_of_elapsed_periods" : 4157,
          "number_of_times_throttled" : 460,
          "time_throttled_nanos" : 581617440755
        }
      }
    }
  }</programlisting>
</section>
</section>
<section id="hot-threads-api">
<title>Hot Threads API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc">Edit me</ulink></title>
<simpara>The hot threads API gets the current hot threads for Logstash. A hot thread is a
Java thread that has high CPU usage and executes for a longer than normal period
of time.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/hot_threads?pretty'</programlisting>
<simpara>The output is a JSON document that contains a breakdown of the top hot threads for
Logstash.</simpara>
<simpara>Example response:</simpara>
<programlisting language="sh" linenumbering="unnumbered">{
  "hot_threads" : {
    "time" : "2017-06-06T18:25:28-07:00",
    "busiest_threads" : 3,
    "threads" : [ {
      "name" : "Ruby-0-Thread-7",
      "percent_of_cpu_time" : 0.0,
      "state" : "timed_waiting",
      "path" : "/path/to/logstash-6.1.0/vendor/bundle/jruby/1.9/gems/puma-2.16.0-java/lib/puma/thread_pool.rb:187",
      "traces" : [ "java.lang.Object.wait(Native Method)", "org.jruby.RubyThread.sleep(RubyThread.java:1002)", "org.jruby.RubyKernel.sleep(RubyKernel.java:803)" ]
    }, {
      "name" : "[test2]&gt;worker3",
      "percent_of_cpu_time" : 0.85,
      "state" : "waiting",
      "traces" : [ "sun.misc.Unsafe.park(Native Method)", "java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)", "java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)" ]
    }, {
      "name" : "[test2]&gt;worker2",
      "percent_of_cpu_time" : 0.85,
      "state" : "runnable",
      "traces" : [ "org.jruby.RubyClass.allocate(RubyClass.java:225)", "org.jruby.RubyClass.newInstance(RubyClass.java:856)", "org.jruby.RubyClass$INVOKER$i$newInstance.call(RubyClass$INVOKER$i$newInstance.gen)" ]
    } ]
  }
}</programlisting>
<simpara>The parameters allowed are:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<literal>threads</literal>
</simpara>
</entry>
<entry>
<simpara>
The number of hot threads to return. The default is 3.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>human</literal>
</simpara>
</entry>
<entry>
<simpara>
If true, returns plain text instead of JSON format. The default is false.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>ignore_idle_threads</literal>
</simpara>
</entry>
<entry>
<simpara>
If true, does not return idle threads. The default is true.
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>See <link linkend="monitoring-common-options">Common Options</link> for a list of options that can be applied to all
Logstash monitoring APIs.</simpara>
<simpara>You can use the <literal>?human</literal> parameter to return the document in a human-readable format.</simpara>
<programlisting language="js" linenumbering="unnumbered">curl -XGET 'localhost:9600/_node/hot_threads?human=true'</programlisting>
<simpara>Example of a human-readable response:</simpara>
<programlisting language="js" linenumbering="unnumbered"> ::: {}
 Hot threads at 2017-06-06T18:31:17-07:00, busiestThreads=3:
 ================================================================================
 0.0 % of cpu usage, state: timed_waiting, thread name: 'Ruby-0-Thread-7'
 /path/to/logstash-6.1.0/vendor/bundle/jruby/1.9/gems/puma-2.16.0-java/lib/puma/thread_pool.rb:187
         java.lang.Object.wait(Native Method)
         org.jruby.RubyThread.sleep(RubyThread.java:1002)
         org.jruby.RubyKernel.sleep(RubyKernel.java:803)
 --------------------------------------------------------------------------------
 0.0 % of cpu usage, state: waiting, thread name: 'defaultEventExecutorGroup-5-4'
         sun.misc.Unsafe.park(Native Method)
         java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
         java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
 --------------------------------------------------------------------------------
 0.05 % of cpu usage, state: timed_waiting, thread name: '[test]-pipeline-manager'
         java.lang.Object.wait(Native Method)
         java.lang.Thread.join(Thread.java:1253)
         org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)</programlisting>
<remark> Working with Plugins</remark>
</section>
</chapter>
<chapter id="working-with-plugins">
<title>Working with plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained
packages called gems and hosted on RubyGems.org. The plugin manager accessed via <literal>bin/logstash-plugin</literal> script is used to manage the
lifecycle of plugins in your Logstash deployment. You can install, remove and upgrade plugins using the Command Line
Interface (CLI) invocations described below.</simpara>
<bridgehead id="http-proxy" renderas="sect2">Proxy configuration<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>The majority of the plugin manager commands require access to the internet to reach <ulink url="https://rubygems.org">RubyGems.org</ulink>.
If your organization is behind a firewall you can set these environments variables to configure Logstash to use your proxy.</simpara>
<programlisting language="shell" linenumbering="unnumbered">export http_proxy=http://localhost:3128
export https_proxy=http://localhost:3128</programlisting>
<bridgehead id="listing-plugins" renderas="sect2">Listing plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>Logstash release packages bundle common plugins so you can use them out of the box. To list the plugins currently
available in your deployment:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin list <co id="CO12-1"/>
bin/logstash-plugin list --verbose <co id="CO12-2"/>
bin/logstash-plugin list '*namefragment*' <co id="CO12-3"/>
bin/logstash-plugin list --group output <co id="CO12-4"/></programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>
Will list all installed plugins
</para>
</callout>
<callout arearefs="CO12-2">
<para>
Will list installed plugins with version information
</para>
</callout>
<callout arearefs="CO12-3">
<para>
Will list all installed plugins containing a namefragment
</para>
</callout>
<callout arearefs="CO12-4">
<para>
Will list all installed plugins for a particular group (input, filter, codec, output)
</para>
</callout>
</calloutlist>
<bridgehead id="installing-plugins" renderas="sect2">Adding plugins to your deployment<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>The most common situation when dealing with plugin installation is when you have access to internet. Using this method,
you will be able to retrieve plugins hosted on the public repository (RubyGems.org) and install on top of your Logstash
installation.</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin install logstash-output-kafka</programlisting>
<simpara>Once the plugin is successfully installed, you can start using it in your configuration file.</simpara>
<bridgehead id="installing-local-plugins" renderas="sect3">Advanced: Adding a locally built plugin<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>In some cases, you want to install plugins which have not yet been released and not hosted on RubyGems.org. Logstash
provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem</programlisting>
<bridgehead id="installing-local-plugins-path" renderas="sect3">Advanced: Using <literal>--path.plugins</literal><ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>Using the Logstash <literal>--path.plugins</literal> flag, you can load a plugin source code located on your file system. Typically this is used by
developers who are iterating on a custom plugin and want to test it before creating a ruby gem.</simpara>
<simpara>The path needs to be in a  specific directory hierarchy: <literal>PATH/logstash/TYPE/NAME.rb</literal>, where TYPE is <emphasis>inputs</emphasis> <emphasis>filters</emphasis>, <emphasis>outputs</emphasis> or <emphasis>codecs</emphasis> and NAME is the name of the plugin.</simpara>
<programlisting language="shell" linenumbering="unnumbered"># supposing the code is in /opt/shared/lib/logstash/inputs/my-custom-plugin-code.rb
bin/logstash --path.plugins /opt/shared/lib</programlisting>
<bridgehead id="updating-plugins" renderas="sect2">Updating plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>Plugins have their own release cycle and are often released independent of Logstash’s core release cycle. Using the update
subcommand you can get the latest version of the plugin.</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin update <co id="CO13-1"/>
bin/logstash-plugin update logstash-output-kafka <co id="CO13-2"/></programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>
will update all installed plugins
</para>
</callout>
<callout arearefs="CO13-2">
<para>
will update only this plugin
</para>
</callout>
</calloutlist>
<bridgehead id="removing-plugins" renderas="sect2">Removing plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>If you need to remove plugins from your Logstash installation:</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin remove logstash-output-kafka</programlisting>
<bridgehead id="proxy-plugins" renderas="sect2">Proxy Support<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>The previous sections relied on Logstash being able to communicate with RubyGems.org. In certain environments, Forwarding
Proxy is used to handle HTTP requests. Logstash Plugins can be installed and updated through a Proxy by setting the
<literal>HTTP_PROXY</literal> environment variable:</simpara>
<programlisting language="shell" linenumbering="unnumbered">export HTTP_PROXY=http://127.0.0.1:3128

bin/logstash-plugin install logstash-output-kafka</programlisting>
<simpara>Once set, plugin commands install, update can be used through this proxy.</simpara>
<section id="plugin-generator">
<title>Generating Plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>You can now create your own Logstash plugin in seconds! The generate subcommand of <literal>bin/logstash-plugin</literal> creates the foundation
for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you
can start adding custom code to process data with Logstash.</simpara>
<simpara><emphasis role="strong">Example Usage</emphasis></simpara>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins</programlisting>
<itemizedlist>
<listitem>
<simpara>
<literal>--type</literal>: Type of plugin - input, filter, output, or codec
</simpara>
</listitem>
<listitem>
<simpara>
<literal>--name</literal>: Name for the new plugin
</simpara>
</listitem>
<listitem>
<simpara>
<literal>--path</literal>: Directory path where the new plugin structure will be created. If not specified, it will be
created in the current directory.
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="offline-plugins">
<title>Offline Plugin Management<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>The Logstash <link linkend="working-with-plugins">plugin manager</link> provides support for preparing offline plugin packs that you can
use to install Logstash plugins on systems that don&#8217;t have Internet access.</simpara>
<simpara>This procedure requires a staging machine running Logstash that has access to a public or
<link linkend="private-rubygem">private Rubygems</link> server. The staging machine downloads and packages all the files and dependencies
required for offline installation.</simpara>
<bridgehead id="building-offline-packs" renderas="sect2">Building Offline Plugin Packs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>An <emphasis>offline plugin pack</emphasis> is a compressed file that contains all the plugins your offline Logstash installation requires,
along with the dependencies for those plugins.</simpara>
<simpara>To build an offline plugin pack:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Make sure all the plugins that you want to package are installed on the staging server and that the staging server can
access the Internet.
</simpara>
</listitem>
<listitem>
<simpara>
Run the <literal>bin/logstash-plugin prepare-offline-pack</literal> subcommand to package the plugins and dependencies:
</simpara>
<programlisting language="shell" linenumbering="unnumbered">bin/logstash-plugin prepare-offline-pack --output OUTPUT [PLUGINS] --overwrite</programlisting>
<simpara>where:</simpara>
<itemizedlist>
<listitem>
<simpara>
<literal>OUTPUT</literal> specifies the zip file where the compressed plugin pack will be written. The default file is
<literal>/LOGSTASH_HOME/logstash-offline-plugins-6.1.0.zip</literal>. If you are using 5.2.x and 5.3.0, this location should be a zip file whose contents will be overwritten.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>[PLUGINS]</literal> specifies one or more plugins that you want to include in the pack.
</simpara>
</listitem>
<listitem>
<simpara>
<literal>--overwrite</literal> specifies if you want to override an existing file at the location
</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara>Examples:</simpara>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash-plugin prepare-offline-pack logstash-input-beats <co id="CO14-1"/>
bin/logstash-plugin prepare-offline-pack logstash-filter-* <co id="CO14-2"/>
bin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats <co id="CO14-3"/></programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>
Packages the Beats input plugin and any dependencies.
</para>
</callout>
<callout arearefs="CO14-2">
<para>
Uses a wildcard to package all filter plugins and any dependencies.
</para>
</callout>
<callout arearefs="CO14-3">
<para>
Packages all filter plugins, the Beats input plugin, and any dependencies.
</para>
</callout>
</calloutlist>
<note><simpara>Downloading all dependencies for the specified plugins may take some time, depending on the plugins listed.</simpara></note>
<bridgehead id="installing-offline-packs" renderas="sect2">Installing Offline Plugin Packs<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>To install an offline plugin pack:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Move the compressed bundle to the machine where you want to install the plugins.
</simpara>
</listitem>
<listitem>
<simpara>
Run the <literal>bin/logstash-plugin install</literal> subcommand and pass in the file URI of
the offline plugin pack.
</simpara>
<formalpara><title>Windows example:</title><para>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash-plugin install file:///c:/path/to/logstash-offline-plugins-6.1.0.zip</programlisting>
</para></formalpara>
<formalpara><title>Linux example:</title><para>
<programlisting language="sh" linenumbering="unnumbered">bin/logstash-plugin install file:///path/to/logstash-offline-plugins-6.1.0.zip</programlisting>
</para></formalpara>
<simpara>This command expects a file URI, so make sure you use forward slashes and
specify the full path to the pack.</simpara>
</listitem>
</orderedlist>
<bridgehead id="_updating_offline_plugins" renderas="sect2">Updating Offline Plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>To update offline plugins, you update the plugins on the staging server and then use the same process that you followed to
build and install the plugin pack:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
On the staging server, run the <literal>bin/logstash-plugin update</literal> subcommand to update the plugins. See <xref linkend="updating-plugins"/>.
</simpara>
</listitem>
<listitem>
<simpara>
Create a new version of the plugin pack. See <xref linkend="building-offline-packs"/>.
</simpara>
</listitem>
<listitem>
<simpara>
Install the new version of the plugin pack. See <xref linkend="installing-offline-packs"/>.
</simpara>
</listitem>
</orderedlist>
</section>
<section id="private-rubygem">
<title>Private Gem Repositories<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>The Logstash plugin manager connects to a Ruby gems repository to install and update Logstash plugins. By default, this
repository is <ulink url="http://rubygems.org">http://rubygems.org</ulink>.</simpara>
<simpara>Some use cases are unable to use the default repository, as in the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>
A firewall blocks access to the default repository.
</simpara>
</listitem>
<listitem>
<simpara>
You are developing your own plugins locally.
</simpara>
</listitem>
<listitem>
<simpara>
Airgap requirements on the local system.
</simpara>
</listitem>
</itemizedlist>
<simpara>When you use a custom gem repository, be sure to make plugin dependencies available.</simpara>
<simpara>Several open source projects enable you to run your own plugin server, among them:</simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="https://github.com/geminabox/geminabox">Geminabox</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://github.com/PierreRambaud/gemirro">Gemirro</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://gemfury.com/">Gemfury</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="http://www.jfrog.com/open-source/">Artifactory</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="_editing_the_gemfile">
<title>Editing the Gemfile<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>The gemfile is a configuration file that specifies information required for plugin management. Each gem file has a
<literal>source</literal> line that specifies a location for plugin content.</simpara>
<simpara>By default, the gemfile&#8217;s <literal>source</literal> line reads:</simpara>
<programlisting language="shell" linenumbering="unnumbered"># This is a Logstash generated Gemfile.
# If you modify this file manually all comments and formatting will be lost.

source "https://rubygems.org"</programlisting>
<simpara>To change the source, edit the <literal>source</literal> line to contain your preferred source, as in the following example:</simpara>
<programlisting language="shell" linenumbering="unnumbered"># This is a Logstash generated Gemfile.
# If you modify this file manually all comments and formatting will be lost.

source "https://my.private.repository"</programlisting>
<simpara>After saving the new version of the gemfile, use <link linkend="working-with-plugins">plugin management commands</link> normally.</simpara>
<simpara>The following links contain further material on setting up some commonly used repositories:</simpara>
<itemizedlist>
<listitem>
<simpara>
<ulink url="https://github.com/geminabox/geminabox/blob/master/README.markdown">Geminabox</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://www.jfrog.com/confluence/display/RTF/RubyGems+Repositories">Artifactory</ulink>
</simpara>
</listitem>
<listitem>
<simpara>
Running a <ulink url="http://guides.rubygems.org/run-your-own-gem-server/">rubygems mirror</ulink>
</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section id="event-api">
<title>Event API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>This section is targeted for plugin developers and users of Logstash&#8217;s Ruby filter. Below we document recent
changes (starting with version 5.0) in the way users have been accessing Logstash&#8217;s event based data in
custom plugins and in the Ruby filter. Note that <xref linkend="event-dependent-configuration"/>
data flow in Logstash&#8217;s config files&#8201;&#8212;&#8201;using <xref linkend="logstash-config-field-references"/>&#8201;&#8212;&#8201;is
not affected by this change, and will continue to use existing syntax.</simpara>
<bridgehead id="_event_object" renderas="sect3">Event Object<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin
developers to interact with the event&#8217;s content. Typically, this API is used in plugins and in a Ruby filter to
retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional
fields created during Logstash&#8217;s filter stages.</simpara>
<simpara>In 5.0, we&#8217;ve re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component
in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we&#8217;ve updated and published most of the plugins in Logstash&#8217;s ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.</simpara>
<bridgehead id="_event_api" renderas="sect3">Event API<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>Prior to version 5.0, developers could access and manipulate event data by directly using Ruby hash syntax. For
example, <literal>event[field] = foo</literal>. While this is powerful, our goal is to abstract the internal implementation details
and provide well-defined getter and setter APIs.</simpara>
<simpara><emphasis role="strong">Get API</emphasis></simpara>
<simpara>The getter is a read-only access of field-based data in an Event.</simpara>
<simpara><emphasis role="strong">Syntax:</emphasis> <literal>event.get(field)</literal></simpara>
<simpara><emphasis role="strong">Returns:</emphasis> Value for this field or nil if the field does not exist. Returned values could be a string,
numeric or timestamp scalar value.</simpara>
<simpara><literal>field</literal> is a structured field sent to Logstash or created after the transformation process. <literal>field</literal> can also
be a nested field reference such as <literal>[field][bar]</literal>.</simpara>
<simpara>Examples:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">event.get("foo" ) # =&gt; "baz"
event.get("[foo]") # =&gt; "zab"
event.get("[foo][bar]") # =&gt; 1
event.get("[foo][bar]") # =&gt; 1.0
event.get("[foo][bar]") # =&gt;  [1, 2, 3]
event.get("[foo][bar]") # =&gt; {"a" =&gt; 1, "b" =&gt; 2}
event.get("[foo][bar]") # =&gt;  {"a" =&gt; 1, "b" =&gt; 2, "c" =&gt; [1, 2]}</programlisting>
<simpara>Accessing @metadata</simpara>
<programlisting language="ruby" linenumbering="unnumbered">event.get("[@metadata][foo]") # =&gt; "baz"</programlisting>
<simpara><emphasis role="strong">Set API</emphasis></simpara>
<simpara>This API can be used to mutate data in an Event.</simpara>
<simpara><emphasis role="strong">Syntax:</emphasis> <literal>event.set(field, value)</literal></simpara>
<simpara><emphasis role="strong">Returns:</emphasis>  The current Event  after the mutation, which can be used for chainable calls.</simpara>
<simpara>Examples:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">event.set("foo", "baz")
event.set("[foo]", "zab")
event.set("[foo][bar]", 1)
event.set("[foo][bar]", 1.0)
event.set("[foo][bar]", [1, 2, 3])
event.set("[foo][bar]", {"a" =&gt; 1, "b" =&gt; 2})
event.set("[foo][bar]", {"a" =&gt; 1, "b" =&gt; 2, "c" =&gt; [1, 2]})
event.set("[@metadata][foo]", "baz")</programlisting>
<simpara>Mutating a collection after setting it in the Event has an undefined behaviour and is not allowed.</simpara>
<programlisting language="ruby" linenumbering="unnumbered">h = {"a" =&gt; 1, "b" =&gt; 2, "c" =&gt; [1, 2]}
event.set("[foo][bar]", h)

h["c"] = [3, 4]
event.get("[foo][bar][c]") # =&gt; undefined

Suggested way of mutating collections:

h = {"a" =&gt; 1, "b" =&gt; 2, "c" =&gt; [1, 2]}
event.set("[foo][bar]", h)

h["c"] = [3, 4]
event.set("[foo][bar]", h)

# Alternatively,
event.set("[foo][bar][c]", [3, 4])</programlisting>
<bridgehead id="_ruby_filter" renderas="sect3">Ruby Filter<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></bridgehead>
<simpara>The <link linkend="plugins-filters-ruby">Ruby Filter</link> can be used to execute any ruby code and manipulate event data using the
API described above. For example, using the new API:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">filter {
  ruby {
    code =&gt; 'event.set("lowercase_field", event.get("message").downcase)'
  }
}</programlisting>
<simpara>This filter will lowercase the <literal>message</literal> field, and set it to a new field called <literal>lowercase_field</literal></simpara>
<remark> These files do their own pass blocks</remark>
</section>
</chapter>
<chapter id="input-plugins">
<title>Input plugins<ulink role="edit_me" url="https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc">Edit me</ulink></title>
<simpara>An input plugin enables a specific source of events to be read by Logstash.</simpara>
<simpara>The following input plugins are available below. For a list of Elastic supported plugins, please consult the <ulink url="https://www.elastic.co/support/matrix#show_logstash_plugins">Support Matrix</ulink>.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Plugin</simpara></entry>
<entry align="left" valign="top"><simpara>Description</simpara></entry>
<entry align="left" valign="top"><simpara>Github repository</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-beats">beats</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events from the Elastic Beats framework</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-beats">logstash-input-beats</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-cloudwatch">cloudwatch</link></simpara></entry>
<entry align="left" valign="top"><simpara>Pulls events from the Amazon Web Services CloudWatch API</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-cloudwatch">logstash-input-cloudwatch</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-couchdb_changes">couchdb_changes</link></simpara></entry>
<entry align="left" valign="top"><simpara>Streams events from CouchDB&#8217;s <literal>_changes</literal> URI</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-couchdb_changes">logstash-input-couchdb_changes</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-dead_letter_queue">dead_letter_queue</link></simpara></entry>
<entry align="left" valign="top"><simpara>read events from Logstash&#8217;s dead letter queue</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue">logstash-input-dead_letter_queue</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-drupal_dblog">drupal_dblog</link></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieves watchdog log events from Drupal installations with DBLog enabled</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-drupal_dblog">logstash-input-drupal_dblog</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-elasticsearch">elasticsearch</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads query results from an Elasticsearch cluster</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-elasticsearch">logstash-input-elasticsearch</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-eventlog">eventlog</link></simpara></entry>
<entry align="left" valign="top"><simpara>Pulls events from the Windows Event Log</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-eventlog">logstash-input-eventlog</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-exec">exec</link></simpara></entry>
<entry align="left" valign="top"><simpara>Captures the output of a shell command as an event</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-exec">logstash-input-exec</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-file">file</link></simpara></entry>
<entry align="left" valign="top"><simpara>Streams events from files</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-file">logstash-input-file</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-ganglia">ganglia</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads Ganglia packets over UDP</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-ganglia">logstash-input-ganglia</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-gelf">gelf</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads GELF-format messages from Graylog2 as events</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-gelf">logstash-input-gelf</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-gemfire">gemfire</link></simpara></entry>
<entry align="left" valign="top"><simpara>Pushes events to a GemFire region</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-gemfire">logstash-input-gemfire</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-generator">generator</link></simpara></entry>
<entry align="left" valign="top"><simpara>Generates random log events for test purposes</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-generator">logstash-input-generator</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-github">github</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a GitHub webhook</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-github">logstash-input-github</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-google_pubsub">google_pubsub</link></simpara></entry>
<entry align="left" valign="top"><simpara>Consume events from a Google Cloud PubSub service</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-google_pubsub">logstash-input-google_pubsub</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-graphite">graphite</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads metrics from the <literal>graphite</literal> tool</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-graphite">logstash-input-graphite</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-heartbeat">heartbeat</link></simpara></entry>
<entry align="left" valign="top"><simpara>Generates heartbeat events for testing</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-heartbeat">logstash-input-heartbeat</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-http">http</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events over HTTP or HTTPS</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-http">logstash-input-http</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-http_poller">http_poller</link></simpara></entry>
<entry align="left" valign="top"><simpara>Decodes the output of an HTTP API into events</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-http_poller">logstash-input-http_poller</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-imap">imap</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads mail from an IMAP server</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-imap">logstash-input-imap</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-irc">irc</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from an IRC server</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-irc">logstash-input-irc</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-jdbc">jdbc</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events from JDBC data</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-jdbc">logstash-input-jdbc</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-jms">jms</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a Jms Broker</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-jms">logstash-input-jms</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-jmx">jmx</link></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieves metrics from remote Java applications over JMX</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-jmx">logstash-input-jmx</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-kafka">kafka</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a Kafka topic</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-kafka">logstash-input-kafka</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-kinesis">kinesis</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events through an AWS Kinesis stream</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-kinesis">logstash-input-kinesis</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-log4j">log4j</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events over a TCP socket from a Log4j <literal>SocketAppender</literal> object</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-log4j">logstash-input-log4j</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-lumberjack">lumberjack</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events using the Lumberjack protocl</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-lumberjack">logstash-input-lumberjack</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-meetup">meetup</link></simpara></entry>
<entry align="left" valign="top"><simpara>Captures the output of command line tools as an event</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-meetup">logstash-input-meetup</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-pipe">pipe</link></simpara></entry>
<entry align="left" valign="top"><simpara>Streams events from a long-running command pipe</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-pipe">logstash-input-pipe</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-puppet_facter">puppet_facter</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives facts from a Puppet server</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-puppet_facter">logstash-input-puppet_facter</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-rabbitmq">rabbitmq</link></simpara></entry>
<entry align="left" valign="top"><simpara>Pulls events from a RabbitMQ exchange</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-rabbitmq">logstash-input-rabbitmq</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-rackspace">rackspace</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events from a Rackspace Cloud Queue service</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-rackspace">logstash-input-rackspace</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-redis">redis</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a Redis instance</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-redis">logstash-input-redis</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-relp">relp</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives RELP events over a TCP socket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-relp">logstash-input-relp</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-rss">rss</link></simpara></entry>
<entry align="left" valign="top"><simpara>Captures the output of command line tools as an event</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-rss">logstash-input-rss</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-s3">s3</link></simpara></entry>
<entry align="left" valign="top"><simpara>Streams events from files in a S3 bucket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-s3">logstash-input-s3</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-salesforce">salesforce</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events based on a Salesforce SOQL query</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-salesforce">logstash-input-salesforce</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-snmptrap">snmptrap</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events based on SNMP trap messages</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-snmptrap">logstash-input-snmptrap</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-sqlite">sqlite</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events based on rows in an SQLite database</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-sqlite">logstash-input-sqlite</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-sqs">sqs</link></simpara></entry>
<entry align="left" valign="top"><simpara>Pulls events from an Amazon Web Services Simple Queue Service queue</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-sqs">logstash-input-sqs</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-stdin">stdin</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from standard input</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-stdin">logstash-input-stdin</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-stomp">stomp</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events received with the STOMP protocol</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-stomp">logstash-input-stomp</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-syslog">syslog</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads syslog messages as events</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-syslog">logstash-input-syslog</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-tcp">tcp</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a TCP socket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-tcp">logstash-input-tcp</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-twitter">twitter</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from the Twitter Streaming API</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-twitter">logstash-input-twitter</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-udp">udp</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events over UDP</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-udp">logstash-input-udp</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-unix">unix</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events over a UNIX socket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-unix">logstash-input-unix</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-varnishlog">varnishlog</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads from the <literal>varnish</literal> cache shared memory log</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-varnishlog">logstash-input-varnishlog</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-websocket">websocket</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a websocket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-websocket">logstash-input-websocket</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-wmi">wmi</link></simpara></entry>
<entry align="left" valign="top"><simpara>Creates events based on the results of a WMI query</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-wmi">logstash-input-wmi</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-xmpp">xmpp</link></simpara></entry>
<entry align="left" valign="top"><simpara>Receives events over the XMPP/Jabber protocol</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-xmpp">logstash-input-xmpp</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-zenoss">zenoss</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads Zenoss events from the fanout exchange</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-zenoss">logstash-input-zenoss</ulink></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="plugins-inputs-zeromq">zeromq</link></simpara></entry>
<entry align="left" valign="top"><simpara>Reads events from a ZeroMQ SUB socket</simpara></entry>
<entry align="left" valign="top"><simpara><ulink url="https://github.com/logstash-plugins/logstash-input-zeromq">logstash-input-zeromq</ulink></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<section id="plugins-inputs-beats">
<title>Beats input plugin<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<titleabbrev>beats</titleabbrev>
<itemizedlist>
<listitem>
<simpara>
Plugin version: v5.0.1
</simpara>
</listitem>
<listitem>
<simpara>
Released on: 2017-08-15
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://github.com/logstash-plugins/logstash-input-beats/blob/v5.0.1/CHANGELOG.md">Changelog</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="_getting_help">
<title>Getting Help<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For questions about the plugin, open a topic in the <ulink url="http://discuss.elastic.co">Discuss</ulink> forums. For bugs or feature requests, open an issue in <ulink url="https://github.com/logstash-plugins/logstash-input-beats">Github</ulink>.
For the list of Elastic supported plugins, please consult the <ulink url="https://www.elastic.co/support/matrix#show_logstash_plugins">Elastic Support Matrix</ulink>.</simpara>
</section>
<section id="_description">
<title>Description<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This input plugin enables Logstash to receive events from the
<ulink url="https://www.elastic.co/products/beats">Elastic Beats</ulink> framework.</simpara>
<simpara>The following example shows how to configure Logstash to listen on port
5044 for incoming Beats connections and to index into Elasticsearch:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  beats {
    port =&gt; 5044
  }
}

output {
  elasticsearch {
    hosts =&gt; "localhost:9200"
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type =&gt; "%{[@metadata][type]}"
  }
}</programlisting>
<note><simpara>The Beats shipper automatically sets the <literal>type</literal> field on the event.
You cannot override this setting in the Logstash config. If you specify
a setting for the <link linkend="plugins-inputs-beats-type"><literal>type</literal></link> config option in
Logstash, it is ignored.</simpara></note>
<important><simpara>If you are shipping events that span multiple lines, you need to
use the <ulink url="https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html">configuration options available in Filebeat</ulink> to handle multiline events
before sending the event data to Logstash. You cannot use the
<xref linkend="plugins-codecs-multiline"/> codec to handle multiline events. Doing so will
result in the failure to start Logstash.</simpara></important>
</section>
<section id="plugins-inputs-beats-options">
<title>Beats Input Configuration Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This plugin supports the following configuration options plus the <xref linkend="plugins-inputs-beats-common-options"/> described later.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-cipher_suites"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-client_inactivity_timeout"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-host"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-include_codec_tag"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-port"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_certificate"/></simpara></entry>
<entry align="left" valign="top"><simpara>a valid filesystem path</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_certificate_authorities"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_handshake_timeout"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_key"/></simpara></entry>
<entry align="left" valign="top"><simpara>a valid filesystem path</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_key_passphrase"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="password">password</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-ssl_verify_mode"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link>, one of <literal>["none", "peer", "force_peer"]</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-tls_max_version"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-tls_min_version"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Also see <xref linkend="plugins-inputs-beats-common-options"/> for a list of options supported by all
input plugins.</simpara>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-beats-cipher_suites">
<title><literal>cipher_suites</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>java.lang.String[TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256]@459cfcca</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The list of ciphers suite to use, listed by priorities.</simpara>
</section>
<section id="plugins-inputs-beats-client_inactivity_timeout">
<title><literal>client_inactivity_timeout</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>60</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Close Idle clients after X seconds of inactivity.</simpara>
</section>
<section id="plugins-inputs-beats-host">
<title><literal>host</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"0.0.0.0"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The IP address to listen on.</simpara>
</section>
<section id="plugins-inputs-beats-include_codec_tag">
<title><literal>include_codec_tag</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
</section>
<section id="plugins-inputs-beats-port">
<title><literal>port</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
This is a required setting.
</simpara>
</listitem>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>The port to listen on.</simpara>
</section>
<section id="plugins-inputs-beats-ssl">
<title><literal>ssl</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>false</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Events are by default sent in plain text. You can
enable encryption by setting <literal>ssl</literal> to true and configuring
the <literal>ssl_certificate</literal> and <literal>ssl_key</literal> options.</simpara>
</section>
<section id="plugins-inputs-beats-ssl_certificate">
<title><literal>ssl_certificate</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="path">path</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>SSL certificate to use.</simpara>
</section>
<section id="plugins-inputs-beats-ssl_certificate_authorities">
<title><literal>ssl_certificate_authorities</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>[]</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Validate client certificates against these authorities.
You can define multiple files or paths. All the certificates will
be read and added to the trust store. You need to configure the <literal>ssl_verify_mode</literal>
to <literal>peer</literal> or <literal>force_peer</literal> to enable the verification.</simpara>
</section>
<section id="plugins-inputs-beats-ssl_handshake_timeout">
<title><literal>ssl_handshake_timeout</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>10000</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Time in milliseconds for an incomplete ssl handshake to timeout</simpara>
</section>
<section id="plugins-inputs-beats-ssl_key">
<title><literal>ssl_key</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="path">path</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>SSL key to use.
NOTE: This key need to be in the PKCS8 format, you can convert it with <ulink url="https://www.openssl.org/docs/man1.1.0/apps/pkcs8.html">OpenSSL</ulink>
for more information.</simpara>
</section>
<section id="plugins-inputs-beats-ssl_key_passphrase">
<title><literal>ssl_key_passphrase</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="password">password</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>SSL key passphrase to use.</simpara>
</section>
<section id="plugins-inputs-beats-ssl_verify_mode">
<title><literal>ssl_verify_mode</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value can be any of: <literal>none</literal>, <literal>peer</literal>, <literal>force_peer</literal>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"none"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>By default the server doesn&#8217;t do any client verification.</simpara>
<simpara><literal>peer</literal> will make the server ask the client to provide a certificate.
If the client provides a certificate, it will be validated.</simpara>
<simpara><literal>force_peer</literal> will make the server ask the client to provide a certificate.
If the client doesn&#8217;t provide a certificate, the connection will be closed.</simpara>
<simpara>This option needs to be used with <literal>ssl_certificate_authorities</literal> and a defined list of CAs.</simpara>
</section>
<section id="plugins-inputs-beats-tls_max_version">
<title><literal>tls_max_version</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>1.2</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The maximum TLS version allowed for the encrypted connections. The value must be the one of the following:
1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2</simpara>
</section>
<section id="plugins-inputs-beats-tls_min_version">
<title><literal>tls_min_version</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>1</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The minimum TLS version allowed for the encrypted connections. The value must be one of the following:
1.0 for TLS 1.0, 1.1 for TLS 1.1, 1.2 for TLS 1.2</simpara>
</section>
</section>
<section id="plugins-inputs-beats-common-options">
<title>Common Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>The following configuration options are supported by all input plugins:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-add_field"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="hash">hash</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-codec"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="codec">codec</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-enable_metric"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-tags"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-beats-type"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="_details">
<title>Details<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-beats-add_field">
<title><literal>add_field</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="hash">hash</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>{}</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a field to an event</simpara>
</section>
<section id="plugins-inputs-beats-codec">
<title><literal>codec</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="codec">codec</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"plain"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.</simpara>
</section>
<section id="plugins-inputs-beats-enable_metric">
<title><literal>enable_metric</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.</simpara>
</section>
<section id="plugins-inputs-beats-id">
<title><literal>id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a unique <literal>ID</literal> to the plugin configuration. If no ID is specified, Logstash will generate one.
It is strongly recommended to set this ID in your configuration. This is particularly useful
when you have two or more plugins of the same type, for example, if you have 2 beats inputs.
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  beats {
    id =&gt; "my_plugin_id"
  }
}</programlisting>
</section>
<section id="plugins-inputs-beats-tags">
<title><literal>tags</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add any number of arbitrary tags to your event.</simpara>
<simpara>This can help with processing later.</simpara>
</section>
<section id="plugins-inputs-beats-type">
<title><literal>type</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-beats/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a <literal>type</literal> field to all events handled by this input.</simpara>
<simpara>Types are used mainly for filter activation.</simpara>
<simpara>The type is stored as part of the event itself, so you can
also use the type to search for it in Kibana.</simpara>
<simpara>If you try to set a type on an event that already has one (for
example when you send an event from a shipper to an indexer) then
a new input will not override the existing type. A type set at
the shipper stays with that event for its life even
when sent to another Logstash server.</simpara>
<note><simpara>The Beats shipper automatically sets the <literal>type</literal> field on the event.
You cannot override this setting in the Logstash config. If you specify
a setting for the <link linkend="plugins-inputs-beats-type"><literal>type</literal></link> config option in
Logstash, it is ignored.</simpara></note>
</section>
</section>
</section>
<section id="plugins-inputs-cloudwatch">
<title>Cloudwatch input plugin<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<titleabbrev>cloudwatch</titleabbrev>
<itemizedlist>
<listitem>
<simpara>
Plugin version: v2.0.2
</simpara>
</listitem>
<listitem>
<simpara>
Released on: 2017-08-15
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://github.com/logstash-plugins/logstash-input-cloudwatch/blob/v2.0.2/CHANGELOG.md">Changelog</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="_installation">
<title>Installation<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For plugins not bundled by default, it is easy to install by running <literal>bin/logstash-plugin install logstash-input-cloudwatch</literal>. See <xref linkend="working-with-plugins"/> for more details.</simpara>
</section>
<section id="_getting_help_2">
<title>Getting Help<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For questions about the plugin, open a topic in the <ulink url="http://discuss.elastic.co">Discuss</ulink> forums. For bugs or feature requests, open an issue in <ulink url="https://github.com/logstash-plugins/logstash-input-cloudwatch">Github</ulink>.
For the list of Elastic supported plugins, please consult the <ulink url="https://www.elastic.co/support/matrix#show_logstash_plugins">Elastic Support Matrix</ulink>.</simpara>
</section>
<section id="_description_2">
<title>Description<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>Pull events from the Amazon Web Services CloudWatch API.</simpara>
<simpara>To use this plugin, you <emphasis role="strong">must</emphasis> have an AWS account, and the following policy</simpara>
<simpara>Typically, you should setup an IAM policy, create a user and apply the IAM policy to the user.
A sample policy for EC2 metrics is as follows:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1444715676000",
            "Effect": "Allow",
            "Action": [
                "cloudwatch:GetMetricStatistics",
                "cloudwatch:ListMetrics"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Stmt1444716576170",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances"
            ],
            "Resource": "*"
        }
    ]
}</programlisting>
<simpara>See <ulink url="http://aws.amazon.com/iam/">http://aws.amazon.com/iam/</ulink> for more details on setting up AWS identities.</simpara>
<simpara># Configuration Example</simpara>
<programlisting language="ruby" linenumbering="unnumbered">input {
  cloudwatch {
    namespace =&gt; "AWS/EC2"
    metrics =&gt; [ "CPUUtilization" ]
    filters =&gt; { "tag:Group" =&gt; "API-Production" }
    region =&gt; "us-east-1"
  }
}</programlisting>
<literallayout class="monospaced">input {
  cloudwatch {
    namespace =&gt; "AWS/EBS"
    metrics =&gt; ["VolumeQueueLength"]
    filters =&gt; { "tag:Monitoring" =&gt; "Yes" }
    region =&gt; "us-east-1"
  }
}</literallayout>
<literallayout class="monospaced">input {
  cloudwatch {
    namespace =&gt; "AWS/RDS"
    metrics =&gt; ["CPUUtilization", "CPUCreditUsage"]
    filters =&gt; { "EngineName" =&gt; "mysql" } # Only supports EngineName, DatabaseClass and DBInstanceIdentifier
    region =&gt; "us-east-1"
  }
}</literallayout>
</section>
<section id="plugins-inputs-cloudwatch-options">
<title>Cloudwatch Input Configuration Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This plugin supports the following configuration options plus the <xref linkend="plugins-inputs-cloudwatch-common-options"/> described later.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-access_key_id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-aws_credentials_file"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-combined"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-filters"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-interval"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-metrics"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-namespace"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-period"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-proxy_uri"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-region"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link>, one of <literal>["us-east-1", "us-east-2", "us-west-1", "us-west-2", "eu-central-1", "eu-west-1", "eu-west-2", "ap-southeast-1", "ap-southeast-2", "ap-northeast-1", "ap-northeast-2", "sa-east-1", "us-gov-west-1", "cn-north-1", "ap-south-1", "ca-central-1"]</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-secret_access_key"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-session_token"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-statistics"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-use_ssl"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Also see <xref linkend="plugins-inputs-cloudwatch-common-options"/> for a list of options supported by all
input plugins.</simpara>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-cloudwatch-access_key_id">
<title><literal>access_key_id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>This plugin uses the AWS SDK and supports several ways to get credentials, which will be tried in this order:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>
Static configuration, using <literal>access_key_id</literal> and <literal>secret_access_key</literal> params in logstash plugin config
</simpara>
</listitem>
<listitem>
<simpara>
External credentials file specified by <literal>aws_credentials_file</literal>
</simpara>
</listitem>
<listitem>
<simpara>
Environment variables <literal>AWS_ACCESS_KEY_ID</literal> and <literal>AWS_SECRET_ACCESS_KEY</literal>
</simpara>
</listitem>
<listitem>
<simpara>
Environment variables <literal>AMAZON_ACCESS_KEY_ID</literal> and <literal>AMAZON_SECRET_ACCESS_KEY</literal>
</simpara>
</listitem>
<listitem>
<simpara>
IAM Instance Profile (available when running inside EC2)
</simpara>
</listitem>
</orderedlist>
</section>
<section id="plugins-inputs-cloudwatch-aws_credentials_file">
<title><literal>aws_credentials_file</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Path to YAML file containing a hash of AWS credentials.
This file will only be loaded if <literal>access_key_id</literal> and
<literal>secret_access_key</literal> aren&#8217;t set. The contents of the
file should look like this:</simpara>
<programlisting language="ruby" linenumbering="unnumbered">    :access_key_id: "12345"
    :secret_access_key: "54321"</programlisting>
</section>
<section id="plugins-inputs-cloudwatch-combined">
<title><literal>combined</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>false</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Use this for namespaces that need to combine the dimensions like S3 and SNS.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-filters">
<title><literal>filters</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
This is a required setting.
</simpara>
</listitem>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Specify the filters to apply when fetching resources:</simpara>
<simpara>This needs to follow the AWS convention of specifiying filters.
Instances: { <emphasis>instance-id</emphasis> &#8658; <emphasis>i-12344321</emphasis> }
Tags: { "tag:Environment" &#8658; "Production" }
Volumes: { <emphasis>attachment.status</emphasis> &#8658; <emphasis>attached</emphasis> }
Each namespace uniquely support certian dimensions. Please consult the documentation
to ensure you&#8217;re using valid filters.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-interval">
<title><literal>interval</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>900</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Set how frequently CloudWatch should be queried</simpara>
<simpara>The default, <literal>900</literal>, means check every 15 minutes. Setting this value too low
(generally less than 300) results in no metrics being returned from CloudWatch.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-metrics">
<title><literal>metrics</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>["CPUUtilization", "DiskReadOps", "DiskWriteOps", "NetworkIn", "NetworkOut"]</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Specify the metrics to fetch for the namespace. The defaults are AWS/EC2 specific. See <ulink url="http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html">http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html</ulink>
for the available metrics for other namespaces.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-namespace">
<title><literal>namespace</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"AWS/EC2"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>If undefined, LogStash will complain, even if codec is unused.
The service namespace of the metrics to fetch.</simpara>
<simpara>The default is for the EC2 service. See <ulink url="http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html">http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html</ulink>
for valid values.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-period">
<title><literal>period</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>300</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Set the granularity of the returned datapoints.</simpara>
<simpara>Must be at least 60 seconds and in multiples of 60.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-proxy_uri">
<title><literal>proxy_uri</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>URI to proxy server if required</simpara>
</section>
<section id="plugins-inputs-cloudwatch-region">
<title><literal>region</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value can be any of: <literal>us-east-1</literal>, <literal>us-east-2</literal>, <literal>us-west-1</literal>, <literal>us-west-2</literal>, <literal>eu-central-1</literal>, <literal>eu-west-1</literal>, <literal>eu-west-2</literal>, <literal>ap-southeast-1</literal>, <literal>ap-southeast-2</literal>, <literal>ap-northeast-1</literal>, <literal>ap-northeast-2</literal>, <literal>sa-east-1</literal>, <literal>us-gov-west-1</literal>, <literal>cn-north-1</literal>, <literal>ap-south-1</literal>, <literal>ca-central-1</literal>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"us-east-1"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The AWS Region</simpara>
</section>
<section id="plugins-inputs-cloudwatch-secret_access_key">
<title><literal>secret_access_key</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>The AWS Secret Access Key</simpara>
</section>
<section id="plugins-inputs-cloudwatch-session_token">
<title><literal>session_token</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>The AWS Session token for temporary credential</simpara>
</section>
<section id="plugins-inputs-cloudwatch-statistics">
<title><literal>statistics</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>["SampleCount", "Average", "Minimum", "Maximum", "Sum"]</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Specify the statistics to fetch for each namespace</simpara>
</section>
<section id="plugins-inputs-cloudwatch-use_ssl">
<title><literal>use_ssl</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Make sure we require the V1 classes when including this module.
require <emphasis>aws-sdk</emphasis> will load v2 classes.
Should we require (true) or disable (false) using SSL for communicating with the AWS API
The AWS SDK for Ruby defaults to SSL so we preserve that</simpara>
</section>
</section>
<section id="plugins-inputs-cloudwatch-common-options">
<title>Common Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>The following configuration options are supported by all input plugins:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-add_field"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="hash">hash</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-codec"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="codec">codec</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-enable_metric"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-tags"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-cloudwatch-type"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="_details_2">
<title>Details<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-cloudwatch-add_field">
<title><literal>add_field</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="hash">hash</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>{}</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a field to an event</simpara>
</section>
<section id="plugins-inputs-cloudwatch-codec">
<title><literal>codec</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="codec">codec</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"plain"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-enable_metric">
<title><literal>enable_metric</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-id">
<title><literal>id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a unique <literal>ID</literal> to the plugin configuration. If no ID is specified, Logstash will generate one.
It is strongly recommended to set this ID in your configuration. This is particularly useful
when you have two or more plugins of the same type, for example, if you have 2 cloudwatch inputs.
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  cloudwatch {
    id =&gt; "my_plugin_id"
  }
}</programlisting>
</section>
<section id="plugins-inputs-cloudwatch-tags">
<title><literal>tags</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add any number of arbitrary tags to your event.</simpara>
<simpara>This can help with processing later.</simpara>
</section>
<section id="plugins-inputs-cloudwatch-type">
<title><literal>type</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-cloudwatch/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a <literal>type</literal> field to all events handled by this input.</simpara>
<simpara>Types are used mainly for filter activation.</simpara>
<simpara>The type is stored as part of the event itself, so you can
also use the type to search for it in Kibana.</simpara>
<simpara>If you try to set a type on an event that already has one (for
example when you send an event from a shipper to an indexer) then
a new input will not override the existing type. A type set at
the shipper stays with that event for its life even
when sent to another Logstash server.</simpara>
</section>
</section>
</section>
<section id="plugins-inputs-couchdb_changes">
<title>Couchdb_changes input plugin<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<titleabbrev>couchdb_changes</titleabbrev>
<itemizedlist>
<listitem>
<simpara>
Plugin version: v3.1.3
</simpara>
</listitem>
<listitem>
<simpara>
Released on: 2017-08-15
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/blob/v3.1.3/CHANGELOG.md">Changelog</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="_installation_2">
<title>Installation<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For plugins not bundled by default, it is easy to install by running <literal>bin/logstash-plugin install logstash-input-couchdb_changes</literal>. See <xref linkend="working-with-plugins"/> for more details.</simpara>
</section>
<section id="_getting_help_3">
<title>Getting Help<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For questions about the plugin, open a topic in the <ulink url="http://discuss.elastic.co">Discuss</ulink> forums. For bugs or feature requests, open an issue in <ulink url="https://github.com/logstash-plugins/logstash-input-couchdb_changes">Github</ulink>.
For the list of Elastic supported plugins, please consult the <ulink url="https://www.elastic.co/support/matrix#show_logstash_plugins">Elastic Support Matrix</ulink>.</simpara>
</section>
<section id="_description_3">
<title>Description<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This CouchDB input allows you to automatically stream events from the
CouchDB <ulink url="http://guide.couchdb.org/draft/notifications.html">_changes</ulink> URI.
Moreover, any "future" changes will automatically be streamed as well making it easy to synchronize
your CouchDB data with any target destination</simpara>
<simpara># Upsert and delete
You can use event metadata to allow for document deletion.
All non-delete operations are treated as upserts</simpara>
<simpara># Starting at a Specific Sequence
The CouchDB input stores the last sequence number value in location defined by <literal>sequence_path</literal>.
You can use this fact to start or resume the stream at a particular sequence.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-options">
<title>Couchdb_changes Input Configuration Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This plugin supports the following configuration options plus the <xref linkend="plugins-inputs-couchdb_changes-common-options"/> described later.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-always_reconnect"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-ca_file"/></simpara></entry>
<entry align="left" valign="top"><simpara>a valid filesystem path</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-db"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-heartbeat"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-host"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-ignore_attachments"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-initial_sequence"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-keep_id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-keep_revision"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-password"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="password">password</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-port"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-reconnect_delay"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-secure"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-sequence_path"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-timeout"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="number">number</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-username"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Also see <xref linkend="plugins-inputs-couchdb_changes-common-options"/> for a list of options supported by all
input plugins.</simpara>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-couchdb_changes-always_reconnect">
<title><literal>always_reconnect</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Reconnect flag.  When true, always try to reconnect after a failure</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-ca_file">
<title><literal>ca_file</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="path">path</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Path to a CA certificate file, used to validate certificates</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-db">
<title><literal>db</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
This is a required setting.
</simpara>
</listitem>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>The CouchDB db to connect to.
Required parameter.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-heartbeat">
<title><literal>heartbeat</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>1000</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Logstash connects to CouchDB&#8217;s _changes with feed=continuous
The heartbeat is how often (in milliseconds) Logstash will ping
CouchDB to ensure the connection is maintained.  Changing this
setting is not recommended unless you know what you are doing.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-host">
<title><literal>host</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"localhost"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>IP or hostname of your CouchDB instance</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-ignore_attachments">
<title><literal>ignore_attachments</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Future feature! Until implemented, changing this from the default
will not do anything.</simpara>
<simpara>Ignore attachments associated with CouchDB documents.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-initial_sequence">
<title><literal>initial_sequence</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>If unspecified, Logstash will attempt to read the last sequence number
from the <literal>sequence_path</literal> file.  If that is empty or non-existent, it will
begin with 0 (the beginning).</simpara>
<simpara>If you specify this value, it is anticipated that you will
only be doing so for an initial read under special circumstances
and that you will unset this value afterwards.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-keep_id">
<title><literal>keep_id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>false</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Preserve the CouchDB document id "_id" value in the
output.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-keep_revision">
<title><literal>keep_revision</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>false</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Preserve the CouchDB document revision "_rev" value in the
output.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-password">
<title><literal>password</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="password">password</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>nil</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Password, if authentication is needed to connect to
CouchDB</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-port">
<title><literal>port</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>5984</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Port of your CouchDB instance.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-reconnect_delay">
<title><literal>reconnect_delay</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>10</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Reconnect delay: time between reconnect attempts, in seconds.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-secure">
<title><literal>secure</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>false</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Connect to CouchDB&#8217;s _changes feed securely (via https)
Default: false (via http)</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-sequence_path">
<title><literal>sequence_path</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>File path where the last sequence number in the _changes
stream is stored. If unset it will write to <literal>$HOME/.couchdb_seq</literal></simpara>
</section>
<section id="plugins-inputs-couchdb_changes-timeout">
<title><literal>timeout</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="number">number</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Timeout: Number of milliseconds to wait for new data before
terminating the connection.  If a timeout is set it will disable
the heartbeat configuration option.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-username">
<title><literal>username</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>nil</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Username, if authentication is needed to connect to
CouchDB</simpara>
</section>
</section>
<section id="plugins-inputs-couchdb_changes-common-options">
<title>Common Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>The following configuration options are supported by all input plugins:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-add_field"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="hash">hash</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-codec"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="codec">codec</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-enable_metric"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-tags"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-couchdb_changes-type"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="_details_3">
<title>Details<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-couchdb_changes-add_field">
<title><literal>add_field</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="hash">hash</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>{}</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a field to an event</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-codec">
<title><literal>codec</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="codec">codec</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"plain"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-enable_metric">
<title><literal>enable_metric</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-id">
<title><literal>id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a unique <literal>ID</literal> to the plugin configuration. If no ID is specified, Logstash will generate one.
It is strongly recommended to set this ID in your configuration. This is particularly useful
when you have two or more plugins of the same type, for example, if you have 2 couchdb_changes inputs.
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  couchdb_changes {
    id =&gt; "my_plugin_id"
  }
}</programlisting>
</section>
<section id="plugins-inputs-couchdb_changes-tags">
<title><literal>tags</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add any number of arbitrary tags to your event.</simpara>
<simpara>This can help with processing later.</simpara>
</section>
<section id="plugins-inputs-couchdb_changes-type">
<title><literal>type</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-couchdb_changes/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a <literal>type</literal> field to all events handled by this input.</simpara>
<simpara>Types are used mainly for filter activation.</simpara>
<simpara>The type is stored as part of the event itself, so you can
also use the type to search for it in Kibana.</simpara>
<simpara>If you try to set a type on an event that already has one (for
example when you send an event from a shipper to an indexer) then
a new input will not override the existing type. A type set at
the shipper stays with that event for its life even
when sent to another Logstash server.</simpara>
</section>
</section>
</section>
<section id="plugins-inputs-dead_letter_queue">
<title>Dead_letter_queue input plugin<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<titleabbrev>dead_letter_queue</titleabbrev>
<itemizedlist>
<listitem>
<simpara>
Plugin version: v1.1.0
</simpara>
</listitem>
<listitem>
<simpara>
Released on: 2017-08-25
</simpara>
</listitem>
<listitem>
<simpara>
<ulink url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/blob/v1.1.0/CHANGELOG.md">Changelog</ulink>
</simpara>
</listitem>
</itemizedlist>
<section id="_installation_3">
<title>Installation<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For plugins not bundled by default, it is easy to install by running <literal>bin/logstash-plugin install logstash-input-dead_letter_queue</literal>. See <xref linkend="working-with-plugins"/> for more details.</simpara>
</section>
<section id="_getting_help_4">
<title>Getting Help<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>For questions about the plugin, open a topic in the <ulink url="http://discuss.elastic.co">Discuss</ulink> forums. For bugs or feature requests, open an issue in <ulink url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue">Github</ulink>.
For the list of Elastic supported plugins, please consult the <ulink url="https://www.elastic.co/support/matrix#show_logstash_plugins">Elastic Support Matrix</ulink>.</simpara>
</section>
<section id="_description_4">
<title>Description<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>Logstash input to read events from Logstash&#8217;s dead letter queue.</simpara>
<programlisting language="sh" linenumbering="unnumbered">input {
  dead_letter_queue {
    path =&gt; "/var/logstash/data/dead_letter_queue"
    start_timestamp =&gt; "2017-04-04T23:40:37"
  }
}</programlisting>
</section>
<section id="plugins-inputs-dead_letter_queue-options">
<title>Dead_letter_queue Input Configuration Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This plugin supports the following configuration options plus the <xref linkend="plugins-inputs-dead_letter_queue-common-options"/> described later.</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-commit_offsets"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-path"/></simpara></entry>
<entry align="left" valign="top"><simpara>a valid filesystem path</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-pipeline_id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-sincedb_path"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-start_timestamp"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Also see <xref linkend="plugins-inputs-dead_letter_queue-common-options"/> for a list of options supported by all
input plugins.</simpara>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-dead_letter_queue-commit_offsets">
<title><literal>commit_offsets</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Specifies whether this input should commit offsets as it processes the events.
Typically you specify <literal>false</literal> when you want to iterate multiple times over the
events in the dead letter queue, but don&#8217;t want to save state. This is when you
are exploring the events in the dead letter queue.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-path">
<title><literal>path</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
This is a required setting.
</simpara>
</listitem>
<listitem>
<simpara>
Value type is <link linkend="path">path</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Path to the dead letter queue directory that was created by a Logstash instance.
This is the path from which "dead" events are read and is typically configured
in the original Logstash instance with the setting <literal>path.dead_letter_queue</literal>.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-pipeline_id">
<title><literal>pipeline_id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"main"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>ID of the pipeline whose events you want to read from.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-sincedb_path">
<title><literal>sincedb_path</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Path of the sincedb database file (keeps track of the current position of dead letter queue) that
will be written to disk. The default will write sincedb files to <literal>&lt;path.data&gt;/plugins/inputs/dead_letter_queue</literal>.</simpara>
<note><simpara>This value must be a file path and not a directory path.</simpara></note>
</section>
<section id="plugins-inputs-dead_letter_queue-start_timestamp">
<title><literal>start_timestamp</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Timestamp in ISO8601 format from when you want to start processing the events from.
For example, <literal>2017-04-04T23:40:37</literal>.</simpara>
</section>
</section>
<section id="plugins-inputs-dead_letter_queue-common-options">
<title>Common Options<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>The following configuration options are supported by all input plugins:</simpara>
<informaltable
frame="all"
rowsep="1" colsep="1"
>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Setting </entry>
<entry align="left" valign="top">Input type</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-add_field"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="hash">hash</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-codec"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="codec">codec</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-enable_metric"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="boolean">boolean</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-id"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-tags"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="array">array</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><xref linkend="plugins-inputs-dead_letter_queue-type"/></simpara></entry>
<entry align="left" valign="top"><simpara><link linkend="string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section id="_details_4">
<title>Details<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>&nbsp;</simpara>
<section id="plugins-inputs-dead_letter_queue-add_field">
<title><literal>add_field</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="hash">hash</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>{}</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a field to an event</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-codec">
<title><literal>codec</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="codec">codec</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>"plain"</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-enable_metric">
<title><literal>enable_metric</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="boolean">boolean</link>
</simpara>
</listitem>
<listitem>
<simpara>
Default value is <literal>true</literal>
</simpara>
</listitem>
</itemizedlist>
<simpara>Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-id">
<title><literal>id</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a unique <literal>ID</literal> to the plugin configuration. If no ID is specified, Logstash will generate one.
It is strongly recommended to set this ID in your configuration. This is particularly useful
when you have two or more plugins of the same type, for example, if you have 2 dead_letter_queue inputs.
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.</simpara>
<programlisting language="json" linenumbering="unnumbered">input {
  dead_letter_queue {
    id =&gt; "my_plugin_id"
  }
}</programlisting>
</section>
<section id="plugins-inputs-dead_letter_queue-tags">
<title><literal>tags</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="array">array</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add any number of arbitrary tags to your event.</simpara>
<simpara>This can help with processing later.</simpara>
</section>
<section id="plugins-inputs-dead_letter_queue-type">
<title><literal>type</literal><ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-dead_letter_queue/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<itemizedlist>
<listitem>
<simpara>
Value type is <link linkend="string">string</link>
</simpara>
</listitem>
<listitem>
<simpara>
There is no default value for this setting.
</simpara>
</listitem>
</itemizedlist>
<simpara>Add a <literal>type</literal> field to all events handled by this input.</simpara>
<simpara>Types are used mainly for filter activation.</simpara>
<simpara>The type is stored as part of the event itself, so you can
also use the type to search for it in Kibana.</simpara>
<simpara>If you try to set a type on an event that already has one (for
example when you send an event from a shipper to an indexer) then
a new input will not override the existing type. A type set at
the shipper stays with that event for its life even
when sent to another Logstash server.</simpara>
</section>
</section>
</section>
<section id="plugins-inputs-drupal_dblog">
<title>Drupal_dblog input plugin<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-drupal_dblog/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<note><simpara>This plugin cannot be installed on the current version of Logstash. We are working on resolving this problem.</simpara></note>
<simpara>Retrieve watchdog log events from a Drupal installation with DBLog enabled.
The events are pulled out directly from the database.
The original events are not deleted, and on every consecutive run only new
events are pulled.</simpara>
<simpara>The last watchdog event id that was processed is stored in the Drupal
variable table with the name "logstash_last_wid". Delete this variable or
set it to 0 if you want to re-import all events.</simpara>
<simpara>More info on DBLog: <ulink url="http://drupal.org/documentation/modules/dblog">http://drupal.org/documentation/modules/dblog</ulink></simpara>
<simpara>&nbsp;</simpara>
<section id="_synopsis">
<title>Synopsis<ulink role="edit_me" url="https://github.com/logstash-plugins/logstash-input-drupal_dblog/edit/master/docs/index.asciidoc">Edit me</ulink></title>
<simpara>This plugin supports the following configuration options:</simpara>
<simpara>Required configuration options:</simpara>
<programlisting language="json" linenumbering="unnumbered">drupal_dblog {
}</programlisting>
<simpara>Available configuration options:</simpara>
