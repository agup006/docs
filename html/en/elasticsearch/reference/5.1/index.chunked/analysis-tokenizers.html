<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title xmlns="">Tokenizers
        | Elasticsearch Reference [5.1]
      | Elastic
    </title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch Reference [5.1]" /><link rel="up" href="analysis.html" title="Analysis" /><link rel="prev" href="analysis-custom-analyzer.html" title="Custom Analyzer" /><link rel="next" href="analysis-standard-tokenizer.html" title="Standard Tokenizer" /><meta xmlns="" name="description" content="Get started with the documentation for Elasticsearch, Kibana, Logstash, Beats, X-Pack, Elastic Cloud, Elasticsearch for Apache Hadoop, and our language clients." /><meta xmlns="" name="DC.type" content="Docs/Elasticsearch/Reference/5.1" /></head><body><div xmlns="" class="page_header">You are looking at documentation for an older release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div><div xmlns="" class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference
      [5.1]
    </a></span> » <span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span> » <span class="breadcrumb-node">Tokenizers</span></div><div xmlns="" class="navheader"><span class="prev"><a href="analysis-custom-analyzer.html">
              « 
              Custom Analyzer</a>
           
        </span><span class="next">
           
          <a href="analysis-standard-tokenizer.html">Standard Tokenizer
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="analysis-tokenizers"></a>Tokenizers<a xmlns="" href="https://github.com/elastic/elasticsearch/edit/5.1/docs/reference/analysis/tokenizers.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><p>A <span class="emphasis"><em>tokenizer</em></span>  receives a stream of characters, breaks it up into individual
<span class="emphasis"><em>tokens</em></span> (usually individual words), and outputs a stream of <span class="emphasis"><em>tokens</em></span>. For
instance, a <a class="link" href="analysis-whitespace-tokenizer.html" title="Whitespace Tokenizer"><code class="literal">whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace.  It would convert the text
<code class="literal">"Quick brown fox!"</code> into the terms <code class="literal">[Quick, brown, fox!]</code>.</p><p>The tokenizer is also responsible for recording the order or <span class="emphasis"><em>position</em></span> of
each term (used for phrase and word proximity queries) and the start and end
<span class="emphasis"><em>character offsets</em></span> of the original word which the term represents (used for
highlighting search snippets).</p><p>Elasticsearch has a number of built in tokenizers which can be used to build
<a class="link" href="analysis-custom-analyzer.html" title="Custom Analyzer">custom analyzers</a>.</p><h3><a id="_word_oriented_tokenizers"></a>Word Oriented Tokenizers<a xmlns="" href="https://github.com/elastic/elasticsearch/edit/5.1/docs/reference/analysis/tokenizers.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>The following tokenizers are usually used for tokenizing full text into
individual words:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
<a class="link" href="analysis-standard-tokenizer.html" title="Standard Tokenizer">Standard Tokenizer</a>
</span></dt><dd>
The <code class="literal">standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.
</dd><dt><span class="term">
<a class="link" href="analysis-letter-tokenizer.html" title="Letter Tokenizer">Letter Tokenizer</a>
</span></dt><dd>
The <code class="literal">letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.
</dd><dt><span class="term">
<a class="link" href="analysis-lowercase-tokenizer.html" title="Lowercase Tokenizer">Lowercase Tokenizer</a>
</span></dt><dd>
The <code class="literal">lowercase</code> tokenizer, like the <code class="literal">letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.
</dd><dt><span class="term">
<a class="link" href="analysis-whitespace-tokenizer.html" title="Whitespace Tokenizer">Whitespace Tokenizer</a>
</span></dt><dd>
The <code class="literal">whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.
</dd><dt><span class="term">
<a class="link" href="analysis-uaxurlemail-tokenizer.html" title="UAX URL Email Tokenizer">UAX URL Email Tokenizer</a>
</span></dt><dd>
The <code class="literal">uax_url_email</code> tokenizer is like the <code class="literal">standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.
</dd><dt><span class="term">
<a class="link" href="analysis-classic-tokenizer.html" title="Classic Tokenizer">Classic Tokenizer</a>
</span></dt><dd>
The <code class="literal">classic</code> tokenizer is a grammar based tokenizer for the English Language.
</dd><dt><span class="term">
<a class="link" href="analysis-thai-tokenizer.html" title="Thai Tokenizer">Thai Tokenizer</a>
</span></dt><dd>
The <code class="literal">thai</code> tokenizer segments Thai text into words.
</dd></dl></div><h3><a id="_partial_word_tokenizers"></a>Partial Word Tokenizers<a xmlns="" href="https://github.com/elastic/elasticsearch/edit/5.1/docs/reference/analysis/tokenizers.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>These tokenizers break up text or words into small fragments, for partial word
matching:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
<a class="link" href="analysis-ngram-tokenizer.html" title="NGram Tokenizer">N-Gram Tokenizer</a>
</span></dt><dd>
The <code class="literal">ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code class="literal">quick</code> →
<code class="literal">[qu, ui, ic, ck]</code>.
</dd><dt><span class="term">
<a class="link" href="analysis-edgengram-tokenizer.html" title="Edge NGram Tokenizer">Edge N-Gram Tokenizer</a>
</span></dt><dd>
The <code class="literal">edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code class="literal">quick</code> →
<code class="literal">[q, qu, qui, quic, quick]</code>.
</dd></dl></div><h3><a id="_structured_text_tokenizers"></a>Structured Text Tokenizers<a xmlns="" href="https://github.com/elastic/elasticsearch/edit/5.1/docs/reference/analysis/tokenizers.asciidoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
<a class="link" href="analysis-keyword-tokenizer.html" title="Keyword Tokenizer">Keyword Tokenizer</a>
</span></dt><dd>
The <code class="literal">keyword</code> tokenizer is a “noop” tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters like <a class="link" href="analysis-lowercase-tokenfilter.html" title="Lowercase Token Filter"><code class="literal">lowercase</code></a> to
normalise the analysed terms.
</dd><dt><span class="term">
<a class="link" href="analysis-pattern-tokenizer.html" title="Pattern Tokenizer">Pattern Tokenizer</a>
</span></dt><dd>
The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.
</dd><dt><span class="term">
<a class="link" href="analysis-pathhierarchy-tokenizer.html" title="Path Hierarchy Tokenizer">Path Tokenizer</a>
</span></dt><dd>
The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code class="literal">/foo/bar/baz</code> → <code class="literal">[/foo, /foo/bar, /foo/bar/baz ]</code>.
</dd></dl></div></div><div xmlns="" class="navfooter"><span class="prev"><a href="analysis-custom-analyzer.html">
              « 
              Custom Analyzer</a>
           
        </span><span class="next">
           
          <a href="analysis-standard-tokenizer.html">Standard Tokenizer
               »
            </a></span></div></body></html>